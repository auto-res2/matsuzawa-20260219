{
  "research_topic": "Proposing an improved Chain-of-Thought based on human thinking methods, evaluated purely through prompt tuning without fine-tuning or time-intensive experiments",
  "queries": [
    "chain-of-thought prompting",
    "prompt tuning reasoning",
    "human-like reasoning prompts"
  ],
  "research_study_list": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "full_text": "1 Geometric instability of out of distribution data across autoencoder architecture Susama Agarwala1, Benjamin Dees 2, and Corey Lowman 1 1Johns Hopkins University Applied Physics Lab 2Johns Hopking University, Mathematics Department Abstract—We study the map learned by a family of autoencoders trained on MNIST, and evaluated on ten different data sets created by the random selection of pixel values according to ten different distributions. Speciﬁcally, we study the eigenvalues of the Jacobians deﬁned by the weight matrices of the autoencoder at each training and evaluation point. For high enough latent dimension, we ﬁnd that each autoencoder reconstructs all the evaluation data sets as similar generalized characters, but that this recon- structed generalized character changes across autoencoder. Eigenvalue analysis shows that even when the reconstructed image appears to be an MNIST character for all out of distribution data sets, not all have latent representations that are close to the latent representation of MNIST characters. All told, the eigenvalue analysis demonstrated a great deal of geometric instability of the autoencoder both as a function on out of distribution inputs, and across architectures on the same set of inputs. The distributions of training, test and validation data is often different than the distribution of the data on which a neural network is deployed. This problem of distribution shift causes neural networks to fail silently, with potentially serious consequences [1], [2]. The re- search community has tried to address this in many ways with different levels of success. For instance, generative models [3], [4] effectively learn the distributions of the probability distribution of the training data set, but they do not address the fact that one often has little control over the deployment data distribution. To address this problem, the research community has come up with a wide family of out-of-distribution detection algorithms [5], [6], [7], [8], with a variety of strengths and weak- nesses. However, to our knowledge, there has not been any concerted effort to understand the geometric properties of the feature map that transforms inputs to a latent representation. While this map is trained on the training, test and validation data distributions, it is a function on a much larger space of possible inputs. In this paper, we consider an autoencoder as a function from input to reconstruction space, such that one can consider its Jacobian matrix at any input point. The eigenvalues and eigenvectors of these Jacobians contain a wealth of information about the geometric properties of the feature map. In [9], the authors studied the geometry of this function on the training data set (MNIST). In this paper, we consider the geometry on points very far out of distribution (speciﬁcally, on 10 data sets where pixel brightness is drawn from 10 different distributions). As autoencoders are supposed to be a method for learning the manifold structure of a data set, they are frequently used as means of out of distribution detection [10], [11]. However, this method of out of distribution detection fails due to the well documented phenomenon of out of distribution reconstruction [3], [12], [13], [14]. We consider the geometric stability of the learned feature map on points so far from the training data set that out of distribution reconstruction is not a worry, and characterize how these geometric properties are different from those on the training set. We believe this line of inquiry will lead to a foundational understanding of what an autoencoder is learning about the geometry of the data manifold as well as the relation to the input space, and lead to a new set of geometric tools for out of distribution detection. Concretely, we evaluate a family of autoencoders with identical architecture except for the dimension of the latent space for various seeds. Then we evaluate 10 data sets, each with pixel values drawn from a different random distribution on these trained networks. We ﬁnd that on data sets far out of distribution, at low latent dimension, the reconstruction often appears to be a reconstructed image (under visual inspection) from the training data set. However, the autoencoder will map any given random distribution of pixels to a variety of differ- ent training images, and this set will change from seed to seed. Surprisingly, however, the geometric data suggests that even though the reconstruction images appear to be close to the reconstructed images of the training data set, the latent representation of the training data and the out arXiv:2201.11902v1  [cs.LG]  28 Jan 2022of distribution data is quite different. Furthermore, we ﬁnd that for higher latent dimensions, the autoencoder consistently maps each random pixel distribution data set to the same class of generalized character, but that this character is both seed and architecture dependent. We ﬁnd that the geometries of the learned feature functions cluster by mean value of the pixel distribution. More surprisingly, we learn that for higher latent dimension, the trained autoencoder is not aware of the orientation of the bases of the input space. I. A UTOENCODERS AND LEARNED GEOMETRY An autoencoder consists of a pair of neural networks trained in tandem, an encoder that maps points from a high dimensional input space, I≃ RN to a lower dimen- sional latent space L≃ Rd, and a decoder that maps the points in the latent space back to a high dimensional reconstruction space (isomorphic to the input space), R≃ RN, with d << N. Let fenc indicate the encoder network, and fdec the decoder. For any input pointx∈I, let y = fdec ◦fenc(x) ∈R be the reconstructed point and z = fenc(x) ∈L the latent representation of said point. By construction, the image of the autoencoder fdec◦fenc : I→R is at most a ddimensional subspace of R. For the purposes of this paper, the autoencoders in question are trained to minimize the reconstruction loss, i.e. to minimize the average Euclidean distance between the input and the corresponding reconstruction point. A. Neural Networks and their Jacobians In this paper, we think of any neural network as the composition of the functions represented by the weight matrices, bias vectors, and the activation functions in each layer. In other words, if a neural network has an N dimensional input space and an M dimensional output, we may represent it as a multi-variable function, NN : RN → RM. The Jacobian matrix of a multi- variable function is the matrix of gradient vectors of each of the components of the function, and is represented as a M ×N matrix with function valued entries. In general, this Jacobian matrix cannot be represented or computed in function form, however, one can always talk about its the value of the Jacobian matrix at any given input point x, JNN(x). This matrix represents a linear transformation from the tangent space the input space at x to the tangent space of the output space at the output point y = NN(x). One may always study the singular value decomposition of JNN(x) to ﬁnd an orthonormal basis of the tangent space Tx(RN) and understand how it transforms in terms of an orthonormal basis of Ty(RM). Note that the Jacobean matrix is only deﬁned where the function is differentiable. In the case of neural networks with ReLU activation functions, as is the case for the autoencoders considered in this paper, the functions are piece wise linear. Therefore, they are not differentiable on a space of measure 0 in RN. In other words, the matrix JNN(x) is not deﬁned with probability 0. In the special case of N = M, as is the case for autoencoders, one may also consider the eigenvalues and eigenvectors of the Jacobian matrix, which quantiﬁes the directions and amounts in which the map JNN(x) dilates or contracts the tangent space Tx(RN). This is fundamentally different from a singular value decompo- sition of JNN(x) as it identiﬁes a common basis for the two tangent spaces, Tx(RN) and Ty(RN), given by the eigenvectors. In other words, the eigenvalues gives insight into the local deformations of the input space as the function learned by the autoencoder maps it to the reconstruction space. The Jacobian matrix of the autoencoder is computed by JI := D ( fdec ◦fenc ) (x) = Dfdec(z) ·Dfenc(x). See Appendix A for details. For any matrix square matrix A, let ⃗λA be the vector of norm decreasing eigenvalues of A. Note that the image of the autoencoder in the recon- struction space is at most d dimensional, for L= Rd. Therefore, for any point x ∈I, the vector ⃗λJI(x) will have at most d non-zero entries, each indicating the ammount of stretching or warping that the corresponding eigenspace of the data point (as a subspace of TxI) undergoes. In particular, the eigenspace of 0 corresponds to the kernel of JI(x). Geometrically, these are the directions of the input space that are collapsed to the origin in the image of the autoencoder. Calculating the eigenvalues of JI(x) is computationally expensive, as algorithms for calculating eigenvalues of an n×n matrix are O(n3) [15]. Therefore, it is easier to calculate the eigenvalues of the Jacobian of a related function, JL:= D ( fenc◦fdec ) (x) =Dfenc(y) ·fdec(z) which represents the deformations of the latent space under the action of the autoencoder composed in the opposite order. In general, there is no reason to expect that these two Jacobians should have any similarity in their eigenvalues. However, in [9], the authors show that when the the reconstruction error is 0,the eigenvalues of JL are similar to the eigenvalues of JI. Theorem I.1. If fdec ◦fenc(x) = x, then for z = fenc(x), thednonzero eigenvalues ofJI(x) are the same as the eigenvalues ofJL(z). 2The proof of this theorem is given in [9], but is included in Appendix A for completeness. Empirically, we see that when the reconstruction error is small, these values are close, but not identical. This property will not hold when the reconstruction error is large (see Figure 1). B. Probing the data manifold structure The Jacobians JI(x) and JL(z) give insight into how well an autoencoder is learning the data manifold un- derlying the training data. The data manifold conjecture simply states that, for any given data set of related objects D, the data actually lies noisily around some low ( δ) dimensional manifold MD [16], [17]. If a neural network learns a good representation of the data, then it learns the structure of MD. In topological terms, learning the structure of MD consists of learning a diffeomorphism φU from an open set U⊂ MD to Rδ. Ideally, if δ > d, then the learned function restricted to the data manifold, fenc(MD) is the composition of φU with a projection onto the latent space: π : Rδ →Rd. If the latent space has greater di- mension than the data manifold ( δ≤d) then fenc(MD) is the composition of φU with an embedding into the latent space: π: Rδ ↪→Rd. The function fdec(L) is the inversion of this function. The commutative diagram for this interaction is shown in display (1). The function fenc is not just deﬁned on the points near MD, but on all of I. Little is understood about the geometry of the neural network fenc on points far away from the data manifold. Let M⊂L be the image of MDunder fenc. Since the input space has much greater dimension than the latent space, N >> d, fenc has a large kernel. That is, many out of distribution points will map onto M. Then fdec maps all these points to the the same reconstruction point. Theorem I.2. Let x ∈I be an input point with latent representation, z′= fenc(x), is close to a pointz∈M in the image of the data manifold in latent space. Then ||⃗λJL(z) −⃗λJL(z′)||2 ≤||JL(z′) −JL(z)||2 F = (O(∥z′−z∥2))2 . Therefore, the distance between the eigenvalue vectors of JL(z′) and JL(z) gives a lower bound on the Frobenius norm of difference of the Jacobean matrices. Namely, even if an out of distribution point is reconstructed to an element of fdec◦fenc(MD) ⊂R, the eigenvalues of JL(z′) gives information about its latent representation. C. In distribution eigenvalue behavior Let Dtrain be the training data set, and MDtrain the corresponding δtrain dimensional data manifold. Min- imizing the reconstruction error ensures the the auto- encoder is a good zeroth order model for a sample of points in and around MDtrain. Studying the eigenvalues of the Jacobian JI(x) gives information about how well the autoencoder performs as a ﬁrst order approximation of data manifold,locally around x. For instance, suppose d ≤δtrain. If the top d eignevalues of JI(x) are one, then the auto-encoder has projected exactly onto the manifold MDtrain at each input point x. If the arguments of the eigenvalues of JI(x) are 0, then there is little rotation locally in passing from each input to reconstruc- tion point. If the product of the top d eigenvalues of JI(x) > 0, then there is no local orientation reversing behavior the input point. Furthermore, for δtrain <d, if the autoencoder projects onto the tangent of the data manifold, then one would expect to see d−δtrain 0 eigenvalues for each JI(x). At the the other extreme, if the autoencoder projects onto all of L, instead of onto a δtrain dimensional submanifold, then one would expect to see the top d eigenvalues of JI(x) and JL(z) remain at 1 for all d. D. Out of distribution eigenvalue behavior Let Dbe a fundamentally different data set than Dtrain. For x∈D, one cannot use the norms of the eigenvalues of JI(x) alone to understand whether or not fenc is projecting Tx(I) onto the tangent space of point on MDtrain. To see why, consider the points on the inside and the outside of a d dimensional unit sphere, Sd embedded inside RN. Consider the points that lie on the rays emanating from the origin through Sd in RN. For points lying close to the origin, any projection onto the surface of the sphere will have the properties that a small perturbation in the domain will result in a large perturbation on the sphere. I.e. the eigenvalues will be large. For projections of the points on the rays on the outside of the sphere, however, a large perturbation in the domain will correspond to a small perturbation on the surface. I.e. the eigenvalues will be small. Furthermore, the average size of the argument of the eigenvalues and the sign of the product of the eigenvalues of JI(x) has structure if the function fenc(x) factors through a projection onto MDtrain. Theorem I.3. Let x∈D be far away from the data man- ifold MDtrain. If the functionfenc(x) factors through a 3parallel transport ontoMDtrain, the sign of the product of the non-zero eigenvalues will be positive (the Jacobian does not reverse orientation fromTxIto TyR), and the arguments of the eigenvalues will be zero (the Jacobian does induce a rotation fromTxIto TyR). Finally, it is worth noting that an autoencoder trained on MDtrain will not capture information on the geometry of MD. For instance, there is no reason for an autoencoder fdec◦fenc that has learned the geometry of MDtrain (as outlined in Section I-C) will have captured any informa- tion about the geometry of a distant evaluation manifold MD. For instance, the appearance of zero eigenvalues after latent dimension d in the Jacobean JI(x), for x ∈ Dshould not indicate that MD has dimension d. Rather, it indicates that the trained autoencoder is keeping track of fewer than d of the dimensions of the input space near x. E. Observed behavior In this paper, we observe that for an x ∈ MDtrain, the autoencoder displays several properties one would expect if the map fdec ◦fenc(x) induced a projection onto TyMDtrain. Namely, the norm of the eigenvalues are, on average, less than one, but remain relatively close to it for both JI(x) and JL(z). Furthermore, as the latent dimension increases, the distribution of eigenvalues develop increasingly long left tails on a log scale, see Figure 3. That is, the proportion of eigenvalues with small norms increases, though not on as quickly as one would expect for a projection. Furthermore, the arguments of the eigenvalues are small, as are the proportion of points with locally orientation reversing behavior. This is consistent with work studying autoencoders as an iterative system, where the output of a trained autoencoder is evaluated again by the same autoencoder, where one ﬁnds that the outcome is not the same, but does converge [18]. Furthermore, this behavior is consistent across autoencoder architecture. However, this behavior is not observed for data sets that are far out of distribution. For instance, we note that, for JL, the arithmetic mean of the eigenvalues of the out of distribution points is frequently quite different from the arithmetic mean of the eigenvalues of the in distribution points (Figure 2), indicating that Frobenius norms of the two matrices are large (by Theorem I.2), even when, visually, the reconstruction image appears to be in MDtrain. In other words, even when the recon- structed image appears to be in the reconstruction of the training manifold, the latent representation of the out of domain point can be quite different from the latent representations of the training data. Furthermore, when the latent dimension is large, about half of the out of distribution data points show orien- tation reversing behavior by JI(x) and the eigenvalues have a larger range of arguments than in the training data, indicating that the map fenc does not factor through a parallel transport onto MDtrain. Since approximately 50% of the points display orientation reversing data, the autoencoder has not learned anything about the orienta- tion of the tangent space, TxI, far out of distribution. II. E XPERIMENT For this paper, we trained nineteen autoencoders on the MNIST data set, each with different latent dimensions, but otherwise identical architectures. We then evaluated these nineteen neural networks on 10 different out of distribution data sets, each consisting of 70,000 draws from different random distributions on the pixel values. Henceforth, we refer to the full MNIST data set as Dtrain, even though the training was done on the stan- dard 60,000 training points of the 70,000 point sample. We observe several notable empirical results. First, at low latent dimension, the autoencoders reconstructs the out of domain data as MNIST images, though incon- sistently. As the latent dimension increases, the au- toencoders stop reconstructing the data sets at MNIST images, but rather reconstruct them as generalized char- acters. It is interesting to note, however, that while the autoencoder seems to have learned some general features of written characters from the MNIST data set, there is no consistency of the reconstructions across either architectures or seeds, indicating the instability of network performance on out of domain data. As further evidence of the instability of the autoencoders, we note variation in the arithmetic and geometric means of the eigenvalues, and that the standard deviation of the log normal of the eigenvalues is, in general greater for the out of distribution data than for the training data (see Figure 4). We consider the log normal of the eigenvalues rather than just the eigenvalues themselves in order to capture the expectation that as latent dimension increases past the intrinsic dimension of MDtrain. While one expects the emergence of more zero (or very small in norm) eigenvalues, which we do not observe. The increased variation within each autoencoder indicates a lack of consistency of the eigenvalues of JI(x) on each out of domain point x. The increased variation across 4autoencoders is further evidence of the instability of network performance on out of domain data. Finally, inspite of the instability or reconstructions across architectures, certain patterns still emerge. Namely, the arithmetic and geometric means of the eigenvalues clus- ter according to the means of the distributions from which they are drawn. In other words, while the autoen- coders do not geometrically encode much data about out of distribution points, they do learn information about the mean pixel values. A. Data and architecture For this paper, we trained nineteen unregularized au- toencoders on the MNIST data set on two seeds. While we are aware that there are many regularization pro- cesses that will improve the accuracy and/ or gener- alization abilities of out autoencoder, we intentionally work with unregularized networks as these have the easiest to interpret geometric properties. Each autoen- coder has four identical layers in the encoder and de- coder differeing only by the dimension of the latent dimension: the encoder half consists of layers mapping between spaces of dimension (784,128,64,32,d), and the decoder half consists of layers between dimensions (d,32,64,128,784), with d∈{2,..., 19}. The MNIST images are 28x28 arrays ﬂattened to a vector for input. Thus the input and reconstruction spaces are I= [0,1]784 = R. Furthermore, the decoder has a ﬁnal tanh function to renormalize the output to a vector with values in the interval [0,1]. The activation function for all layers is ReLU, making the autoencoder represent a piece wise linear function, and is trained for 300 epochs, with seeds 0 and 1. After training, we evaluate 10 diferent samples of 70,000 datapoints drawn from 10 different out of distribution data sets, each deﬁned by a random distribution on I, with different means: truncated normal between 0 and 1, (mean 0.5), uniform over [0,1] (mean .5), 4 Bernoulli distribution with means 0.13, 0.25, 0.5, 0.75, and 0.87, and three Beta distributions one with α = .8, β = 5, mean .14, another with α = .5, β = .5, mean .5, and the ﬁnal with α = 5, β = .8, mean = .86. We refer to the data sets as Dnorm, Dunif, Dp for the 4 Bernoulli distributions (where p∈{.13,.25,.5,.74,.87} and Dα,β for the three Beta distributions (where (α,β) ∈ {(.8,5),(.5,.5),(5,.8)}). We note that the mean value for the MNIST data set is .13, with most of the the values being 0 (black), and a small percentage being near 1 (white). While the location of the bright pixels is clearly important for the classiﬁer, we chose D.13 and D.8,5 to mimic this behavior. The data sets D.87 and D5,.8 are chosen to reverse this behavior. All other distributions are chosen to interpolate between these two extremes. B. Geometry of out of distribution data As we draw the out of distribution data from well deﬁned distributions, we may characterize this data in terms of several data manifolds. Most simply, we may consider each data set as being noisily distributed around a 0 dimensional manifold, the point where each pixel has the mean brightness. In this manner, we have 10 different 0 dimensional data manifolds, indicated as above as MDp, MDα,β, MDnorm and MDunif. Note that given the similarities of the means, from this perspective, the manifolds MD.13 and MD.8,5 are close; MDnorm, MDunif and MD.8,5 , are the same; and MD.87 and MD5,.8 are close together. However, the corresponding data sets are different, as the sampling around the point corresponding to the manifold is different in each case. We may also consider these points to be lying on a larger data manifold parameterized by the properties of the distributions. Allowing pto vary from 0 to 1, we obtain a 1 dimensional data manifold MBernoulli, where each Dp is a sample of points distributed noisily (according to the appropriate Bernoulli distribution) about the point p ∈MBernoulli. Similarly, there is a two dimensional MBeta parametrized by 0 < α, βwhere each Dα,β is a sample of points distributed noisily according to the appropriate Beta distribution. Finally, Dnorm and Dunif come from tow points on the two dimensional manifold Mnorm, parametrized by µand σwith data is distributed around it according to the truncated normal distribution with mean 0 <µ< 1 and 0 <σ <∞: Dnorm samples around the point µ = .5, σ = 1 and Dunif around µ= .5, σ= ∞. Note that, as discussed in Section I-D, the eigenvalues of the JI(x) do not capture the intrinsic dimension of the out of distribution data. Namely, one does not observe the emergence of 0 (or small normed) eigenvalues at low latent dimension, as one would expect if the autoencoder had learned the intrinsic dimension (see Figure 3). C. Reconstruction images Looking at these 10 data sets, evaluated on the 19 different autoencoders, we ﬁnd that for each autoencoder 5with large enough latent dimension, the reconstructed images can qualitatively be placed into two groups, those with mean less than .5, and those with means at least .5 (see Tables I and II). Examining the behavior of the eigenvalues, we get a ﬁner granularity of three mean based clusters: those with mean less than .5, those with mean .5 and those with mean greater than .5 (see Figure 2). Because of this behavior, in the tables and ﬁgures below, we only present the data from a low mean, or dark, data set, D.13, the data from a moderate meaned data set Dnorm and the data from a high meaned, or bright, data set , D5,.8. Visually, there are two things to note. First, for a given autoencoder with a large enough latent dimension, the reconstructed image appears be a generalized character, not a one from the MNIST data set. For lower latent dimensions, the reconstructed images appear uniformly to be a character from the MNIST data set. For very low latent dimensions, the 10 distributions are reconstructed as elements of the MNIST data set, but not uniformly. Visual similarity of the reconstruction images for each autoencoder implies that one may think of the image of all these data manifolds as lying in a subspace of R, Mdist,d, where d= dim(L). That is, for fenc,d and fdec,d the autoencoder with d dimensional latent space, we write fdec,d ◦fenc,d : MD∗ →Mdist,d for MD∗ corresponding to the Bernoulli, Beta or trun- cated normal distribution manifolds. It is striking to note that the manifold Mdist,d is a function of d, i.e. different for each architecture. Furthermore, it appears to be different for each seed. We posit that some of this observed behavior is due to the fact that at very low latent dimensions, there are not enough degrees of freedom in the latent space for the autoencoder to have learned enough features to be able to do anything but noisily assign out of distribution objects similarly. As the degrees of freedom increase, the autoencoders seem to ﬁrst be able to identify the out of distribution elements as similar. However, these systems do not have the potential to reconstruct images as anything other than those that are in distribution. Eventually, for high enough d, the autoencoder has enough degrees of freedom to be able to reconstruct the out of distribution image as a different out of distribution image. How it does it, however, is my no means consistent across systems. Fig. 1: The ratio of the L2 norm of the difference in eigenvalues to the latent dimension for Dtrain, D5,.8, Dnormal and D.13. The reconstruction errors are only small for Dtrain, therefore, the distance ∥⃗λJI(x) − ⃗λJL(z)∥2/d is larger the out of domain data. D. Eigenvalue behavior Next, we study the behavior of the eigenvalues of JI(x) and JL(z). By Theorem I.1, if the reconstruction error is small, we expect the vectors ⃗λJI(x)−⃗λJL(z)o be small. When the reconstruction error is large, we expect to see no such pattern. This is borne out in Figure 1. For the in distribution data, the arithmetic and geometric means of the eigenvalues are near 1, with a slow decline as the latet dimension increases (see Figure 2). The similarity of scale between the arithmetic and geometric means indicates that eigenvalues do not contain extreme outliers at any latent dimension. As discussed in Section I-C, this indicates that the autoencoder is learnining a function that is close to a projection onto the data manifold. At high latent dimenion, the autoencoder is not learning to ignore certain dimensions of the data (as evidenced by the failure of 0 eigenvalues to appear in signiﬁcant quantities, see 3), but the decrease in mean eigenvalue indicates that the autoencoder has learned to contract certain dimensions. The fact that, at higher latent dimension, the autoencoder retains some information about all dimensions of the latent space (rather than collapsing them altogether) is consistent with the fact that at higher latent dimension, the out of domain data is reconstructed as generalized characters. In other words, this is exactly the behavior one would expect if the autoencoder is learning (and therefore reconstructing to) more than just the MNIST digits. For the out of distribution data the arithmetic and geo- metric means for both JI and JL are far less constant than those of the in distribution data. This architecture 6Fig. 2: The arithemetic and geometric means for Dtrain and the 10 out of distribution evaluation sets. Eventually, the mean eigenvalues for the out of distribution data sets drop below those of Dtrain. The mean ⃗λJI(x) cluster for the out of distribution data sets in a way that the ⃗λJL(x) does not. dependent variation of the eigenvalues of the autoen- coder is evidence of the geometric instability of the autoencoder on out of distribution data. However, for large enough latent dimension, there are three distinct clusters of eigenvalues of JI: the three darkest distribu- tions(those with the lowest mean pixel value) have the highest average eigenvalue, followed by the distributions with a mean of .5, and the three with the highest mean pixel value have the lowest mean eigenvalue. Inspite of the apparent learning, most of the eigenvalue information points to a low level of geometric stability on out of domain data. At low latent dimension, when the reconstructed images appear to lie in MDtrain for all out of distribution data sets, the mean eigenvalues of JL for many out of distribution data sets are not close to the mean eigenvalue for the training data. By Theorem I.2, the squared distance between the vectors of eigenvalues ∥⃗λJL(z) −⃗λJL(z′)∥2 is a lower bound for the distance squared between the latent representation z,z′ ∈L. A large difference in means (as observed in Figure 1) implies that the latent representations of the points in certain data sets are, on average, far away from the latent representations of the points in the training data, even though the reconstructed images are similar. Figure 3 shows the distribution of eigenvalues in the training data, representative bright, moderate and dark data sets. Note that for each of the out of distribution data set, the variance of the distribution of logarithms of the norms of the eigenvalues in any given latent dimension, as well as the medians of the same across Fig. 3: The box plots for log(|⃗λJI(x))i| (blue) and log(|⃗λJI(x))i| (red). The medians of the distributions any given latent dimension varies more than for the in distribution data. Fig. 4: The standard deviations of the log |λi|distribution for every out of domain data set is higher than the stan- dard deviations for the training data in JI(x), showing the geometric instability of the autoencoder on out of doman data. latent dimension is much greater than the same variance for the in distribution data (see Figure 4). This again points to the geometric instability of the autoencoder far from the training manifold. Finally, we note that, by Theorem I.3 autoencoder does not map points in I far from the training manifold onto MDtrain via an orientation preserving, non-rotating map (such as a parallel transport in Iin the Euclidean metric). Notably, on the training data, the distribution of arguments of the eigenvalues is small (see Figure 6) as is the proportion of points for which the orientation is reversed (see Figure 5). However, for the out of distribution data, the arguments for JI(x) can be any angle for the bright data sets. The range of possible angles decreases with range, but even for the dark data sets, is consistently larger than for the training data. Similarly, for the all but the darkest data sets, at high enough latent dimension, the matrices JI(x) reverse the orientation of Tx(I) approximately half the time. Given the general instability across architectures of other 7Fig. 5: The proportion of points for which the product of the top d eigenvalues of JI(x) (left) or JL(z) (right) is negative, indicating that orientation of coordinate system has been ﬂipped. There are very few of these points for Dtrain, while for the out of distribution data, the proportions are near half for JI(x). Fig. 6: The box plots for |Arg(⃗λJI(x))|) (blue) and |Arg(⃗λJI(x))|) (red). Note these arguments cover a signiﬁcantly wider range for the out of distribution data sets than the training data. geometric properties on out of distribution data, the consistency of this coin ﬂip on the orientation of the tangent space is remarkable. III. C OMPARISON WITH RELATED WORK The machine learning community is not a stranger to the differential geometry literature. There is a large body of work on learning metric functions on the latent spaces for (variational) autoencoders and to determine the reconstruction loss [19], [20], [21], [22], [23]. Others propose building the reconstruction error out of the Mahalanobis distance function [14] to better detect out of distribution inputs. These, and other efforts, aim to use differential geometry tools to improve the performance of neural networks. Our work differs from these in that we are not interested in performance enhancing method- ologies, but in understanding the geometry feature map so that we may explain how the feature map perceives and interacts with out of domain data. We note that Contraction Autoencoders (CAEs) use the Frobenius norm of the Jacobian of the encoder map to regularize the loss function [24]. CAEs and their higher order counterparts [25] are better at capturing smoother, more accurate and lower dimensional representations of the data manifold. As mentioned above, we explic- itly avoid regularization techniques in the autoencoders trained for this paper in order to better understand the geometry of the learned functions. Furthermore, the Ja- cobian matrices used in these cases were of the encoder maps only, while our work focuses on the Jacobian matrix for the two networks combined. IV. C ONCLUSION In this paper, we study the learned geometry of a family of trained autoencoders, on input points far away from the data set on which the system was trained. Instead of studying the performance outputs of the autoencoders (which we expect to be poor) we look at the geometric features of the autoencoder, viewed as a function from input to reconstruction space. In doing so, we make the following observations. From the point of view of the reconstructed images, at low latent dimension the data sets of randomly distributed pixel values are all mapped to images that appear to come from the training data set. However, which class of training image is reconstructed is not consistent neither within random data set, nor across random data sets, nor across seeds. At higher latent dimensions, the reconstructed images across all data sets in a given architecture and seed appear to be a generalized characters. However, this character is not consistent across architecture or seed. From a geometric perspective, we show that while, for lower latent dimension, the reconstructed images appear to be the same as those in the training data, the latent rep- resentations of the training and out of distribution points are quite different. Furthermore, we ﬁnd evidence that that geometries of the learned functions (as measured by the eigenvalues of JI(x) cluster according to the mean value of the pixel distribution. Finally, and possibly most surprisingly, we observe that for higher latent dimension, the trained autoencoder is not aware of the orientation of the bases of the input space. We contend that studying the geometry of the trained autoencoder will give insight into how the network is processing both in and out of distribution data, and will play a key role in explaining the phenomenon of out of distribution reconstruction. 8APPENDIX COMPUTING THE JACOBIANS Explicitly, for the autoencoder structure used in this paper we may write fenc as a composition of the weight matrices and activation functions at each layer. That is, fenc = fenc,4 ◦fenc,3 ◦fenc,2 ◦fenc,1 fdec = fdec,4 ◦fdec,3 ◦fdec,2 ◦fdec,1 where fenc,i(x) =    ReLU128(A128×784x+ b128) i= 1 ReLU64(A64×128x+ b64) i= 2 ReLU32(A32×64x+ b32) i= 3 Ad×32x+ bd i= 4, fdec,i(x) =    ReLU32(A32×dx+ b32) i= 1 ReLU64(A64×32x+ b64) i= 2 ReLU128(A128×64x+ b128) i= 3 tanh784(A784×128x+ b784) i= 4, . Here, each An×m is a real n×m matrix, bm is a m dimensional column vector, and ReLUm : Rm →Rm is the vector valued function that is ReLU in each component (similarly for tanhm). The Jacobians for the autoencoders JI(x) and JL(z) are computed locally by repeated application of the chain rule through both the encoder and decoder layers. Note that the derivative of a layer is applied to the output from the last layer: F = Fn ◦···◦ F1, then ∂F = ∏ ∂Fi|Fi−1◦···◦F1(x) . Explicitly the derivative of a afﬁne transformation is the associated matrix is ∂(Ax+ b) ∂x = A. Recall that the ReLU function is, component wise the piece wise linear function ReLU(x) = { 0 x< 0 x x ≥0 . The derivative of the ReLU function applied to an N dimensional vector is the Heaviside function embedded along the diagonals of an N ×N: ∂ReLU(x) ∂x = H(x) = { 1, x ≥0 0, x< 0 . and the derivative of the tanh function applied to an N dimensional vector is the derivative embedded along the diagonals of an N ×N dimensional matrix is ∂tanh(x) ∂x = 1−tanh2(x) Note that in the autoencoder, the ReLU and the tanh are applied component wise to the outputs of the previous layer. Thus the corresponding term in the Jacobian cal- culation is a matrix with the functions H(x) or tanh(x) along the diagonals. As a particular example, we can compute that the deriva- tive of fenc,1 is ∂fenc,1(x) =H(A128×784x+ b)I128A128×784 and the other derivatives can be computed similarly, as can the derivatives of compositions (using the chain rule). GEOMETRIC RESULTS Proof of Theorem I.1 Proof. This is a result of the chain rule. Since fdec ◦fdec(x) = x, let w be an eigenvector of JI(x) with eigenvalue λ. Then ∂fdec|z ◦∂fenc|x(w) =λw, where z= fenc(x) as above. If v= ∂fenc|x(w) then ∂fenc|x ◦∂fdec|z(v) =∂fenc|x ◦∂fdec◦ ∂fenc|x(w) =∂fenc|x(λw) =λ(v) . Topological interpretation of an autoencoder MD⊃UD φU → → ↙ ↖  ↓ ↓  Rδ π ↘ ↘  Rδ π ↙ ↙  UD⊂MD φU← ← ↙ ↖  ↓ ↓  I fenc → → L fdec → → R (1) Display (1) shows how an autoencoder learns from a topological point of view. A datamanifold ( MD) lies with a complicated embedding inside the input space I. The actual data set ( D) lies on and around MD. We say that it is distributed noisily around MD, however, we make no statements about the properties (such as 9higher moments or homoskedasticity) of this distribution. The autoencoder is supposed to approximate MD using piecewise linear maps, and reconstruct this piecewise linear approximation in R. This learning process is shown in the bottom row. If an autoencoder has correctly learned the manifold structure of MD, then the compo- sition on the bottom row, restricted to the manifold is the identity: fdec ◦fdec(MD) =I . Note, this identity should hold everywhere on MD, speciﬁcaly, we do not expect it to hold on the D, which are, with high probability, not on the manifold. In other words, for a point x on MD, the Jacobian matrix JI(x) should be the identity on the tangent space of MD, Tx(MD) ⊂Tx(I) and the normal space, T⊥ x (MD) ⊂Tx(I), should be its kernel. If the autoencoder learns the manifold MD, then it has learned an atlas on MD. Any δ dimensional manifold is deﬁned by a set of open sets Ui such that their union gives the manifold ∪iUi = MD, and a set of diffeomorphisms φi : Ui →Rδ. Each pair (Ui,φi) is called a chart on MD. The set of charts is called an atlas. If an autoencoder learns the structure of MD, it has learned a chart on MD, call it φ(U). The open set U is an open set that contains all the points on the manifold, as well as the projection of the data points off the manifold onto it. The image φ(U) is in Rδ. If d ≥ δ, this is embedded into the latent space (the map π in dispaly 1). If d < δthen the map π is a projection of φ(U) onto Rd. In actuality, the map π is never a projection, or an embedding. The autoencoder learns to ﬁt the data as best it can with the d degrees of freedom that is is given. Therefore, when d < δ, the autoencoder minimizes its loss function, not by projecting onto a lower dimensional space as it would do if it had learnt the manifold structure, but by ﬁnding a d dimensional submanifold of MD that better ﬁts the data. Similarly, when d > δ, the autoencoder minimizes its loss function by using the extra dimensions to ﬁt the data. In doing so, it has learned a manifold that is too large, and allows for phenomenon such as out of domain reconstructions and reconstruction to generalized characters, as seen in this paper. Proof of Theorem I.2 Proof. Recall that the Frobenius norm of an n×nmatrix, A, can be written ∥A∥2 F = min m,n∑ i=1 σi(A)2 where σi(A) denotes the ith singular value of A (in decreasing order). Weyl’s majorization theorem gives, for any p >0 and any 1 ≤k≤n that k∑ i=1 |λi|p ≤ k∑ i=1 σp i. (2) The ﬁrst inequality comes from setting p= 2. The equality is a result of Taylor’s theorem. Proof of Theorem I.3 Proof. Display (3) gives the commutative diagram if fenc(x) factors through a projection onto MDtrain: I fenc → →  π ↘ ↘  L fdec → → R MDtrain fenc ↗ ↗ , (3) where π denotes a paralell transport of x onto the data manifold. Note that for x′ ∈MDtrain, JI(x′) has eigenvalues of 1 or 0. Therefore, the sign of the products of the non-zero eigenvalues will be 1. Similarly, the ar- guments of the eigenvalues of JI(x′) are 0. Furthermore, as geodesics in Euclidean space are straight lines, any parallel transport of x to x′, will not introduce rotation (including orientation reversal) of the of the basis of TxI . Therefore, the composition fenc : I → Lwill not introduce rotation or orientation reversing behavior. 10ADDIONAL FIGURES REFERENCES [1] T. Romm, “Amazon’s facial-recognition tool misidentiﬁed 28 lawmakers as people arrested for a crime, study ﬁnds,” Washington Post, July 26, 2018. [2] B. Wilson, J. Hoffman, and J. Morgenstern, “Predictive inequity in object detection,” arXiv preprint arXiv:1902.11097, 2019. [3] E. Nalisnick, A. Matsukawa, Y . W. Teh, D. Gorur, and B. Lakshminarayanan, “Do deep generative models know what they don’t know?” arXiv preprint arXiv:1810.09136, 2018. [4] J. D. Havtorn, J. Frellsen, S. Hauberg, and L. Maaløe, “Hierarchical vaes know what they don’t know,” arXiv preprint arXiv:2102.08248, 2021. [5] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks,” arXiv preprint arXiv:1610.02136, 2016. [6] S. Liang, Y . Li, and R. Srikant, “Enhancing the reliability of out-of-distribution image detection in neural networks,” arXiv preprint arXiv:1706.02690, 2017. [7] T. DeVries and G. W. Taylor, “Learning conﬁdence for out-of-distribution detection in neural networks,” arXiv preprint arXiv:1802.04865, 2018. [8] D. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, D. Zhang, R. Le Priol, and A. Courville, “Out-of-distribution generalization via risk extrapolation (rex),” in International Conference on Machine Learning. PMLR, 2021, pp. 5815–5826. [9] S. Agarwala, B. Dees, A. S. Gearheart, and C. Lowman, “Geometry and generalization: Eigenvalues as predictors of where a network will fail to generalize,” arXiv:2107.06386. [10] Y . Zhao, B. Deng, C. Shen, Y . Liu, H. Lu, and X.-S. Hua, “Spatio-temporal autoencoder for video anomaly detection,” in Proceedings of the 25th ACM International Conference on Multimedia, ser. MM ’17. New York, NY , USA: Association for Computing Machinery, 2017, p. 1933–1941. [Online]. Available: https://doi.org/10.1145/3123266.3123451 [11] S. Liang, Y . Li, and R. Srikant, “Enhancing the reliability of out-of-distribution image detection in neural networks,” in International Conference on Learning Representations, 2018. [Online]. Available: https://openreview.net/forum?id=H1VGkIxRZ [12] S. Yoon, Y .-K. Noh, and F. C. Park, “Autoencoding under normalization constraints,” arXiv preprint arXiv:2105.05735, 2021. [13] Z. Xiao, Q. Yan, and Y . Amit, “Likelihood regret: An out-of-distribution detection score for variational auto-encoder,” arXiv preprint arXiv:2003.02977, 2020. [14] T. Denouden, R. Salay, K. Czarnecki, V . Abdelzad, B. Phan, and S. Vernekar, “Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance,” arXiv preprint arXiv:1812.02765, 2018. [15] B. Parlett, “The qr algorithm,” Computing in Science Engineering, vol. 2, no. 1, pp. 38–42, 2000. [16] Y . Bengio, A. Courville, and P. Vincent, “Representation learning: A review and new perspectives,” IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798–1828, 2013. [17] G. Alain and Y . Bengio, “What regularized auto-encoders learn from the data-generating distribution,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 3563–3593, 2014. [18] A. Radhakrishnan, M. Belkin, and C. Uhler, “Downsampling leads to image memorization in convolutional autoencoders,” CoRR, vol. abs/1810.10333, 2018. [Online]. Available: http://arxiv.org/abs/1810.10333 [19] G. Arvanitidis, L. K. Hansen, and S. Hauberg, “Latent space oddity: on the curvature of deep generative models,” arXiv preprint arXiv:1710.11379, 2017. [20] G. Arvanitidis, S. Hauberg, and B. Sch ¨olkopf, “Geometrically enriched latent spaces,” arXiv preprint arXiv:2008.00565, 2020. [21] D. Eklund and S. Hauberg, “Expected path length on random manifolds,” arXiv preprint arXiv:1908.07377, 2019. [22] S. Hauberg, “Only bayes should learn a manifold (on the estimation of differential geometric structure from data),” arXiv preprint arXiv:1806.04994, 2018. [23] A. Tosi, S. Hauberg, A. Vellido, and N. D. Lawrence, “Metrics for probabilistic geometries,” arXiv preprint arXiv:1411.7432, 2014. [24] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y . Bengio, “Contractive auto-encoders: Explicit invariance during feature extraction,” in Icml, 2011. [25] S. Rifai, G. Mesnil, P. Vincent, X. Muller, Y . Bengio, Y . Dauphin, and X. Glorot, “Higher order contractive auto-encoder,” inJoint European conference on machine learning and knowledge discovery in databases. Springer, 2011, pp. 645–660. 11Bright (D5,.8) Moderate ( Dnormal) Dark ( D.13) d = 2 d = 3 d = 4 d = 5 d = 6 d = 7 d = 8 d = 9 d = 10 d = 11 d = 12 d = 13 d = 14 d = 15 d = 16 d = 17 d = 18 d = 19 d = 20 TABLE I: Reconstructions of a bright distribution ( D5,.8), and moderate meaned distribution ( Dnormal) and a dark distribution (D.13) for seed 0. Note that by latent dimension 12, all of the reconstructioned images look like the same genearlized (non-MNIST) character. For low latent dimension, there is variation in the reconstructed images, which all look like MNIST characters. For moderate latent dimensions, the variations between classes has disappeared, but the reconstructions still look like MNIST characters. 12Bright (D5,.8) Moderate ( Dnormal) Dark ( D.13) d = 2 d = 3 d = 4 d = 5 d = 6 d = 7 d = 8 d = 9 d = 10 d = 11 d = 12 d = 13 d = 14 d = 15 d = 16 d = 17 d = 18 d = 19 d = 20 TABLE II: Reconstructions of a bright distribution ( D5,.8), and moderate meaned distribution ( Dnormal) and a dark distribution (D.13) for seed 1. Note that for high latent dimension, all of the reconstructioned images look like the same genearlized (non-MNIST) character, but that these characters are different from those in seed 0 (shown in Figure I.) 13",
      "references": [
        "Amazon’s facial-recognition tool misidentiﬁed 28 lawmakers as people arrested for a crime, study ﬁnds",
        "Predictive inequity in object detection",
        "Do deep generative models know what they don’t know?",
        "Hierarchical vaes know what they don’t know",
        "A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks",
        "Enhancing the reliability of out-of-distribution image detection in neural networks",
        "Learning conﬁdence for out-of-distribution detection in neural networks",
        "Out-of-distribution generalization via risk extrapolation (rex)",
        "Geometry and generalization: Eigenvalues as predictors of where a network will fail to generalize",
        "Spatio-temporal autoencoder for video anomaly detection",
        "Autoencoding under normalization constraints",
        "Likelihood regret: An out-of-distribution detection score for variational auto-encoder",
        "Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance",
        "The qr algorithm",
        "Representation learning: A review and new perspectives",
        "What regularized auto-encoders learn from the data-generating distribution",
        "Downsampling leads to image memorization in convolutional autoencoders",
        "Latent space oddity: on the curvature of deep generative models",
        "Geometrically enriched latent spaces",
        "Expected path length on random manifolds",
        "Only bayes should learn a manifold (on the estimation of differential geometric structure from data)",
        "Metrics for probabilistic geometries",
        "Contractive auto-encoders: Explicit invariance during feature extraction",
        "Higher order contractive auto-encoder"
      ],
      "meta_data": {
        "arxiv_id": "2201.11902v1",
        "authors": [
          "Susama Agarwala",
          "Ben Dees",
          "Corey Lowman"
        ],
        "published_date": "2022-01-28T02:30:22Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies geometric behavior/instability of the learned autoencoder map under severe distribution shift. Trains a family of MNIST autoencoders varying only latent dimension and seed, then probes 10 synthetic out-of-distribution (OOD) datasets with random pixel values from different distributions. Main findings: (1) OOD inputs can reconstruct to MNIST-like digits at low latent dimension, but which digit varies by input/seed; at higher latent dimension each model maps each OOD dataset to a consistent “generalized character,” but this character is architecture- and seed-dependent. (2) Jacobian-eigenvalue analysis shows strong geometric instability on OOD inputs and across architectures: even when reconstructions look in-distribution, latent-space Jacobian spectra differ substantially from those of MNIST, implying latent representations are far from those of true MNIST points. (3) Eigenvalue statistics cluster primarily by mean pixel intensity of the OOD distribution. (4) For high latent dimension, the input-to-reconstruction Jacobian often reverses orientation about half the time on OOD data, suggesting the trained map is not aligned with a stable input-space basis/orientation far from the training manifold.",
        "methodology": "Treats the autoencoder f = f_dec∘f_enc as a differentiable (piecewise-linear) map R^784→R^784 and analyzes local geometry via Jacobians. Computes J_I(x)=D(f_dec∘f_enc)(x)=D f_dec(z)·D f_enc(x) and also J_L(z)=D(f_enc∘f_dec)(z)=D f_enc(y)·D f_dec(z) (cheaper, lower dimension). Uses theoretical link: when reconstruction error is zero, the d nonzero eigenvalues of J_I(x) match those of J_L(z); empirically compares spectra when reconstruction error is small vs large. Measures for each point/distribution/architecture: eigenvalue magnitudes (log|λ|), arithmetic and geometric means, arguments (complex phases), variance across points, and orientation reversal via sign of product of top-d eigenvalues. Interprets spectra in terms of projection onto/manifold learning of the MNIST data manifold and stability of the learned feature map on far-OOD inputs.",
        "experimental_setup": "Data: MNIST (28×28 flattened to 784), trained on standard 60k training set; evaluation includes MNIST plus 10 OOD datasets of 70,000 samples each where each pixel is i.i.d. drawn from a specified distribution on [0,1]. OOD distributions: truncated normal on [0,1] (mean 0.5), uniform[0,1] (mean 0.5), Bernoulli with p∈{0.13,0.25,0.5,0.75,0.87}, Beta with (α,β)∈{(0.8,5),(0.5,0.5),(5,0.8)}. Model family: 19 unregularized fully-connected ReLU autoencoders with identical layer widths except latent dimension d∈{2,…,19} (encoder 784→128→64→32→d; decoder d→32→64→128→784 with final tanh to [0,1]). Training: 300 epochs; two random seeds (0 and 1). Validation/analysis: qualitative inspection of reconstructions across d and seeds; quantitative spectral analysis of Jacobians for in-distribution vs each OOD dataset, including comparisons between J_I and J_L (especially the norm of eigenvalue-vector differences) and distributional statistics (means, boxplots, standard deviations, orientation reversal rate).",
        "limitations": "Scope limited to simple unregularized MLP autoencoders on MNIST; results may not transfer directly to convolutional, variational, or regularized autoencoders (e.g., contractive AEs) or to higher-resolution/natural image datasets. OOD data are synthetic i.i.d. per-pixel distributions; they test extreme shift but do not represent structured real OOD (e.g., different objects/domains). Heavy reliance on Jacobian eigenvalues is computationally expensive for J_I and uses J_L as a proxy whose theoretical equivalence holds only at zero reconstruction error; for large reconstruction error spectra may diverge, complicating interpretation. The analysis is primarily descriptive (statistics/visualization) without proposing or validating an OOD detector or downstream task performance metric. Orientation reversal interpretation assumes Euclidean parallel transport/intuitions; for piecewise-linear ReLU networks, non-differentiable measure-zero sets and region boundaries may affect local Jacobians. Limited seeds (2) and latent dimensions (2–19) constrain statistical generality.",
        "future_research_directions": "Extend geometric probing to other architectures (convolutional, variational, denoising/contractive/regularized AEs) and larger datasets to test whether instability/orientation effects persist and how regularization changes Jacobian spectra. Develop OOD detection or uncertainty measures based on Jacobian-spectrum features (e.g., spectral clustering by mean intensity, phase dispersion, orientation-flip rate, or distance between in-distribution and test eigenvalue statistics) and evaluate against established OOD benchmarks. Analyze why eigenvalue statistics cluster by mean pixel value—connect to bias/activation sparsity and to invariances of ReLU MLPs; study sensitivity to input basis via random orthogonal transforms and input normalization. Study the relationship between latent representations and reconstructions for OOD: characterize when “in-distribution-looking” reconstructions correspond to far latent codes; investigate mapping multiplicity/degeneracy of f_enc far from the manifold. Provide theoretical characterization of Jacobian behavior far from the data manifold for piecewise-linear networks (region geometry, expected orientation flips) and link to manifold learning conjectures. Explore computationally efficient approximations (Hutchinson trace estimators, randomized SVD) for Jacobian spectral summaries to enable scaling to higher dimensions and real-time monitoring.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Automatic Chain of Thought Prompting in Large Language Models",
      "full_text": "1    Testing System Intelligence    Joseph Sifakis,   Université Grenoble Alpes, Verimag Laboratory   Joseph.Sifakis@univ-grenoble-alpes.fr  Abstract  We discuss the adequacy of system intelligence tests and practical problems raised by their  implementation. We propose the replacement test as the ability of a system to replace  successfully another system performing a task in a given context. We show how this test  can be used to compare aspects of human and machine intelligence that cannot be taken  into account by the Turing test.  We argue that building sy stems passing the replacement  test involves a series of technical problems that are outside the scope of current AI.  We present a framework for implementing the proposed test and validating system  properties. We discuss the inherent limitations of AI system validation and advocate new  theoretical foundations for extending existing rigorous test methods.  We suggest that the replacement test, based on the compleme ntarity of skills between  human and machine, can lead to a multitude of intelligence concepts reflecting the ability to  combine data-based and symbolic knowledge to varying degrees.    1. Introduction   There is currently a confusion about what intelligence is and how it can be achieved. The  spectacular r ise of AI, is accompanied by a frenzy of optimism  fueled by the media and  large technology companies, who  through impressive large-scale projects, spread opinions  suggesting that human- level AI is only a matter of years away. The recent impressive  achievements of large language models  make some believe that machine learning and its  subsequent developments will enable us to meet the intelligence challenge – this is only a  matter of time!  Is it possible to define concepts of intelligence based on rigorous criteria that provide a  technical basis for judging the extent to which a system exhibits intelligent behavior by  being able to develop and apply trustworthy knowledge?    We discuss criteria for assessing  intelligent behavior of systems . We argue that the Turing  test fails to capture the many facets of human intelligence. We propose an alternative  operational definition of intelligence that compares the ability of an agent to successfully  replace another agent in the execution of a task.   The proposed replacement test  relativizes the concept of intelligence and allows a good  deal of freedom in its definition. I n particular , it allows us to understand human  intelligence as the result of the combination of a wide range of task skills . This view of 2    intelligence is adopted by autonomous sy stems intended to replace human agents   performing tasks in existing organizations , as envisioned by the I nternet of Things. Its  realization requires solving difficult technical problems not related to the system ’s ability  to behave intelligently.   Building autonomous systems is not a simple machine learning problem . We use the term  \"AI system\" for any type of system incorporating artificial neural networks, whether  machine learning system or autonomous system.  Finding adequate tests for system intel ligence raises a very challenging and topical  problem: providing evidence that a system meets desired properties. Given the inability to  apply model- based verification techniques to machine learning systems , empirical  property validation through testing se ems to be the only viable approach. Today, the  desired properties of AI systems go far beyond the technical properties sought by  traditional system development methodologies. The vision of responsible AI, or even AI  aligned with human values, stems from th e aspiration to emulate characteristics of human  behavior. We discuss the possibility of extending current validation techniques to AI  systems and highlight the limitations of a rigorous validation approach.    A comparison between machines and humans shows important differences  and  complementarity in their ability to develop and apply knowledge. Machines cannot match  human situational awareness, because human thinking is based on common sense  knowledge and can combine symbolic mental models with concrete sensory knowledge. In  addition, humans have a complex and poorly understood value- based decision system that  allows them to adapt by managing a wide variety of goals across a broad range of activity  domains.  On the other hand, machines can learn complex relationships and produce knowledge from  multidimensional data, while humans show very limited capabilities in this type of task.   We show that the combination of complementary cognitive abilities of machines and  humans leads to a vast space of possible intelligences, which calls for further exploration.    The paper is structured as follows.    In Section 2, we define the replacement test and show that it can capture essential aspects  of human intelligence. We discuss the technical  issues involved in building autonomous  systems and point out that they include challenging engineering problems that are outside  the scope of current AI.  In Section 3, we present the principles of a testing framework and discuss the limitations of  validating properties of AI systems that do not lend themselves to formalization. We  advocate new theoretical foundations for extending existing rigorous test methods to AI  systems.  In Section 4, we argue that the replacement test, based on the complementarity of skills  between humans and machines , can lead to a multitude of intelligence concepts reflecting  the ability to combine data-based and symbolic knowledge to varying degrees. 3    2. Deepening the concept of intelligence   2.1 The replacement test    The Turing test [1], often used to demonstrate that machines are intelligent, cannot  rigorously account for the many aspects of human intelligence. Humans can move, speak  and behave socially, abilities that cannot be captured by a conversation game.   One obvious criticism is that the human experimenter decides on success based on   subjective criteria. An equally important criticism is that the choice of questions may be  biased; some may favor  human capacity for abstract reasoning and others  may require  analytical skills of computational complexity.   One of the main strengths of the Turing test over many other definitions [2] is that it  provides an operational characterization of intelligence that can be implemented in a given  experimental setting. However, there remain problems related to the practical decision of  its success. Since failing the test proves nothing  and only passing is relevant [3 ], it is  important to have practical criteria for deciding how successful the test is. This questio n of  sufficient evidence of the validity of a tested property is discussed in Section 2.    In [4], we proposed the replacement test  as a generalization of the Turing test to compare  the ability of two systems to perform a given task, not necessarily verbal. In this test, the  objective is to check  whether a system S1  can successfully replace a nother system S2   performing a task characterized by a success criterion P.     To formalize the replacement test, we consider that the systems being compared are  embedded in the same test context that determines the input variables x and output  variables y of the system s accessible to the experimente r, with their respective domains  Dom(x), Dom(y).   We represent the behavior of a  system S under test by an equation of the form y=C[S](x),  where C is the test context. The success criterion is a predicate P(x,y) that is true when the  system provides a successful output y to the input x.  Given a test context C and two systems S1, S2  that can be e mbedded in the context C , we  say that S1 can replace S2 in the execution of a task with success criterion P,   if ∀t∈Dom(x) P(t ,C[S2](t)) ⇒ P(t,C[S1](t)), i.e., S1 is at least as successful as S2 in  completing the task.    We say that S1 and S2 are equivalent  if ∀t∈Dom(x) P(t ,C[S1](t))=P(t,C[S2](t)). In other  words, P cannot distinguish between S1 and S2 performing in the context C.     Note that in this comparison, we need to consider not only functional behavior, but also  time-dependent behavior. Limiting ourselves to purely functional properties may not allow  us to distinguish a fast system from a slow one that responds with  practically unacceptable  delays. Taking such a criterion into account will favor the machines when the behaviors  being compared are functionally equivalent. 4      It is clear that this equivalence can account for the Turing test where the systems S1 and S2  are a human and a machine, respectively. The variables x and y range over questions and  answers in some natural language. The context C defines the way the agents  (human or  machine) operate in the exchange of messages with the experimenter. Finally, P is the  predicate that accounts for the cr iteria applied by the experimenter to compare the  questions with the corresponding answers.     As the Turing test, the replacement test provides an operational definition of intelligence. It  can be used to com pare the ability of two systems, intelligent or not, to satisfy success  criteria specified by the property P. It is applicable to compare all kinds of systems  embedded in the same testing context, whether they are programs, game systems or  agents. It can give rise to intelligence tests when we consider human intelligence tasks;  for  example, to compare the ability of agents to learn, teach, write a text, etc. by  meeting given  success criteria. An inherent difficu lty in this comparison is that the success criteria may  not lend themselves to a rigorous formulation, as is the case for technical tasks.      We can extend the replacement test to  systems consisting of interacting components. For  example, for driving tasks,  we define the system under test y= C[S1,...,S n](x), that consists  of n interacting vehicles with x and y n-tuples of input and output variables.  The test context C is a traffic infrastructure with roads and signaling  equipment that define  the constraints under which veh icles travel. The input x i is a driving scenario  for the i- th  vehicle, i.e. its initial position and speed, and its destination. The output yi of the i-th vehicle  is the sequence of its successive positions with its kinematic characteristics at each  position, from its starting point to its destination. Finally, the property P characterizes the  success criteria for the pairs (x,y), including safety properties , e.g. collision avoidance, but  also performance properties , e.g. the absence of congestion. Note that P can be formalized  using logic [5] and thus can be evaluated rigorously for a given experiment.    In the example considered, we can test whether a tuple of autonomo us vehicles (S1', ..., Sn')  can replace a corresponding tuple of human drivers  (S1, ..., S n). Note that applying the  replacement test to systems of interacting components allows us to compare systems with  respect to emergent pr operties that characterize the ir overall behavior, and thus to  distinguish individual from collective intelligence, as we will see in the next section.    Note the importance  of the context C, especially for systems of interacting components  sharing a common environment . For example, the latter is specified for autonomous  driving systems, as an operational design domain [ 6] which describes the operating  conditions under which the systems are designed to function, including road types, speed  range, environmental conditions (weather, time of day/time of night, etc.), traffic  regulations and applicable regulations. 5    2.2 Autonomous systems     The replacement test allows for broader definitions of intelligence, in particular the ability  to perform a given set of tasks with associated success criteria. It therefore lends itself well  to the characterization of human intelligence as a combination of a wide range of abilities.  The Oxford Learner’s Dictionary defines intelligence as \"the ability to learn, understand,  and think logically  about things; the ability to do this well \" [7]. This view of intelligence is  adopted by autonomous systems intended to replace humans performing tasks in existing  organizations. We explain below that its realization implies a concept of intelligence very  different from that of the Turing test.    Autonomous systems support a paradigm of AI systems that goes beyond machine learning  systems, which are often transformational systems th at interact with human operators.  They are distributed,  mission-critical systems immersed in a physical and human  environment. They are composed of agents, each pursuing specific goals but having to  coordinate to achieve the overall system goals by develop ing collective intelligence. These  characteristics make their design extremely difficult, as evidenced by the failure of  industrial projects promising fully autonomous cars in the near future [8].     The behavior of an autonomous agent can be understood as the co mbination of five  functions [4, 9]. These consist of two functions for achieving situational awareness  (perception and reflection) by creating a model of its environment, and two functions (goal  management and planning) for decision- making based on th is model. The fifth function  concerns the production and application of knowledge to compensate for uncertainty and  incomplete knowledge of the environment. Typically, the autopilot of an autonomous car  can use pre- stored knowledge in maps to complement th e information produced by its  perception function. Thus, an autonomous agent combines reactive and proactive behavior.  It reacts to inputs provided by sensors and produces commands executed by actuators,  while it creates new situations by using acquired knowledge and pursuing goals.    The two existing approaches to building autonomous agents  are not up to the challenge.  The first one follows the traditional model -based engineering approach, defeated by the  complexity of the agent and the necessary use of non- explainable AI [10] for perception  and knowledge management. The second approach, adopted by certain Big Tech  companies, aims at developing end -to-end machine learning solutions that lack guarantees  of trustworthiness.  It s hould be stressed that building  autonomous agents is not the end of the story. The  successful replacement of a human performing a task in a physical and human environment  raises non trivial engineering problems.   It requires the development of adequate interfaces involving sensors and ac tuators as well  as man- machine interaction systems. Furthermore, it requires a detailed risk analysis to  identify possible hazards and set up mechanisms for the mitigation of their effects. For  example, to integrate an autonomous driving platform into a vehicle's cyber -physical 6    environment, it is necessary to analyze the safety and security risks and to implement the  appropriate recovery mechanisms.   The harmonious collaboration between an autonomous agent and a human operator is  often sought to improve  reliability in the accomplishment of a mission. However, this  common-sense idea does not yield the expected results at first sight and can even be  dangerous. For example, in the case of an autonomous car with a human supervisor, the  human agent solicited by the autopilot may not have the time to take control of the vehicle.  Conversely, in the event of a critical situation, an untimely intervention by the human  operator overriding the decisions of the autopilot can be dangerous.  Safe collaboration  between autonomous systems and humans raises symbiotic autonomy issues that go far  beyond traditional HMI [4].    Finally, it is not enough to build  agents capable of achieving their own goals. Their  collective behavior must satisfy global properties characterizing goals that the autonomous  system is supposed to achieve. They are properties of distributed systems whose  realization requires careful design of protocols and their implementation through adequate  coordination mechanisms. The properties may include simple properties such as  distributed mutual exclusion and scheduling, as well as more complicated properties such  as self-organization to achieve dynamically changing goals [1 1] or self-healing to cope with  agent failures [12].   For example, in an autonomous driving system, it is important that the collective behavior,  through appropriate rules and coordination mechanisms, meets fairness and performance  criteria, e.g., by preventing \"selfish\" behavior of agents that could lead to traffic jams or  inefficient occupation of the road. Correctness of agents with respect to their safe driving  and mission goals does not guarantee that their integration will not have undesirable  effects. The extent to which global system properties can be factored into the design  requirements of individual agents remains an open question.    Figure 1 illustrates some of the points made above by drawing a distinction between  collective intelligence and agent intelligence. It considers that intelligent behavior is the  combination of situational awareness and decision- making capabilities. The com plexity of  situational awareness ranges from single domain to multiple domains and finally to open  world awareness. The complexity of decision- making increases as we move from a single  goal to multiple goals and from a single agent to a system of agents.   Currently, AI focuses mainly on single domain and single goal agents. Autonomous agents  deal with fixed sets of goals in different domains. Finally, human agents exhibit unlimited  awareness for dynamically changing goal sets.   7      Figure 1: From agent intelligence to collective intelligence     For autonomous systems composed of arbitrary sets of agents, we distinguish three levels  of difficulty.   The first level conc erns the proper integration of agents so that their interaction does not  hinder the achievement of their individual goals.   The second level consists in ensuring that the collective behavior of the integrated agents  meets the goals that characterize the expected services of the system. For instance, careless  integration of self- driving agents can cause bottlenecks, unfairness and even compromise  their safety.  Finally, the third level corresponds to systems in which agents must act in synergy to  achieve resilience despite dynamically changing goals and environmental conditions, as in  human societies. For example, self-healing requires successively 1) the detection of risks by  agents; 2) the mitigation of risks to allow a minimal availability of the system; and 3) the  recovery of the system through self-organization.    The above  technical analysis of the vision of autonomy inspired by the idea of the  replacement of man by machine reveals the multiple facets of the intelligence challenge. It  shows that we need to  build a whole  edifice of which neural networks  are only a building  block.   8    3. Validating properties of AI systems     The implementation o f operational definitions of intelligence, such as the Turing test and  the replacement test, raises the question of whether a system S satisfies a property P  specifying a success criterion. In this section, we discuss the principles of validation by  testing and examine the extent to which existing test  methods can provide empirical  evidence that AI systems satisfy their relevant properties.  3.1 Property validation by testing   3.1.1 Testing as an empirical validation process    There is a big difference betwee n verification and testing regarding both the type of  properties and the degree of confidence that they are satisfied. Test methods are subject to  observability and controllability constraints. They can experimentally analyze the observed  behavior of the s ystem in response to external stimuli. On the contrary, verification can  examine the whole system behavior described by a model, and decide about the validity of  its properties. In particular, we can verify properties involving universal quantification e.g .  that all system states are safe, or that any system run involves  a rejuvenation state. These  properties can only be falsified by testing, by discovering system runs that violate them.  Currently, neural networks cannot be validated by reasoning, including verification  techniques. Extracting models of their behavior motivates  many works on explainable AI   [10]. This is theoretically possible given the structure of a neural network and the  mathematical function that characterizes the input/output behavior of its nodes  e.g.  [13,14]. In particular, for feed -forward networks we can propagate the input values along  each layer to compute the corresponding output values . However, model generation is  hampered by the nonlinearity of the activation functions and the complexity of the network  structure. Practical explainability based on rigorous behavioral models seems currently out  of reach. The validation of neural networks can therefore only be empirical.    We review the princi ples of empirical validation of system properties  through testing,  which are necessary to decide on operational definitions of intelligence.  To validate that a system S satisfies a property P(x,y) , a test environment includes the  system under test y=C[S](x) connected to two other systems, as shown in Figure 2:    • A Test Case Generator that applies test cases  t∈Dom(x), generated following a test   method;   • An Oracle that for each test case t and the corresponding  run r, r∈Dom(y), evaluates  P(t,r) and provides a verdict accordingly.    The claim \"S satisfies P\" means that for any test case t and corresponding run r, P(t,r)=true.   9        Figure 2: The testing environment    Testing is a general paradigm for developing empirical knowledge.   The objective of a test method is to estimate the degree of validity of P based on the  verdicts produced by the oracle for suitably chosen test sets (sets of test cases). The system  under test can be a physical system, for which we want to test a hypothesis P or an artifact  for which we want to validate a behavioral property P. We may want to test the capabilities  of an agent (human or machine) or even the collective prop erties of a system of agents,  such as an autonomous driving system, or the opinion of a population.    The oracle can be either a machine- computed algorithm or a human expert able to judge  according to unambiguous and justifiable criteria. Clearly, P is limited to properties that  express relationships between observables, inputs and outputs of the system under test.    The test cases are input values selected according to a test method that depends on the  degree of knowledge of the system under test. We distinguish two extreme cases:   1) black box testing , when we only have access to the  input and the output of the system   under test;   2) white box testing , wh en we have a model , which can be used to explore as much as  possible, the behavior of the system under test.   White-box testing combines experimental data with reasoning about a model for a  thorough exploration of system behavior, resulting in stronger validity guarantees . This  can be the case when testing electromechanical systems or software systems written in a  high-level programming language, where the test case generation process can be guided by  the results of the system model analysis.   In the current state of knowledge, only black box testing can be applied to neural networks.     The number of the test cases is often extremely large or even infinite. Exhaustive black box  testing is only possible for systems that are memoryless functions on finite domains, such  as combinatorial circuits.  For simple transformational systems, the test cases are patterns of input data e.g. to test  functional software. For general systems, they are sequences of input data of arbitrary  length that generate sequences of output data. Often, test methods must tailor test cases to  system responses, such as for controlled experiments in physics or testing of embedded  systems.   10    3.1.2 Requirements for a general testing framework     How to select the test cases among all the possible input values?  Random testing  techniques are d efeated by the complexity o f the task. As explained in [1 5], if we assume  that the probability of detecting one failure per test case is constant over time, it is  practically impossible to guarantee satisfactory reliability levels, e.g., less than 10 -8  failures/hour. To overcome this difficulty, test methods rely on  criteria to reduce the  complexity of the task taking into account the property to be validated and making   simplifying assumptions to be checked separately.     We propose a general framework for testing the validity of a property P for a system S. This  framework summarizes the key characteristics of testing techniques and provides  requirements for rigorous evaluation of test results.    Note that the property P ind uces an observational equivalence relation on a  test set. Two  test cases t1, t2 are equivalent for P (we write t1 ≈ P t2) if for any corresponding runs r1 and  r2, we have P(t1,r1) = P(t2,r2). We can thus reduce the testing comp lexity by considering  only one test case per equivalence class if we assume that P holds  [16]. Alternatively,  following the metamorphic testing approach [ 17], we can try to falsify P if equivalent test  cases are distinguished by P.    Other practical criteria for the classification of the test cases can be their significance for  the satisfaction of the property P. The significance may reflect the likelihood that the  system environment applies a test case e.g. by distinguishing between frequent test cases  and infrequent ones. It can also reflect the effectiveness of the test case in exploring  features that affect property satisfaction.   These considerations lead to simplifications of the testing problem. When the test cases are  sequences, simplifications may result from the observation that it is sufficient to test  property P for test cases of length less than a given value.    The general framework for testing that \"S satisfies P” uses two interdependent functions:   • First, an efficiency function  eff that for given T provides eff (T)∈[0,1] measuring the  extent to which the application of T explores features of the system behavior relevant  to the property P. This function is used for choosing test sets T.  • Second, a score function sc  that for a given T and the corresponding set of runs  R  computes sc(T,R), a measure of the likelihood that S meets P. Without giving a precise  definition, sc(T,R) provides quantitative information about the degree of validity of P.    For example, in test methods for hardware or software systems, the efficiency function  characterizes a degree of coverage of the system behavior, based on two types of criteria:    11    1) structural criteria that provide the percentage of the system structure exercised by the  test cases, such as the percentage of source code lines or the percentage of branches of a  control graph, e.g. [18];   2) functional criteria that indicate that the system can perform cer tain essential functions,  such as transmitting messages, braking, or adapting to stimuli, e.g. [19].   In these cases, the score function usually provides the average success rate with  corresponding coverage, or with confidence level and range for statistic al testing methods  [20,21]. Note that b ased on the calculated score, the oracle can guide the test case  generator to adapt by selecting test cases that improve the efficiency of the testing process  [22].    Additionally, the two functions  eff and sc should satisfy the following minimal  requirements.   • Monotonicity of eff: For test sets  T1, T2,  T1⊆T2 implies eff(T1)≤ eff(T2). In other  words, the test method is progressive; adding new test cases to a given test set does  not degrade its efficiency.   • Consistency of eff with respect to P: If two test cases t1, t2 are not distinguishable by  P then they are equally efficient i.e., t1 ≈P t2 implies eff({t1})=eff({t2}).  • Reproducibility: Equally efficient test sets T1, T2 should yield “similar” scores within  a degree estimated by the theory, i.e., eff(T1)=eff(T2) implies sc(T1,R1) ~ sc(T2,R2),  where R1 and R2 are the s ets of runs corresponding to T1, T2. The relation ~ is a  similarity relation on the set of scores with a degree parameter that allows to  compare the proximity of two scores.  Reproducibility is an essential epistemic requirement that guarantees the  “objectivity” of the testing method , e.g. that the outcome of the testing process does  not depend on the choice of a particular test set, but only on its efficiency [23].    Additional requirements can ensure a smooth constructive testing process.    • One requirement is to  allow building increasingly efficient test sets taking into  account only the efficiency of their elements, i.e., for T1, T2, T3, T4 test sets,  eff(T1)=eff(T2) and eff(T3)=eff(T4) implies eff(T1 ∪T3)=eff(T2 ∪T4).    • Another equally important requirement is that for any sequence of test sets of  increasing efficiency the corresponding scores are increasingly accurate.    The proposed framewo rk provides the necessary conditions for the development of  reliable empirical knowledge through testing . It takes up some of the ideas in [24] about  testing and the observational equivalence  ≈ P. It is part of an empirical approach to  Computing [ 25,26] of increasing importance with the predominant role of arti ficial  intelligence techniques.   12    3.2 Validating properties of AI systems  3.2.1 Limitations of AI systems     To what extent can we apply rigorous test methods to validate relevant properties of AI   systems?    Engineering practices follow epistemic imperatives requiring  that when a property is  assigned to a system, it must be accompanied by its rigorous definition and usable  validation criteria allowing its falsification. Attributing a property to a system without  validation criteria is just technically irrelevant chatter. System requirements can be broken  down into sets of three different types of properties.   Safety properties mean that the system, during system  execution, will never reach \"bad  states\" characterized by explicit conditions on its variables.   Security properties mean that the system is resilient to attacks that threaten data integrity,  privacy and system availability.   Finally, performance properties characterize technical and economic criteria concerning the  resources and their exploitation.   Safety and security properties can only be falsified by testing. Their non- falsification is only  an indication of a certain degree of validity depending on technical score criteria.     The application of rigorous test methods to AI systems suffers from limitations that are  often overlooked due to the tendency to break free from the shackles of standard  engineering practices.  Note that the existence of adversarial examples for neural networks [ 27] contradicts  requirements of the testing framework . An adversarial example is a corrupted version of  input data that is misclassified by the system while it cannot be distinguished by the oracle.  This means that we have two test cases t1, t2 such that t1 ≈ P t2 and thus eff({t1})=eff({t2})  while the scores of the corresponding tests are different. However, the requirement  violated by this anomaly can be relaxed if we can find adequate statistical criteria  characterizing the root causes of th e adversarial examples and the factors amplifying the  phenomenon.    Other limitations concern the type of properties teste d, in particular \"human- centric\"  properties that cannot be defined in terms of observables of the system under test.    Many works superficially attribute mental attitudes such as belief, desire and intention to  autonomous systems [ 28,29,30]. Some even consider that in an autonomous system, “we  cannot show that an agent always does the right thing, but only that its actions are taken   for the right reasons” [28].   Judging machine behavior according to ethical criteria means that machines can  understand and predict/estimate the consequences of their choices. It practically implies   that they can build a semantic model of the external wor ld on which they can evaluate the  impact of their choices [ 9]. Moreover, the Chinese room argument [31] shows that the 13    ability to understand cannot be discerned experimentally. Finally, further evidence that all  of these considerations are technically irre levant is that when it comes to implementing  mental attitudes  in software systems, belief, desire, and intention simply become   knowledge, the set of possible goals, and the generated plan to achieve a given goal,  respectively [28]. Moving from correctness with respect to objective criteria to the respect  of ethical rules opens the door to a bogus and irrational debate about how to evaluate the  impact and role of AI.     Here are some examples that illustrate the tendency to assert desire d properties without  taking the necessary precautions to ensure that assertions are  well-founded from a logical  or epistemological point of view.     Claiming that an autonomous driving system is safe enough because it has driven 10 billion  miles in simulation does not necessarily imply that the real system is safe [32]. The  simulated miles must be related to the \"real ” miles to show that the simulation takes into  account the many different situations according to their significanc e, e.g., different road  types, traffic conditions, weather conditions, etc.    The concept of \"responsible AI,\" embraced in particular b y big tech companies [3 3,34],  requires that the development and use of AI meet criteria such as fairness, transparency,  and accountability, which are difficult, if not impossible, to specify and test.    Finally, “AI alignment\" with human values  e.g. [35], is completely unfounded, as we do not  even understand how the human volition emerges and how the associated value- based  decision-making system works. At some point, it will be necessary to demonstrate that the  advocated multi- objective optimization approaches can capture human d ecision  mechanisms involving a large number of dynamically changing and hierarchically  structured goals, subject to different temporal constraints and dependent on complex  intertwined value systems that are currently poorly understood [9].  Figure 3 shows, for six types of systems and corresponding properties, differences  in the  applicability of test methods.    The first two cases correspond to white- box testing, where mathematical models  complement empirical knowledge about the tested systems, allowing to reason about and  explore in depth their behavior. The o racle in both cases applies methods based on  objective criteria, and non-falsification is conclusive evidence of their validity.     The third case is an application of the statistical approach to estimate  the effectiveness of a  vaccine. Applied statistics propose sampling techniques that rely on an observational  equivalence relation on the possible experiments. A sample includes a proportion of  representatives per class reflecting their significance in the test space. For the system  under consideration, namely a population, it is practically impossible to obtain detailed  behavioral models. However, sampling techniques allow  us to achieve an adequate 14    coverage of the population concerned. The o racle is a human analyst who applies a  predefined method based on experimental data to decide the outcome of a test campaign.         Figure 3: Examples illustrating differences in the applicability of test methods.    The following three cases concern AI  systems whose characteristics direct us towards  statistical methods to be developed.  For an image classifier with unambiguous classification criteria,  even if the property P  cannot be formalized, we can amply rely on a human oracle. We need a sampling technique  based on appropriate coverage criteria to estimate the probability that the system will  perform as expected.  Of course, these criteria must take into account possible adversarial  anomalies.  For autonomous driving systems, it is possible to formalize the properties and associated  validation techniques applied by the oracle. For example, for a traffic rule, it is p ossible to  check formally whether the observed behavior violates this property [ 5]. Nevertheless, we  lack sampling techniques to generate sets of driving scenarios that provide adequate  coverage of real-life situations and to estimate test campaign scores.    Finally, for large language models performing natural language processing, rigorous testing  seems almost impossible.   First, because there is no characterization of the precise nature of the relationship between  prompts and responses. Thus, the property to be validated cannot be specified  unambiguously unless the language is restricted to subsets rooted in rigorous semantics.  Second, in order to develop statistics-based techniques, we need coverage and significance  criteria, which seems difficult, if not impossible, for natural languages.  These issues are often ignored by work that considers  success on various benchmarks  designed to model meaning -sensitive tasks to be sufficient ev idence that language models  understand natural language [36]. In addition to the fact that the training data for language 15    models do not account for meaning, it is impossible to demonstrate that the benchmarks  are not free of bias.    3.2.2 Testing operational definitions of intelligence    We discuss the effective application of the testing  framework for comparing two systems  S1 and S2 integrated in the same context C and performing a task characterized by a  success criterion P(x, y).  For which type of task, and therefore for which associated criterion  P, is it possible to  develop test methods that estimate the probability that the two systems are equivalent, i.e.  ∀t∈Dom(x) P(t,C[S1](t))=P(t,C[S2](t))?  We have already mentioned the inherent limitations of conversational natural language  tasks, for which the property P cannot be rigorously formulated. This limitation is lifted for  tasks performed by systems whose I/O behavior involves quantities, such as physical  system controllers. In this respect, the Turing test differs from replacement tests for tasks  of this type where, in principle, P can be formalized.  Another peculiarity of the Turing test is that the property P admits a finite, albeit very  complex, representation. Since the test domain is discrete and the length of the test cases  and corresponding answers is finite, the property P can be stored in a finite memory in the  form of a correspondence between questions and answers (exactly as in the Chinese room  argument [31]).   The consequence of this observation is that such a  theoretical possibility defeats the  purpose of the Turing test. If human intelligence can be apprehended by a conversational  game in natural language, then machines endowed with th e finite relation characterizing  the success criterion P can become at least as intelligent as humans, with the obvious  advantage of much shorter response times.   This remark also applies to any test based on a natural language conversation, where the  success criterion is a finite set of questions/answers. Consider, for example, the highly  relevant \"Human or Machine\" test designed to distinguish humans from machines,  proposed and studied in [37]. If the distinguishing criteria are test cases where humans and  machines react differently, it is always possible to make machines indistinguishable by  modifying their behavior so that they also mimic humans for these test cases.  This discussion suggests that if human intelligence can be captured by a finite set of tests,  then machines can imitate humans . However, the  above thought experiment is not  transposable to replacement tests for tasks with infinite domains e.g., where the inputs and  outputs are physical quantities . Even if the length of the experiments is finite, there are  infinitely many possible test cases. To compare these systems, it is necessary to develop the  theoretical foundations of statistical test methods, as explained in 3.1.2. 16    4. Multiple Intelligences     The impressive suc cess of AI today leads to a wave of optimism that masks the fact that it  focuses on single-domain, single-task systems, which falls far short of covering the multiple  aspects of human intelligence.     Without a clear idea of what intelligence is, we cannot develop a theory of how it works.  To  say that “S1 is smarter than S2” is meaningless without specifying the task(s) and the  criteria for success.     On the one hand, the proposed replacement test is consistent with the position that there  are multiple intel ligences, each characterized by the ability to perform tasks purposefully   in a given context.  Tasks can be of any type, and the environment in which the agents  operate need not be real . For example, we can compare the ability of a human playing a  game in a  virtual reality environment to that of a machine. The machine would easily beat  the human in a game requiring good memory and computational intelligence.    On the other hand, in the debate about how to achieve artificial g eneral intelligence, we  should take human intelligence as a reference by considering replacement tests for a  characteristic set of tasks requiring human skills.   As these tasks may involve interaction with the physical or human environment,  intelligence is not limited to solvi ng abstract computational problems, but  also involves  solving associated implementation problems.  Human intelligence is not \"general purpose\";  it is the result of historical evolution in a given physical environment. Human intelligence  would be shaped differently if man had lived in a different environment, for example on  another planet.    The replacement test applied to a group of collaborating agents characterizes their ability  to achieve goals that none of them can achieve by working separately. It therefore takes  into account the fact that agents are immersed in a physical world of which they can only  have a limited perception and field of action. This leads to a concept of collective  intelligence measuring the ability of a group of agents to overcome the ir individual  limitations, for example to build a complex artifact. This concept could become irrelevant   for super-intelligent and hyper-powerful machines that surpass the human condition.     Autonomous systems born out of the need to replace human agents  with machines adopt  the replacement test and app ear to be a bold step toward artificial general intelligence.  They provide to some extent, a methodological basis for comparing humans and machines  in terms of their ability to perform skilled tasks.   Humans can combine concrete sensory information with c ommon-sense knowledge [3 8].  This is a vast semantic model built throughout life by learning and reasoning; it involves  concepts, cognitive rules and patterns, used to interpret sensory information and languag e.  Human understanding combines bottom -up reasoning from the sensor level to mental 17    models, and top -down reasoning from semantic models to perception. This is a major  difference between humans and neural networks. Humans have no difficulty recognizing a  partially snow-covered stop sign because they can associate sensory inform ation with its  conceptual model and properties. In contrast, a neural network  must be trained to  recognize stop signs in all weather conditions [9].   In order for machines to match hum an situational awareness, they must be able to  gradually develop knowledge about their environment through a combination of learning  and reasoning. This is probably the most difficult problem to solve, as evidenced by the  poor progress made so far in the semantic analysis of natural languages.    Humans, as autonomous systems, are endowed with a specific value- based decision- making mechanism [3 9] that can handle a multiplicity of goals to satisfy their  corresponding needs. They choose between action alternati ves based on a system of value  scales to determine, for each action, the units of value needed or generated to perform it.  The decision mechanism estimates for actions a subjective \"value balance\" depending on  their domains: economic, political, legal, edu cational, military, epistemic, mor al, religious,  aesthetic, etc . [9]. For example, a fals ified tax declaratio n may bring an economic benefit  but in return may lead to a penalty (cost in the legal value scale). Note that in order to  establish the value balance for an action affecting different domains, humans implicitly  accept a certain correspondence between the value scales based in particular on their  common-sense knowledge.    The value systems of individuals reflect a value system common to the social organization  to which they belong, and which aims to promote the achievement of common goals,  rewarding beneficial actions and penalizing actions detrimental to the social good. This  ensures social cohesion and the synergy of individuals to achieve common goals. V alue- based decision-making mechanisms allow us to understand how society and its individuals  behave as dynamic systems, and how social intelligence emerges.   Societies are autonomous systems capable of managing multiple goals and adapting to  changes in their environment. Human intelligence has a social dimension because it is  capable of creating synergies and contributing to social goals, but also because it has been  shaped during evolution, immersed in social life.   It will take time to develop autonomous systems capable of intelligent collective behavior.    For operational definitions of intelligence to be useful, we need well-founded approaches to  validate system properties [40]. This question raises epistemological and methodological  issues that deserve further exploration. The lack of explicit and faithful  behavioral models  for neural networks prevents formal verification, which limits both the type of properties  considered and the level s of certainty and validity achievable. Only properties that express  relationships on observables can be tested, which exc ludes properties related to mental  attitudes that are outside the scope of a black box test analysis. 18    Another induced limitation is that validation cannot provide guarantees as strong as those  obtained by model-based validation. Statistical validation with  adequate theoretical  foundations seems to be the only realistic approach.    The fact that natural language transformers fall outside the scope of rigorous validation  techniques raises legitimate questions about the possibility of establishing empirical  evidence of their properties by adequately relaxing existing epistemic requirements. In this  respect, it seems appropriate to find a compromise between system certification and  human qualification. The former relies on a formal methodology to test established   evaluation criteria, while the latter attests, by means of an examination, to a person's skills  and abilities.  What if we replaced rigorous testing with qualification examinations? After all, it is not  impossible for large language models to pass final exams just as well as students.   However, we should not ignore two fundamental differences between neural networks and  human beings. Firstly, human th inking is robust, whereas neural networks are not (slight  changes in questions imply different answers) ; secondly, human thinking is better placed  to avoid inconsistencies, thanks to semantic control based on common-sense knowledge.      Figure 4: The space of possible intelligences    Operational definitions of intelligence allow us to compare behaviors, but ignore the way  they are implemented. Two behaviorally equivalent systems may use  very different  creative processes.  Humans have good situational awareness, combine symbolic and concrete know ledge, and  make highly effective value- based decisions. Nevertheless, their cognitive abilities are  limited to grasp complex relationships and make optimal decisions [41]. On the other hand, 19    data-based techniques are proving to be unbeatable in generating knowledge from high - dimensional data. Inspired by this complementarity, we can imagine a  space of possible  intelligences encompassing the abilities to generate and apply data- based knowledge and  symbolic knowledge, as shown in Figure 4 . A super-intelligent agent could combine both to  the highest degree (upper right corner).    Can we bridge the gap between symbolic and concrete knowledge by using neural  networks exclusively? We are very far from being abl e to answer this question, which  requires an in- depth study of the human mechanisms of management and development of  symbolic knowledge.   In some cases, reasoning can be replaced by a complex model -based evaluation. Recent  results show that large language models can open the way to efficient solutions to symb olic  reasoning problems [42].  However, it is doubtful that all the unexplored human capabilities in symbolic knowledge  processing can be emulated by data-driven techniques. There is still a long way to go before  machines can match the multifaceted human capabilities in power of abstrac tion and  creativity.    Acknowledgements: The ideas in this paper benefited from the comments and critiques of  Cristian Calude, David Harel and Assaf Marron.  I would like to thank the many colleagues  whose criticism and comments helped to prepare this revised version.   References     [1] Alan M. Turing. Computing Machinery and Intelligence. Mind, Vol. 59, No. 236 (Oct.  1950), pp. 433-460.  [2] Shane Legg and Marcus Hutter. A Collection of Definitions of Intelligence.  arXiv:0706.3639v1 [cs.AI] 25 Jun 2007.  [3] Robert M French. The Turing Test: the first 50 years. Trends in cognitive sciences, 2000,  Elsevier.  [4] Joseph Sifakis and David Harel. Trustworthy Autonomous System Development. ACM  Transactions on Embedded Computing Systems, Volume 22, Issue 3 , pp 1 –24,  https://doi.org/10.1145/3545178.  [5] Marius Bozga and Joseph Sifakis. Specification and Validation of Autonomous Driving  Systems: A Multilevel Semantic Framework. arXiv:2109.06478v1 [cs.MA] 14 Sep 2021.   [6] DOT. Preparing for the Future of Transportation: Automated Vehicle 3.0.  U.S.  Department of Transportation. Washington, DC. October 4, 2018.  [7] Oxford Learner’s Dictionaries.   https://www.oxfordlearnersdictionaries.com/definition/english/intelligence.  [8]Bloomberg Law.  https://news.bloomberglaw.com/product-liability-and-toxics-law/waymo-ceo-says-self- driving-cars-wont-be-ubiquitous-for-decades.    20    [9] Joseph Sifakis. Understanding and changing the world, From Information to Knowledge  and Intelligence, Springer, May 2022.  [10] Veda C. Storey, Roman Lukyanenko, Wolfgang Maass and Jeffrey Parsons. Explainable  AI. Communications of the ACM, April 2022, Vol. 65 No. 4, Pages 27-29, 10.1145/3490699.  [11] Manuele Brambilla, Eliseo Ferrante, Mauro Birattari and Marco Dorigo. Swarm  robotics: a review from the swarm engineering perspective. Swarm Intell 7, 1–41 (2013).  https://doi.org/10.1007/s11721-012-0075-2.   [12] Chris Schneider, Adam Barker and Simon A. Dobson. A survey of self- healing systems  frameworks. Softw. Pract. Exp. 45(10): 1375-1398 (2015).  [13] Guy Katz, Clark Barrett, David Dill, Kyle Julian and Mykel Kochenderfer, Reluplex: An  Efficient SMT Solver for Verifying Deep Neural Networks, arXiv:1702.01135v2 [cs.AI], 19  May 2017.  [14] Nicola Franco, Tom Wollschläger, Nicholas Gao, Jeanette Miriam Lorenz, Stephan  Günnemann, Quantum Robustness Verification: A Hybrid Quantum-Classical Neural  Network Certification Algorithm, arXiv:2205.00900v1 [quant-ph], 2 May 2022.  [15] Ricky W. Butler and George B. Finelli . The infeasibility of quantifying the reliability of  life-critical real -time software, IEEE Transactions on Software  Engineering, Volume: 19,  Issue 1, January 1993.  [16] T. J. Ostrand and M. J. Balcer. The category -partition method for specifying and  generating functional tests. CACM Volume 31 Issue 6, June 1988, pp 676–686,   https://doi.org/10.1145/62959.62964.  [17] Zhi Quan Zhou and Liqun Sun. Metamorphic testing of driverless cars.  Communications of the ACM, Volume 62, Issue 3, March 2019. 61 –67.  https://doi.org/10.1145/3241979.  [18] Joachim Wegener, Andre Baresel and Harmen Sthamer. Evolutionary test environment  for automatic structural testing. Information and Software Technology 43 (2001) 841-854.  [19] Claude Jard and  Thierry Jéron. TGV: theory, principles and algorithms. A tool for the  automatic synthesis of conformance test cases for non -deterministic reactive systems.  International Journal on Software Tools for Technology Transfer volume 7, pages 297 –315  (2005).  [20] A. Denise, M.-C. Gaudel and S.-D. Gouraud. A generic method for statistical testing. IEEE  15th International Symposium on Software Reliability Engineering, 2-5 Nov. 2004.  [21] S. -D. Gouraud, A. Denise, M.- C. Gaudel and B. Marre. A new way of automating  statistical testing methods. IEEE Proceedings 16th Annual International Conference on  Automated Software Engineering (ASE 2001), 26-29 Nov. 2001.  [22] Mark Harman, Yue Jia and Yuanyuan Zhang. Achievements, open problems and  challenges for search -based software testing. 2015 IEEE 8th International Conference on  Software Testing, Verification and Validation (ICST).  [23] And y Cockburn, Pierre  Dragicevic, Lonni Besancon, and Carl Gutwin. Threats of a  Replication Crisis in Empirical Computer Science. CACM, August 2020, Vol. 63, No. 8.  [24] Peter Wegner, Towards Empirical Computer Science, The Monist, Vol. 82, No. 1,  Philosophy of Computer Science (January 1999), pp. 58-108.  [25] Allen Newell and Herbert Simon. Computer science as empirical inquiry: Symbols and 21    search. ACM Turing award lectures, CACM, March 1976, Vol 19, Number 3.  [26] Peter Denning. Is Computer Science Science? CACM. April 2005, Vol 48, No 4.  [27] Alexey Kurakin , Ian J. Goodfellow  and SamyBengio. Adversarial Examples in the  Physical World. arXiv:1607.02533v4 [cs.CV] 11 Feb 2017.  [28] Louise Dennis, Michael Fisher, Marija Slavkovik and M att Webster. Formal verification  of ethical choices in autonomous systems. Robotics and Autonomous Systems 77 (2016) 1 – 14.   [29] Michael Anderson and Susan Leigh Anderson. Machine Ethics: Creating an Ethical  Intelligent Agent, AI Magazine Volume 28 Number 4 (2007).  [30] Virginia Dignum et al. Ethics by Design: Necessity or Curse? AIES'18, February 2 -3,  2018, New Orleans, LA, USA.  [31] John R. Searle. Minds, brains and programs. Behav. Brain Sci. 3, 1980, 417-424.  [32] WAYMO. Waymo reaches 5 million self- driven miles. February 27, 2018. Retrieved  from https://blog.waymo.com/2019/08/waymo-reaches-5-million-self-driven.html.  [33] Alejandro Barredo Arrieta et al. Explainable Artificial Intelligence (XAI): Concepts,  Taxonomies, Opportunit ies and Challenges toward Responsible AI. arXiv:1910.10045v2  [cs.AI] 26 Dec 2019.  [34] Google. Why we focus on Al (and to what end). January 16, 2023.   https://blog.google/technology/ai/why-we-focus-on-ai-and-to-what-end/   [35] P. Vamplew, R. Dazeley, C. Foale, S. Firmin and J. Mummery. Human -aligned artificial  intelligence is a multiobjective problem. Ethics and Information Technology, 20(1), 27 -40  (2018). https://doi.org/10.1007 /s10676-017-9440-6.   [36] Emily M. Bender, Timnit Gebru, Angelina McMillan- Major and Shmargaret S hmitchell.  On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? FAccT '21:  Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,  March 2021 Pages 610–623https://doi.org/10.1145/3442188.3445922.  [37] David Harel and Assaf Marron. Human or Machine:  Reflections on Turing -Inspired  Testing for the Everyday, arXiv:2305.04312v3 [cs.AI] 20 Jun 2023.  [38] Ernest D avis and Gary Marcus. Commonsense Reasoning and Commonsense  Knowledge in Artificial Intelligence. Communications of the ACM, September 2015, Vol. 58  No. 9, Pages 92-103.  [39] Antonio Rangel, Colin Camerer, and P. Read Montague. Neuroeconomics: The  neurobiology of value-based decision-making. Nat Rev Neuro sci. 2008 July; 9(7): 545–556.  doi:10.1038/nrn2357.  [40] Moshe Vardi. Would Turing Have Passed  the Turing Test?  CACM, September 2014,  VOL. 57, No. 9.  [41] Herbert A. Simon. Theories of bounded rationality. CIP WP  #66, March 1964.  https://iiif.library.cmu.edu/file/Simon_box00008_fld00585_bdl0001_doc0001/Simon_box 00008_fld00585_bdl0001_doc0001.pdf   [42] Shima Imani, Liang Du and Harsh Shrivastava. MathPrompter: Mathematical reasoning  using large language models. arXiv:2303.05398v1 [cs.CL] 4 Mar 2023.",
      "references": [
        "Computing Machinery and Intelligence.",
        "A Collection of Definitions of Intelligence.",
        "The Turing Test: the first 50 years.",
        "Trustworthy Autonomous System Development.",
        "Specification and Validation of Autonomous Driving Systems: A Multilevel Semantic Framework.",
        "Preparing for the Future of Transportation: Automated Vehicle 3.0.",
        "Understanding and changing the world, From Information to Knowledge and Intelligence,",
        "Explainable AI.",
        "Swarm  robotics: a review from the swarm engineering perspective.",
        "A survey of self- healing systems  frameworks.",
        "Reluplex: An  Efficient SMT Solver for Verifying Deep Neural Networks,",
        "Quantum Robustness Verification: A Hybrid Quantum-Classical Neural  Network Certification Algorithm,",
        "The infeasibility of quantifying the reliability of  life-critical real -time software,",
        "The category -partition method for specifying and  generating functional tests.",
        "Metamorphic testing of driverless cars.",
        "Evolutionary test environment  for automatic structural testing.",
        "TGV: theory, principles and algorithms. A tool for the  automatic synthesis of conformance test cases for non -deterministic reactive systems.",
        "A generic method for statistical testing.",
        "A new way of automating  statistical testing methods.",
        "Achievements, open problems and  challenges for search -based software testing.",
        "Threats of a  Replication Crisis in Empirical Computer Science.",
        "Towards Empirical Computer Science,",
        "Computer science as empirical inquiry: Symbols and  search.",
        "Is Computer Science Science?",
        "Adversarial Examples in the  Physical World.",
        "Formal verification  of ethical choices in autonomous systems.",
        "Machine Ethics: Creating an Ethical  Intelligent Agent,",
        "Ethics by Design: Necessity or Curse?",
        "Minds, brains and programs.",
        "Explainable Artificial Intelligence (XAI): Concepts,  Taxonomies, Opportunit ies and Challenges toward Responsible AI.",
        "Human -aligned artificial  intelligence is a multiobjective problem.",
        "On the Dangers of Stochastic Parrots: Can Language Models  Be Too Big?",
        "Human or Machine:  Reflections on Turing -Inspired  Testing for the Everyday,",
        "Commonsense Reasoning and Commonsense  Knowledge in Artificial Intelligence.",
        "Neuroeconomics: The  neurobiology of value-based decision-making.",
        "Would Turing Have Passed  the Turing Test?",
        "Theories of bounded rationality.",
        "MathPrompter: Mathematical reasoning  using large language models."
      ],
      "meta_data": {
        "arxiv_id": "2305.11472v2",
        "authors": [
          "Joseph Sifakis"
        ],
        "published_date": "2023-05-19T06:46:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces the “replacement test” as an operational definition of (system) intelligence: a system S1 is intelligent w.r.t. a task/context if it can successfully replace another system S2 under a success predicate P in a context C, including for interacting multi-agent systems (collective intelligence). Critiques the Turing test for narrowness, subjectivity, and susceptibility to finite Q/A mimicry, and argues that autonomy and trustworthiness hinge on engineering and validation problems beyond current AI. Proposes a principled testing/validation framework (efficiency and score functions with requirements like monotonicity and reproducibility) and analyzes why rigorous validation is hard for modern AI, especially neural networks and LLMs. Frames intelligence as multiple, task-relative capabilities arising from complementarity of human symbolic/common-sense/value-based reasoning and machine data-driven learning.",
        "methodology": "Formalizes replacement testing via behavioral embedding y=C[S](x) and success predicate P(x,y); defines replaceability and equivalence via universal implications/equality over input domain. Extends to time-dependent behavior and to systems of interacting components to capture emergent/collective properties. Develops a testing framework with a test-case generator + oracle, distinguishing black-box vs white-box testing; introduces observational equivalence induced by P, coverage/significance notions, and two abstract functions: eff(T) measuring exploration/coverage relevant to P and sc(T,R) estimating likelihood of satisfying P; states axioms (monotonicity, consistency w.r.t. P, reproducibility) and discusses statistical testing as the likely foundation for AI validation.",
        "experimental_setup": "No new empirical experiments; paper is conceptual/theoretical with illustrative case studies and references. Discusses hypothetical/representative testing setups: (i) conversational Turing-style tests; (ii) autonomous driving: scenarios as inputs, trajectories as outputs, context as traffic infrastructure/ODD, with safety/performance predicates formalizable in logic; (iii) image classification with human oracle; (iv) LLM benchmarking, argued to lack rigorous property definitions and coverage criteria. Validation methods emphasized: black-box testing for NNs, scenario generation/sampling, oracle-based evaluation, and statistical scoring/coverage rather than formal verification.",
        "limitations": "Framework remains high-level: eff and sc are not concretely instantiated for major AI domains (driving, LLMs), and robust statistical foundations for scenario sampling/coverage are largely open. Highlights intrinsic limits of black-box testing: universal properties (safety/security) can only be falsified; guarantees are weak without models. Adversarial examples violate a key efficiency-consistency assumption. Many “human-centric” properties (understanding, ethical intention, alignment) are not observable/testable in the framework. For LLMs, prompts/responses lack a precise semantics and significance/coverage criteria for natural language are unclear, making rigorous validation close to impossible.",
        "future_research_directions": "Develop rigorous statistical testing theory for AI with explicit coverage/significance metrics tied to operational domains (e.g., ODD-based scenario spaces for autonomy) and calibrated score/confidence measures. Create methods to relate simulation to real-world distributions and to handle adversarial phenomena via root-cause/statistical characterizations. Build hybrid autonomy architectures combining learning with symbolic/common-sense reasoning and knowledge management; investigate mechanisms for machine situational awareness beyond pattern recognition. Formalize and validate collective intelligence/global properties of multi-agent autonomous systems (coordination, fairness, self-organization, self-healing) and how to derive global requirements from local agent specs. Explore practical evaluation regimes between certification and “qualification exams” for opaque models like LLMs while accounting for robustness and semantic consistency.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Large Language Models are Zero-Shot Reasoners",
      "full_text": "Emergent Analogical Reasoning in Large Language Models Taylor Webb1,*, Keith J. Holyoak1, and Hongjing Lu1,2 1Department of Psychology 2Department of Statistics University of California, Los Angeles, CA, USA *Correspondence to: taylor.w.webb@gmail.com Abstract The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. 1 Introduction Analogical reasoning is at the heart of human intelligence and creativity. When confronted with an unfamiliar problem, human reasoners can often identify a reasonable solution through a process of structured comparison to a more familiar situation. 1 This process is an essential part of human reasoning in domains ranging from everyday problem-solving2 to creative thought and scientific innovation. 3 Indeed, tests of analogical reasoning ability are uniquely effective as measures of fluid intelligence: the capacity to reason about novel problems. 4, 5 Recently, there has been considerable debate about whether and how a capacity for analogical thought might be captured in deep learning systems. 6 Much of this recent work has focused on training neural networks on very large datasets (sometimes containing millions of problems). 7, 8 Though this is a challenging task that has spurred the development of some interesting approaches, 9–12 it does not address the issue of whether analogical reasoning can emerge zero-shot (i.e., without direct training), the capacity most central to human thought. An alternative approach, also based on deep learning, involves large language models (LLMs). 13 LLMs have recently sparked great interest (and controversy) for their potential to perform few-shot, and even zero-shot, reason- ing. These models employ relatively generic neural network architectures with up to billions of parameters, and are trained using a simple predictive objective (predicting the next token in a sequence of text) with massive web-based text corpora consisting of billions of tokens. Though there is significant debate about the capabilities of these mod- els,14 a potential advantage is their ability to solve problems with little direct training, sometimes requiring only a few examples, or even a simple task instruction (typically without any updating of model parameters). This feature raises the question of whether LLMs might be capable of human-like, zero-shot analogical reasoning. To answer this question, we evaluated the language model GPT-3 13 on a range of zero-shot analogy tasks, and performed direct comparisons with human behavior. These tasks included a novel text-based matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices (SPM), 15 a visual analogy problem set commonly viewed as one of the best measures of fluid intelligence. 5 Unlike the original visual SPM problems, our Digit Matrices task was purely text-based so that it could be used to evaluate GPT-3’s ability to induce abstract Published atNature Human Behaviour(2023) https://doi.org/10.1038/s41562-023-01659-w 1 arXiv:2212.09196v3  [cs.AI]  3 Aug 2023Figure 1: Summary of results. Matrix reasoning results show average accuracy on all problems in Digit Matrices problem set, a novel text-based matrix reasoning task designed to emulate Raven’s Standard Progressive Matrices (SPM) problems. 15 Note that the Digit Matrices were purely text-based, and therefore do not test for the ability to perform abstract reasoning directly over visual inputs, as in the original SPM. Letter string results show average performance for novel letter string analogy problem set, based on problems from Hofstadter and Mitchell. 16 Both matrix reasoning and letter string results reflect performance on generative task. Verbal analogy results show average performance on UCLA Verbal Analogy Test. 17 Story analogy problems involved identification of analogous stories based on higher-order relations, using materials from Gentner et al.18 Both verbal and story analogy results reflect multiple-choice accuracy, with chance performance indicated by gray horizontal line. Chance performance for the two generative tasks (matrix reasoning and letter string analogies) is close to zero, due to the very large space of possible generative responses. Black error bars represent standard error of the mean for average performance across participants. Each dot represents accuracy for a single participant (matrix reasoning, N=40; letter string analogies, N=57; verbal analogies, N=57; story analogies, N=54). Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. rules (though not the ability to do so directly from visual inputs). Strikingly, we found that GPT-3 performed as well or better than college students in most conditions, despite receiving no direct training on this task. GPT- 3 also displayed strong zero-shot performance on letter string analogies, 16 four-term verbal analogies, 17, 19–21 and identification of analogies between stories. 18, 22, 23These results add to the growing body of work characterizing the emergent capabilities of LLMs,24–28 and suggest that the most sophisticated LLMs may already possess an emergent capacity to reason by analogy. 2 Results We evaluated the language model GPT-3 on a set of analogy tasks, and compared its performance to human behavior. GPT-3 is a large-scale (175B parameters), transformer-based29 language model developed by OpenAI.13 The original base model was trained on a web-based corpus of natural language consisting of over 400 billion tokens, using a training objective based on next-token prediction (given a string of text, the model is trained to predict the token most likely appear next). A number of variants on this base model have since been developed by fine-tuning it in various ways. These include training the model to generate code, 30 and training it to respond appropriately to human prompts, using either supervised learning or reinforcement learning from human feedback (RLHF). 31 Our evaluation focused on the most recent model variant, text-davinci-003 (here referred to simply as ‘GPT-3’), which was the first to incorporate RLHF (along with the concurrently released, but distinct, ChatGPT model). We found that text-davinci-003 displayed particularly strong performance on our analogy tasks, but earlier model variants also performed well in some task settings, suggesting that multiple factors contributed to text-davinci-003’s analogical capabilities (Supplementary Figures 1- 3). See Section S2 for further discussion. 2Our evaluation featured four separate task domains, each designed to probe different aspects of analogical reason- ing: 1) text-based matrix reasoning problems, 2) letter-string analogies, 3) four-term verbal analogies, and 4) story analogies. For each task domain, we performed a direct comparison with human behavior, assessing both overall performance and error patterns across a range of conditions relevant to human analogical reasoning. Figure 1 shows a summary of these results. We also performed a qualitative analysis of GPT-3’s ability to use analogical reasoning to solve problems. 2.1 Matrix reasoning problems We designed a text-based matrix reasoning task, the Digit Matrices, to emulate the structure of Raven’s Standard Progressive Matrices (SPM).15 The task is illustrated in Figure 2. The dataset was structured similarly to the work of Matzen et al., 32 who created, and behaviorally validated, a visual matrix reasoning dataset with the same rule structure as the original SPM. The Digit Matrices dataset thus has a similar rule structure to SPM, but is guaranteed to be novel for both humans and LLMs. Digit Matrix problems consisted of either digit transformations (Figures 2b- 2e) or logic problems (Figures 2f- 2g). Transformation problems were defined based on a set of three rule types – constant (Figure 2c), distribution-of-3 (Figure 2d), and progression (Figure 2e) – and consisted of one or more rules per problem. When multiple rules were present (Figure 2b), each rule was bound to a different spatial location within each cell (e.g., one rule was bound to the left digit in each cell, and another rule was bound to the right digit). Logic problems were defined based on set relations – OR, AND, and XOR – and involved only a single rule per problem. In some logic problems, the corresponding elements were spatially aligned (Figure 2f), whereas in others they were permuted (Figure 2g). We hypothesized that spatial alignment would be beneficial when solving the problems via analogical mapping, as it should highlight the isomorphism . 33 Digit Matrices problems were presented to GPT-3 without any prompt or in-context task examples. Figure 3 shows zero-shot performance on the Digit Matrices problems for GPT-3 and human participants (N=40, UCLA undergraduates). GPT-3 surpassed the average level of human performance on all problem types, both when generating answers directly (Figure 3a; logistic regression, main effect of GPT-3 vs. human participants: odds ratio (OR) = 1.88, p = 0.005, 95% confidence intervals (CI) = [1.21, 2.91]), and when selecting from a set of answer choices (Figure 3b; main effect of GPT-3 vs. human participants: OR = 6 .27, p = 2.3 ×10−8, CI = [3.28, 11.99]). It is worth emphasizing, however, that participants displayed a range of performance levels on this task, with some participants outperforming GPT-3 (indeed, the best participant answered every problem correctly). In addition to showing strong overall performance, GPT-3’s pattern of performance across problem subtypes was similar to that observed in human participants (correlation analysis: r(30) = 0.39, p = 0.027). This correlation was driven both by the pattern of performance across major problem types (one-, two-, three-rule, and logic problems; main effect of problem type on generative accuracy: OR = 0 .5, p = 2 × 10−16, CI = [0 .44, 0.56],; main effect of problem type on multiple-choice accuracy: OR = 0 .56, p = 2 × 10−16, CI = [0 .5, 0.64]), and by differences within each problem type. Problems with progression rules were more difficult than those without them (Figure 3c; main effect of progression vs. no progression, human participants: OR = 0 .41, p = 0 .0001, CI = [0 .24, 0.69]; GPT-3: OR = 0.07, p = 1.9 × 10−5, CI = [0 .02, 0.24]); for multi-rule problems, performance was negatively correlated with the number of unique rules in each problem, even when holding constant the number of total rules (Figure 3d; main effect of number of unique rules, human participants: OR = 0 .61, p = 0.0047, CI = [0 .44, 0.86]; GPT-3: OR = 0.25, p = 3 × 10−10, CI = [0 .17, 0.39]); and logic problems were more difficult when the corresponding elements were spatially permuted vs. aligned (Figure 3e; main effect of spatial alignment, human participants: OR = 0 .52, p = 0.0017, CI = [0.35, 0.79]; GPT-3: OR = 0.06, p = 2×10−11, CI = [0.03, 0.14]). These effects replicate well-known characteristics of human analogical reasoning: problems defined by relations (e.g., progression) are typically more difficult than problems defined by the features of individual entities (e.g., constant or distribution-of-3);32, 34problem difficulty is typically driven by the degree of relational complexity, as defined by the number of unique relations; 35 and analogical mapping is easier when a greater number of constraints supports the correct mapping (as is the case in the spatially aligned logic problems).33 GPT-3’s pattern of performance thus displayed many of the characteristics of a human-like analogical mapping process. We also found that GPT-3 was sensitive to contextual information in ways that both improved and impaired its performance, similar to human reasoners (Supplementary Figure 4). It is important to highlight the differences between the Digit Matrices and traditional visual matrix reasoning problems. In order to solve visual matrix reasoning problems, pixel-level inputs must be parsed into objects, and visual attributes (shape, size, etc.) must be disentangled. In the Digit Matrices, the text-based inputs are already 3a 1 2 3 4 5 6 7 8 b [ 5 9 3 ] [ 8 9 2 ] [ 1 9 7 ] [ 8 4 7 ] [ 1 4 3 ] [ 5 4 2 ] [ 1 2 2 ] [ 5 2 7 ] [ ]? [ 1 2 7 ] [ 1 4 3 ][ 5 9 3 ] [ 5 2 7 ][ 8 2 3 ] [ 5 4 2 ] [ 8 9 7 ][ 5 2 3 ] 1 2 3 4 5 6 7 8 c [ 5 ] [ 1 ] [ 9 ] [ 5 ] [ 1 ] [ 9 ] [ 5 ] [ 1 ] [ ? ] Constant d [ 6 ] [ 2 ] [ 4 ] [ 2 ] [ 4 ] [ 6 ] [ 4 ] [ 6 ] [ ? ] Distribution-of-3 e [ 3 ] [ 5 ] [ 7 ] [ 1 ] [ 3 ] [ 5 ] [ 5 ] [ 7 ] [ ? ] Progression f [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] 9 7 9 7 9 7 4 8 9 8 7 4 4 8 4 ? OR (aligned) g [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] 1 0 0 1 5 0 7 1 7 1 7 5 7 ?0 5 OR (permuted) Figure 2: Matrix reasoning problems. (a) Example problem depicting structure of Raven’s Progressive Matrices. 15 Problems consist of a 3 × 3 matrix populated with geometric forms, in which each row or column is governed by the same set of abstract rules. Problem solvers must identify these rules, and use them to infer the missing cell in the lower right, by selecting from the set of 8 choices below. (b) Example problem illustrating the novel Digit Matrices problem set. Problems consist of a 3 × 3 matrix, in which each cell is demarcated by brackets, and populated by digits. The problems are governed by the same rule structure as Raven’s Standard Progressive Matrices. The example problems in (a) and (b) are structurally isomorphic (i.e., governed by the same set of rules). The reader is encouraged to derive the solution to each problem. The solutions to both problems are given in Supplementary Section S1. Problems were governed either by one or more transformation rules (b-e), or by a single logic rule (f,g). (c) Constant rule: same digit appears across either rows or columns. (d) Distribution-of-3 rule: same set of 3 digits appears in each row or column, but with order varied. (e) Progression rule: digits either increase or decrease, by values of 1 or 2, across rows or columns. In the example shown here, digits increase by 2 across rows. (f) OR rule: the set of digits present in a particular row or column are defined as the union of the sets present in the other rows or columns. In the illustrated example, the digits in the second column are formed from the union of the sets in the first and third columns. This example illustrates how the spatial alignment of the corresponding elements can make it easier to intuitively grasp the underlying rule. (g) More challenging logic problem governed by same rule (OR), but in which the corresponding elements are spatially permuted. Other logic problems were governed either by an AND rule or an XOR rule (not pictured). parsed and disentangled, essentially providing GPT-3 (which is not capable of visual processing) with pseudo-symbolic inputs. Interestingly, despite these significant differences, we found that overall error rates for human participants 4a  b c  d  e Figure 3: Matrix reasoning results. GPT-3 matched or exceeded human performance for zero-shot Digit Matrices. (a) Generative accuracy for major problem types, including transformation problems with between one and three rules, and logic problems. (b) Multiple-choice accuracy for major problem types. (c) Two-rule problems with at least one progression rule were more difficult than those without. (d) For three-rule problems, performance was a function of the number of unique rules. (e) Spatially permuted logic problems were more difficult than spatially aligned problems. Human results reflect average performance for N=40 participants (UCLA undergraduates). Black error bars represent standard error of the mean across participants. Each dot represents accuracy for a single participant. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. Note that the rightmost bar in (d) does not show individual scores because each participant only completed a single problem with three unique rules. were very similar for the Digit Matrices vs. the original image-based SPM problem set, and showed a similar pattern across problem types (Figure 4). These results suggest that, while the Digit Matrices do not engage the visual processes involved in traditional SPM problems (i.e., deriving disentangled representations from pixel-level inputs), they likely engage a similar set of core reasoning processes (i.e., inducing abstract rules from those representations). More generally, performance on verbal, visuospatial, and mathematical analogy problems are known to be highly correlated for people. 5 Accordingly, GPT-3’s success on the Digit Matrices can be taken as evidence that it has acquired core capabilities underlying analogy, though it will be important in future work to investigate how these reasoning processes might be integrated with visual processing. 2.2 Letter string analogies A central feature of human analogical reasoning is its flexibility. Human reasoners are capable of identifying abstract similarities between situations even when these situations are superficially quite different. Often this involves a process of re-representation, in which an initial problem representation is revised so as to facilitate the discovery of 5Figure 4: Human performance for Digit Matrices vs. Raven’s Standard Progressive Matrices (SPM). SPM15 does not contain three-rule problems, but performance was very similar across one-rule, two-rule, and logic problems. SPM results reflect average performance for N=80 participants (data from 32). Digit Matrices results reflect average performance for N=40 participants. Error bars represent standard error of the mean. Each dot represents accuracy for a single participant. an analogy.36–38 Hofstadter and Mitchell 16, 39 introduced the letter string analogy domain to evaluate computational models of analogical reasoning, with a particular emphasis on the process of re-representation. The basic problem structure is illustrated in Figure 5a. In this example, the source string ‘a b c d’ has been transformed by converting the final letter to its successor, resulting in the string ‘a b c e’. This transformation must be identified, and then applied to the target string ‘i j k l’, yielding the answer ‘i j k m’. Though this example is simple, letter string problems can be made quite complex by introducing various gener- alizations between the source and target strings. For instance, the target may involve groups of letters rather than individual letters (e.g., ‘i i j j k k l l’), or may involve a sequence with a reversed order relative to the source (e.g., ‘l k j i’). In these cases, the transformation identified in the source (e.g., a successor transformation applied to the final letter in the sequence) must be generalized to an analogous transformation (e.g., a successor transformation applied to the final group of letters , or a predecessor transformation applied to the first letter). This feature makes letter string analogy problems well-suited to test the capacity for re-representation. To evaluate GPT-3, we created a novel letter string problem set (Figure 5), and carried out a systematic com- parison with human participants (N=57, UCLA undergraduates). The problem set involved a range of different transformation (Figure 5d) and generalization types (Figure 5e). Each transformation type could be combined with any generalization type, and multiple generalization types could be combined together to yield more challenging problems (Figure 5b). Problems were presented to GPT-3 along with a prompt (‘Let’s try to complete the pattern:’), using a format similar to the Digit Matrices. Figure 6 shows the results of this evaluation. GPT-3 showed stronger overall performance than human participants (Figure 6a; logistic regression, main effect of GPT-3 vs. human participants: OR = 1 .76, p = 6 .3 × 10−5, CI = [1.34, 2.31]), an effect that was driven primarily by stronger performance on zero-generalization problems (main effect of GPT-3 vs. human participants for zero-generalization problems: OR = 1 .76, p = 0.0007, CI = [1.27, 2.46]). Performance was strongly affected by the number of generalizations in both GPT-3 and human participants (main effect of number of generalizations, GPT-3: OR = 0 .51, p = 2 × 10−16, CI = [0 .45, 0.57]; human participants: OR = 0.66, p = 5.9 × 10−16, CI = [0 .6, 0.73]). GPT-3 and human participants also showed similar error patterns across transformation types (Figure 6b) and generalization types (Figure 6c), as quantified by a correlation analysis for accuracy across different problem subtypes ( r(39) = 0.7, p = 3.6 × 10−7). We also investigated a novel variant on letter string problems involving generalization from letters to real-world concepts (Figure 5c). GPT-3 showed strong performance on these problems, though with some discrepancies for different transformation types (Figure 6d). These results suggest that GPT-3 has developed an abstract notion of successorship that can be flexibly generalized between different domains (e.g., alphabetic successorship vs. temper- 6a a b c d → a b c e       i j k l →       ? b x l x l x k x k x j x j x i x i →        a b c d → a b c e       ? c cold cool warm →        a b c → a b c       ? d a b c d → a b c d e Extend sequence a b c d → a b c e Successor b c d e → a c d e Predecessor a b b c d e → a b c d e Remove redundant letter a b c w e → a b c d e Fix alphabetic sequence a d c b e → a b c d e Sort Transformation types e a b c d → a b c e       Letter-to-number 1 2 3 4 →       ? a b c d → a b c e       Grouping i i j j k k l l →       ? a b c d → a b c e       Longer target i j k l m n o p →       ? a b c d → a b c e       Reversed order l k j i →       ? a b c d → a b c e       Interleaved distractor i x j x k x l x →       ? a b c d → a b c e       Larger interval i k m o →       ? Generalization types Figure 5: Letter string analogy problems. Transformation between source strings must be identified and applied to target string. Mapping between source and target may involve one or more generalizations. (a) Easy problem involving zero generalizations. (b) Difficult problem involving three generalizations (grouping, reversed order, and interleaved distractors). (c) Problem involving generalization from letters to real-world concepts. (d) Transformations were sampled from set of six possible types: sequence extension, successor transformation (applied to the last letter in the string), predecessor transfor- mation (applied to the first letter in the string), removal of a redundant letter, ‘fixing’ an alphabetic sequence (replacing an out-of-place letter), and sorting. (e) Generalizations were sampled from set of six possible types: letter-to-number, grouping, longer target string, reversed order, interleaved distractors, and larger interval. ature successorship). One important caveat is that GPT-3’s performance on this task was somewhat sensitive to the way in which prob- lems were formatted. For instance, performance suffered when no prompt was provided (Supplementary Figure 5a), or when problems were presented in the form of a complete sentence (Supplementary Figure 5b). However, even in these cases, GPT-3’s zero-shot performance was both within the range of human participants (within one standard deviation), and closely matched the pattern of human performance across problem types (correlation analysis, no prompt: r(39) = 0.6, p = 5.3 × 10−5, sentence format: r(39) = 0.76, p = 4.2 × 10−6). 2.3 Four-term verbal analogies Though matrix reasoning and letter string analogies involve a high degree of relational complexity, one limitation is that they consist of highly constrained, synthetic relations, such as alphabetic or numerical successorship. GPT-3’s ability to solve problems involving more real-world concepts (e.g., ‘a b c → a b d, cold cool warm → ?’) suggests that its analogical capabilities may not be limited to such artificial settings. To further evaluate GPT-3’s capacity to reason about real-world relational concepts, we tested it on four-term verbal analogy problems involving a broader range of semantic relations. 7a  b c  d Figure 6: Letter string analogy results. GPT-3 displayed strong performance on letter string problems, and showed a similar pattern to human participants across conditions. (a) GPT-3 and human performance as a function of the number of generalizations between source and target. (b) Performance on zero-generalization problems as a function of transformation type. (c) Performance on one-generalization problems as a function of generalization type. (d) Performance on problems requiring generalization from letters to real-world concepts. Human results reflect average performance for N=57 participants (UCLA undergraduates). Black error bars represent standard error of the mean across participants. Each dot represents accuracy for a single participant. Note that (b-d) do not show individual participant results because each participant only completed one problem in each condition. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. We evaluated GPT-3 on four separate datasets.17, 19–21 To the best of our knowledge, these constitute an exhaus- tive set of four-term verbal analogy problems for which human behavioral data is available. 41 Each dataset contains a series of four-term analogy problems in the form ‘A:B::C:?’, together with a set of answer choices (i.e., potential choices of D). For each problem, GPT-3 was evaluated by presenting the problem together with each potential answer choice, and selecting the option for which GPT-3 assigned a higher log probability. The problem and GPT-3’s choice 8a  b c  d Figure 7: Verbal analogy results. (a) Results for UCLA Verbal Analogy Test (VAT). 17 Human results reflect average performance for N=57 participants. Black error bars represent standard error of the mean. Each dot represents accuracy for a single participant. (b) Results for dataset from Sternberg and Nigro. 19 Human results reflect average performance for N=20 participants. (c) Results for SAT analogy problems from Turney et al.20 These problems involve five answer choices, and thus chance performance is 20%. Human results reflect an estimate of the average performance for high school students taking the SAT (see40 for details). (d) Results for dataset from Jones et al. 21 Human results reflect average performance for N=241 participants. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. Gray horizontal lines represent chance performance. were then appended to the context window for the next problem, thereby simulating any contextual effects that might arise when solving multiple problems in a row, as human participants typically do. Figure 7 shows the results for all datasets. GPT-3 performed as well or better than human participants (minimum education level of high-school graduation, located in the United States and recruited using Amazon Mechanical Turk) on the UCLA Verbal Analogy Test (VAT), 17 involving categorical, functional, antonym, and synonym relations (Figure 7a), and on a dataset from Sternberg and Nigro 19 involving these same four relation types and linear order relations (Figure 7b). On a dataset of SAT analogy problems from Turney et al., 20 GPT-3 surpassed the estimated average level of performance for high school students taking the SAT (Figure 7c). GPT-3 also showed performance in the same range as human participants (though numerically weaker) on a problem set from Jones et al.21 involving categorical, compositional, and causal relations (Figure 7d). In addition to displaying generally strong performance on these problem sets, GPT-3 also displayed sensitivity 9to semantic content similar to that observed in human participants. In the dataset from Jones et al. 21 (Figure 7d), participants performed worse on problems in which the analogs were semantically distant (i.e., the A and B terms had low semantic similarity to C and D), an effect that was also displayed by GPT-3 (logistic regression, effect of semantic distance for GPT-3: OR = 3 .24, p = 0.0165, CI = [1 .24, 8.5]). These results align with a more general phenomenon in which human reasoning is facilitated by semantically meaningful or coherent content. 24, 42 Figure 8: Story analogy results. Results for identification of analogies between stories, using materials from Gentner et al.18 When presented with a source story and two target stories, both GPT-3 and human participants showed a preference for target stories that shared higher-order relations with the source vs. those that only shared first-order relations. Near analogy condition involves within-domain comparison between stories with similar entities. Far analogy condition involves cross-domain comparison between stories with different entities. Human results reflect average performance for N=54 participants (UCLA undergraduates). Black error bars represent standard error of the mean across participants. Each dot represents accuracy for a single participant. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. Gray horizontal line represents chance performance. 2.4 Story analogies Human reasoners are able not only to form analogies between individual concepts, but can also identify correspon- dences between complex real-world events, involving many entities and relations. When making such comparisons, human reasoning is especially sensitive to higher-order relations – relations between relations – notably causal rela- tions between events. Such higher-order relations play a central role in some cognitive theories of analogy, 43 and it is thus important to establish whether GPT-3 displays a similar sensitivity to them. To address this question, we tested GPT-3 on a set of story analogies from Gentner et al. 18 In each set, a source story is compared to two potential target stories, each of which is matched with the source story in terms of first-order relations, but only one of which shares the same causal relations as the source (see Methods Section 4.6.1 for examples). Gentner et al. found that human participants rated the target stories as more similar when they shared the same causal relations as the source story. These problems are further defined by two different comparison conditions. In the near analogy condition (referred to as ‘literal similarity’ vs. ‘mere appearance’ by Gentner et al.), the target stories also share the same basic entities as the source story, making for a less abstract, within-domain comparison. In the far analogy condition (referred to as ‘true analogy’ vs. ‘false analogy’ by Gentner et al.), the target stories involve different entities from the source story, but share first-order relations, resulting in a more challenging, cross-domain comparison. To facilitate a direct comparison with GPT-3, we performed a new behavioral study with these materials. For each source story, participants indicated which of two target stories was more analogous. Both GPT-3 and human 10participants (N=54, UCLA undergraduates) showed a sensitivity to higher-order relations (Figure 8), most often selecting the target story that shared causal relations with the source (combined near and far analogy; GPT-3, binomial test: p = 0.0005; human participants, one-sample t-test: t(53) = 21.3, p = 1.1 × 10−27; null hypotheses for both tests is chance-level performance of 0.5). This effect was significant for both GPT-3 and human participants in the near analogy condition (GPT-3, binomial test: p = 0.0039; human participants, one-sample t-test: t(53) = 21.5, p = 8 .5 × 10−28), but only human participants showed a significant effect in the far analogy condition (GPT-3, binomial test: p = 0.065; human participants, one-sample t-test: t(53) = 16.7, p = 9.3 × 10−23). Unlike the other task domains considered in the present work, this was a case in which college students clearly outperformed GPT-3 (logistic regression, main effect of GPT-3 vs. human participants: OR = 0 .37, p = 0.0003, CI = [0 .21, 0.63]). Indeed, a significant number of participants (15/54) selected the analogous story on every trial. However, in an initial investigation of GPT-4, 44 we found that it displays stronger performance on this task, more robustly picking the analogous story even in the far analogy condition, and displaying nearly perfect performance in the near analogy condition (Supplementary Figure 6, Section S4.3). It therefore seems likely that further scaling of large language models will enhance their sensitivity to causal relations. 2.5 Analogical problem-solving In everyday thinking and reasoning, analogical comparisons are often made for the purpose of achieving some goal, or solving a novel problem. Thus far, our tests of GPT-3 have assessed its capacity for identifying analogies in text- based inputs with varying formats, but can GPT-3 also use these analogies to derive solutions to novel problems, as human reasoners do? As a preliminary investigation of this issue, we performed a qualitative evaluation using a paradigm developed by Gick and Holyoak.22 In that paradigm, participants are presented with a target problem in the form of a story. In the original study, Duncker’s radiation problem was used.45 In that problem, a doctor wants to use radiation to destroy a malignant tumor, but destroying the tumor with a single high-intensity ray will also damage the surrounding healthy tissue. The solution – to use several low-intensity rays that converge at the site of the tumor – is rarely identified spontaneously, but participants are more likely to discover this solution when they are first presented with an analogous source story. In the original study, the source story involved a general who wants to capture a fortress ruled by an evil dictator, but cannot do so by sending his entire army along a single road, which would trigger landmines. The general instead breaks his army up into small groups that approach the fortress from multiple directions, thus avoiding triggering the mines. We first presented GPT-3 with the target problem in isolation. GPT-3 proposed a solution that involved injecting a radiation source directly into the tumor, rather than identifying the intended solution based on the convergence of multiple low-intensity radiation sources (Supplementary Section S5.1). However, when first presented with the general story, followed by the target problem, GPT-3 correctly identified the convergence solution (Supplementary Section S5.2). GPT-3 was further able to correctly explain the analogy, and to identify the specific correspondences between the source story and target problem when prompted (e.g., general ↔ doctor, dictator ↔ tumor, army ↔ rays). We also found similar results when using distinct source analogs taken from another study 46 (Supplementary Section S5.3). In a more challenging version of this paradigm, participants were first presented with both the general story, and two other non-analogous stories intended to serve as distractors. In this context, human participants were much less likely to identify the convergence solution. However, when given a prompt to explicitly consider the previously presented stories when trying to solve the radiation problem, participants were often able to correctly identify the analogous general story, and use this analogy to devise the convergence solution. Remarkably, we found that GPT-3 displayed these same effects. When presented with these same distracting, non-analogous stories, GPT-3 no longer identified the convergence solution, instead proposing the same solution that it proposed in response to the radiation problem alone (Supplementary Section S5.4). But when prompted to consider the previous stories, GPT-3 both correctly identified the general story as most relevant, and proposed the convergence solution (Supplementary Section S5.5). We also evaluated GPT-3 using materials from a developmental study that employed a similar paradigm. 23 In that study, children were tasked with transferring gumballs from one bowl to another bowl that was out of reach, and provided with a number of materials for doing so (e.g., a posterboard, an aluminum walking cane, a cardboard tube), permitting multiple possible solutions. The key result was that when children were first presented with an analogous source story (about a magical genie trying to transfer jewels between two bottles), they were more likely 11to identify a solution to the target problem that was analogous to the events described in the source story. When presented with this target problem, GPT-3 mostly proposed elaborate, but mechanically nonsensical so- lutions, with many extraneous steps, and no clear mechanism by which the gumballs would be transferred between the two bowls (Supplementary Sections S5.6-S5.8). However, when asked to explicitly identify an analogy between the source story and target problem, GPT-3 was able to identify all of the major correspondences, even though it could not use this analogy to discover an appropriate solution. This finding suggests that GPT-3’s difficulty with this problem likely stems from its lack of physical reasoning skills, rather than being due to a difficulty with analogical mapping per se. It is also worth noting that in the original study, this task was presented to children with real physical objects, which likely aided the physical reasoning process relative to the purely text-based input provided to GPT-3. Overall, these results provide some evidence that GPT-3 is capable of using analogies for the purposes of problem-solving, but its ability to do so is constrained by the content about which it can reason, with particular difficulty in the domain of physical reasoning. 3 Discussion We have presented an extensive evaluation of analogical reasoning in a state-of-the-art large language model. We found that GPT-3 appears to display an emergent ability to reason by analogy, matching or surpassing human performance across a wide range of text-based problem types. These included a novel problem set (Digit Matrices) modeled closely on Raven’s Progressive Matrices, where GPT-3 both outperformed human participants, and captured a number of specific signatures of human behavior across problem types. Because we developed the Digit Matrix task specifically for this evaluation, we can be sure GPT-3 had never been exposed to problems of this type, and therefore was performing zero-shot reasoning. GPT-3 also displayed an ability to solve analogies based on more meaningful relations, including four-term verbal analogies and analogies between stories describing complex real-world events. It is certainly not the case that GPT-3 mimics human analogical reasoning in all respects. Our tests were limited to processes that can be carried out within a local temporal context, but humans are also capable of retrieving potential source analogs from long-term memory, and ultimately of developing new concepts based on the comparison of multiple analogs. Unlike humans, GPT-3 does not have long-term memory for specific episodes. It is therefore unable to search for previously-encountered situations that might create useful analogies with a current problem. For example, GPT-3 can use the general story to guide its solution to the radiation problem, but as soon as its context buffer is emptied, it reverts to giving its non-analogical solution to the problem – the system has learned nothing from processing the analogy. GPT-3’s reasoning ability is also limited by its lack of physical understanding of the world, as evidenced by its failure (in comparison with human children) to use an analogy to solve a transfer problem involving construction and use of simple tools. GPT-3’s difficulty with this task is likely due at least in part to its purely text-based input, lacking the multimodal experience necessary to build a more integrated world model. 47 Finally, we found GPT-3 was limited in its ability to evaluate analogies based on causal relations, particularly in cross-domain comparisons between stories (far analogy). But despite these major caveats, our evaluation reveals that GPT-3 exhibits a very general capacity to identify and generalize – in zero-shot fashion – relational patterns to be found within both formal problems and meaningful texts. These results are extremely surprising. It is commonly held that although neural networks can achieve a high level of performance within a narrowly-defined task domain, they cannot robustly generalize what they learn to new problems in the way that human learners do. 6, 48–50 Analogical reasoning is typically viewed as a quintessential example of this human capacity for abstraction and generalization, allowing human reasoners to intelligently approach novel problems zero-shot. Our results indicate that GPT-3 – unlike any other neural network previously tested on analogy problems – displays a capacity for such zero-shot analogical reasoning across a broad range of tasks. The deep question that now arises is how GPT-3 achieves the analogical capacity that is often considered the core of human intelligence. One possibility is that, perhaps as a result of the sheer size and diversity of GPT-3’s training data, it has been forced to develop mechanisms similar to those thought to underlie human analogical reasoning – despite not being explicitly trained to do so. The consensus among cognitive scientists working on analogy is that this human ability depends on systematic comparison of knowledge based on explicit relational representations. It is unclear whether and how GPT-3 would implement these processes. Does GPT-3 possess some form of emergent relational representations, and if so, how are they computed? Does it perform a mapping process similar to the type that plays a central role in cognitive theories of analogy 43? A few properties of the transformer architecture, 29 on which GPT-3 and other large language models are based, 12are worth considering here. The first is the central role played bysimilarity. Transformers are built on a self-attention operation, which involves explicitly computing the similarity between each pair of vectors in the inputs to each layer. This pairwise evaluation of similarity is also a key feature of cognitive models of analogy, where it provides the primary constraint guiding the process of analogical mapping. In traditional symbolic models, 51 this takes the form of literal identicality between symbols, but in more recent models,52, 53a graded similarity function that operates over vector-based inputs is used, much like the self-attention operation in transformers. Second, transformer self-attention employs a form of indirection, in which one set of embeddings is used to reference another set of embeddings (i.e., keys vs. values) – arguably a form of variable binding. Cognitive scientists have long hypothesized that variable binding plays a central role in analogical reasoning, and abstract reasoning more broadly, as it potentially allows generalization of abstract roles across different contexts.48, 54–58 It may be that these features of the transformer make it better equipped to perform zero-shot reasoning than other neural architectures. This possibility aligns with recent evidence that the transformer architecture is an important factor contributing toward the emergence of few-shot learning.27 But although the mechanisms incorporated into large language models such as GPT-3 may have some important links to building blocks of human reasoning, we must also entertain the possibility that this type of machine intelli- gence is fundamentally different from the human variety. Humans have evolved to reason within bounds imposed by limited computational power and biological constraints .59 Thus, we tend to approach complex problems by breaking them into a set of simpler problems that can be solved separately, 60 an approach that plays a particularly important role in solving challenging analogy problems such as Raven’s Matrices. 61 It is possible that GPT-3, through sheer computational scale, is able to solve such complex problems in a holistic and massively parallel manner, without the need to segment them into more manageable components. It must also be noted that, regardless of the extent to which GPT-3 employs human-like mechanisms to perform analogical reasoning, we can be certain that it did not acquire these mechanisms in a human-like manner. LLMs receive orders of magnitude more training data than do individual human beings (at least if we consider linguistic inputs alone), 59 and so they cannot be considered as models of the acquisition of analogical reasoning over the course of human development. Nor can they be considered good models of the evolution of analogical reasoning, as their analogical abilities are derived entirely from being trained to predict human-generated text. Human natural language is replete with analogies; accurately predicting natural language therefore likely requires an ability to appreciate analogies. But there is no reason to suppose that the same system, absent human-generated inputs, would spontaneously develop a disposition to think analogically, as apparently happened at some point in human evolution.62 Thus, to the extent that large language models capture the analogical abilities of adult human reasoners, their capacity to do so is fundamentally parasitic on natural human intelligence. Nevertheless, the present results indicate that this approach may be sufficient to approximate human-like reasoning abilities, albeit through a radically different route than that taken by biological intelligence. 4 Methods The present research complied with all relevant ethical regulations, and human behavioral experiments were approved by the UCLA Institutional Review Board (IRB protocol #22-000841, approved May 17, 2022). 4.1 Code Most code was written in Python v3.9.6, using the following packages: NumPy v1.24.3,63 SciPy v1.10.1,64 statsmodels v0.13.5,65 Matplotlib v3.7.1, 66 and pandas v2.0.1. 67 Logistic regression analyses were carried out in R v4.2.2. 68 Experimental stimuli for human behavioral experiments were written in JavaScript using jsPsych v7.2.1. 69 4.2 GPT-3 We queried GPT-3 in an automated fashion through the OpenAI API. All simulations reported in the main text employed the text-davinci-003 model variant. Additional simulations, reported in the Supplementary Results, also employed the davinci, code-davinci-002, and text-davinci-002 variants. The temperature was set to 0 in all simula- tions. We set max tokens (the parameter controlling the maximum number of generated tokens for a given prompt) to 10 for Digit Matrices, 40 for letter string analogies, 10 for four-term verbal analogies, and 256 for story analogies and analogical problem-solving. All other parameters were set to their default values. 13For each prompt, GPT-3 generates a proposed completion (a string of tokens), and assigns log probabilities to each token in the prompt and the completion. We used these log probabilities to evaluate GPT-3 on multiple-choice problems. For each choice in a given problem, we concatenated the problem with the choice, and treated the average log probability assigned to the choice tokens as a score, selecting the answer choice with the highest score. This approach was used for Digit Matrices and four-term verbal analogies. 4.3 Digit Matrices 4.3.1 Dataset The digit matrix problems consisted of two major problem categories: transformation and logic problems. Trans- formation problems contained anywhere from one to five rules, whereas logic problems each contained only a single rule. Transformation problems were defined using a combination of three rule types: constant, distribution-of-3, and progression. The constant rule was defined by the same digit appearing across either rows or columns. The following example shows an instance of a column-wise constant rule (correct answer: ‘9’): [5] [1] [9] [5] [1] [9] [5] [1] [?] The distribution-of-3 rule was defined by the same set of three digits appearing in each row or column, but with the order permuted. In the following example, the digits 6, 2, and 4 appear in each row (correct answer: ‘2’): [6] [2] [4] [2] [4] [6] [4] [6] [?] The progression rule was defined by a progressive increase or decrease in value, in units of either 1 or 2, across either rows or columns. In the following example, digits increase by units of 2 across rows (correct answer: ‘9’): [3] [5] [7] [1] [3] [5] [5] [7] [?] Transformation rules could be combined to form multi-rule problems, by assigning each rule to a particular spatial location within each cell. The following example shows a two-rule problem, in which the left digit in each cell is governed by a progression rule (digits decrease by units of 1 across columns), and the right digit in each cell is governed by a distribution-of-3 rule (correct answer: ‘4 9’): [7 1] [8 9] [6 3] [6 9] [7 3] [5 1] [5 3] [6 1] [ ? ] Logic problems were defined by one of three rules: OR, XOR, and AND. In the OR rule, a particular row or column contained all entities that appeared in either of the other rows or columns. In the following example, the middle column contains all entities that appear either in the left or right columns (correct answer: ‘8’): [ 7] [ 7 4 ] [4 ] [9 7] [9 7 4 8] [4 8] [9 ] [9 8] [ ? ] The XOR rule was the same, except that entities appearing in both of the other rows or columns were excluded. In the following example, only items that appear in either the left or middle columns, but not both, will appear in the right column (correct answer: ‘4 3’): [6 4] [6 1] [4 1] [6 1] [3 6] [1 3] [4 1] [1 3] [ ? ] 14In the AND rule, a particular row or column contained only entities that appeared in both of the other rows or columns. In the following example, the left column contains only digits that appear in both the left and middle columns (correct answer: ‘9’): [2 9 7] [1 9 7] [ 9 7] [2 9 5] [1 9 5] [ 9 5] [2 9 ] [1 9 ] [ ? ] For some logic problems, the within-cell spatial position of corresponding elements was aligned, as in the previously presented OR and AND problems. In other logic problems, corresponding elements were spatially permuted. The following example (involving an OR rule) illustrates how this makes it more difficult to intuitively grasp the underlying rule (correct answer: ‘0’): [ 1] [7 1 ] [7 ] [1 0] [5 0 7 1] [7 5] [0 ] [ 0 5] [ ? ] Within each problem type (one- through five-rule and logic problems), there were a number of specific problem subtypes. There were 6 one-rule subtypes, 6 two-rule subtypes, and 10 subtypes for three-rule, four-rule, five-rule, and logic problems. We generated 100 instances of each subtype (except in the case of progression problems, for which there were fewer possible problem instances). The one-rule problem subtypes consisted of a row-wise constant problem, a column-wise constant problem, two distribution-of-3 problems, and two progression problems (one with an increment of 1 and one with an increment of 2). The two- and three-rule problem subtypes consisted of all possible combinations of two or three rules (allowing for the same rule to be used multiple times within each problem). The four- and five-rule problem subtypes were sampled from the set of all possible combinations of four or five rules. There were five spatially aligned logic problem subtypes, and five spatially permuted logic problem subtypes. Three out of each of these five subtypes were OR problems (defined by the row or column in which the set union appeared), and the other two were AND and XOR problems. For each problem, we also procedurally generated a set of 7 distractor choices, making for a set of 8 total answer choices. Distractors were generated using different methods for the transformation and logic problems. These methods were chosen based on the approach of Matzen et al., 32 who performed an analysis of the answer choices in the original SPM. For transformation problems, the following methods were used to generate distractors: 1. Sample a random cell from the problem. 2. Sample a random cell from the problem, sample a random digit within that cell, and apply an increment or decrement of either 1 or 2. 3. Start with the correct answer, apply an increment or decrement of either 1 or 2 to a randomly sampled digit. 4. Randomly sample a previously generated distractor for this problem, apply an increment or decrement of either 1 or 2 to a randomly sampled digit. 5. Randomly generate a new answer choice (with the appropriate number of digits given the problem type). For multi-rule transformation problems, the following additional methods were also used: 1. Start with the correct answer, randomly permute the digits. 2. Sample a random cell from the problem, randomly permute the digits. 3. Randomly sample a previously generated distractor for this problem, randomly permute the digits. 4. Randomly sample digits from multiple cells within the problem and combine. 5. Randomly sample digits from previously generated distractors for this problem and combine. For logic problems, distractors were generated by sampling from the set of all possible subsets of elements that appeared within the problem, including the empty set (the correct answer was an empty set on some logic problems), but excluding the correct answer. For spatially permuted logic problems, the spatial position of the elements within each distractor was randomly permuted. For spatially aligned logic problems, the order of the elements within each distractor was chosen so as to be consistent with the order that they appeared in the problem. 154.3.2 Human behavioral experiments Human behavioral data was collected in two online experiments. All experiments were approved by the UCLA Insti- tutional Review Board (IRB protocol #22-000841, approved May 17, 2022), and all participants provided informed consent. All participants were UCLA undergraduates. Forty-three participants completed the first experiment, but three participants were excluded from analysis due to the fact that they got nearly every answer incorrect, and produced an apparently random pattern of responses (e.g. random permutations of the same three digits for all problems). The remaining 40 participants (31 female, 18-35 years old, average age = 21.3 years old) were included in our analysis. Forty-seven participants (37 female, 18-42 years old, average age = 21.2 years old) completed the second experiment. No statistical methods were used to pre-determine sample sizes. There was no overlap between the participants in the first and second experiments. Participants received course credit for their participation. In both experiments, participants were first presented with a set of instructions, and a single one-rule example problem involving a constant rule. For each problem, participants first generated a free-response answer, and then selected from the set of answer choices. Problems were presented in a spatially arranged matrix format, as they appear in Figure 2. Problems remained on the screen until participants made a response. In the first experiment (Figure 3), participants were presented with one-, two-, three-rule, and logic problems. There were 6 problem subtypes each for the one- and two-rule problems, and 10 problem subtypes each for the three- rule and logic problems, making for 32 problem subtypes in total. Participants received these problem subtypes in random order. Each participant received randomly sampled instances of each problem subtype. In the second experiment (Supplementary Figure 4), participants were presented with one- through five-rule problems. There were 6 problem subtypes each for the one- and two-rule problems, and 10 problem subtypes each for the three- through five-rule problems, making for 42 problem subtypes in total. Problems were presented in order of increasing complexity, with all one-rule problem subtypes first, followed by all two-rule problem subtypes, and so on. For one-rule problems, the two constant problems were presented first, followed by the two distribution-of-3 problems, followed by the two progression problems. 4.3.3 Evaluating GPT-3 GPT-3 was evaluated on the Digit Matrices by presenting each complete problem as a prompt, including brackets and line breaks, followed by an open bracket at the start of the final cell. For example, the three-rule problem in Figure 2b would be presented to GPT-3 in the following format: [5 9 3] [8 9 2] [1 9 7] \\n[8 4 7] [1 4 3] [5 4 2] \\n[1 2 2] [5 2 7] [ GPT-3’s generated responses were truncated at the point where a closing bracket was generated. For logic problems, generated answers were counted as correct if they contained the correct set of digits, regardless of their order. For transformation problems, generated answers were only counted as correct if they contained the correct digits in the correct order. The same criteria were applied when evaluating human responses. To evaluate GPT-3’s multiple-choice performance, for each answer choice, the choice was appended to the problem followed by a closing bracket, and presented to GPT-3 as a prompt. The average log probability of the tokens corresponding to the answer choice (not counting the brackets) was computed. The answer choice with the highest average log probability was treated as GPT-3’s selection. In our primary evaluation (Figure 3), GPT-3 was presented with 40 problem instances from each of the 32 problem subtypes used in the first human behavioral experiment. GPT-3 solved each one zero-shot (without any fine-tuning or in-context learning). We also evaluated how GPT-3 performed when presented with problems in order of increasing complexity (Sup- plementary Figure 4). GPT-3 performed 20 runs on this task. For each run, GPT-3 was presented with a series of the same 42 problem subtypes used in the second human behavioral experiment (with different instances of these subtypes in each run). After GPT-3 answered each problem, the selected multiple-choice answer was appended to the problem, and the combined problem and answer choice were recursively appended to the prompt for the next problem. This meant that the size of the prompt grew with each problem. For some of the final five-rule problems, the prompt exceeded the size of GPT-3’s context window (4096 tokens). When this occurred, problems from the beginning of the context window were deleted until the entire prompt fit within the window. This resulted in the deletion of a few one-rule problems from the beginning of the prompt. For one-rule problems, the two constant problems were presented first, followed by the two distribution-of-3 rules, followed by the two progression problems. 164.3.4 Statistical analyses Results were analyzed using both regression and correlation analyses. Logistic regression analyses were carried out at the individual trial level, with each data point corresponding to a particular trial from a particular participant (or GPT-3). The dependent variable in all regression analyses was a binary variable coding for whether a particular response was correct or incorrect. For the first digit matrix experiment, we fit separate regression models for generative vs. multiple-choice responses. Two predictors were used: problem type (one-, two-, three-rule, and logic problems), and a binary predictor coding for GPT-3 vs. human participants. We also performed more fine-grained analyses for generative responses within each problem type. These analyses were performed separately for GPT-3 vs. human responses. For two-rule problems, a single binary predictor coded for whether a problem contained a progression rule. For three-rule problems, a single predictor coded for the number of unique rules present in a given problem. For logic problems, a binary predictor coding for whether a problem was spatially aligned vs. permuted. We also fit regression models comparing the results of the first and second experiments. These analyses were performed separately for GPT-3 vs. human responses, and only included responses for one- to three-rule problems (since these were the only problem types in common between the two experiments). Two predictors were used: problem type (one-, two-, and three-rule problems), and experiment (experiment 1 vs. 2). Correlation analyses were carried out by correlating the accuracy for GPT-3 vs. human participants across all 32 problem subtypes. 4.4 Letter string analogies 4.4.1 Problem set Each letter string analogy problem involved one of six transformation types: sequence extension, successor, predeces- sor, removing a redundant letter, fixing an alphabetic sequence, and sorting. In the sequence extension transforma- tion, the source involved an alphabetically ordered sequence of four letters followed by an extension of this sequence involving five letters, as in the following example: [a b c d] [a b c d e] In the successor transformation, the source involved an alphabetically ordered sequence of four letters, followed by that same sequence, but with the final letter replaced by its successor, as in the following example: [a b c d] [a b c e] In the predecessor transformation, the source involved an alphabetically ordered sequence of four letters, followed by that same sequence, but with the first letter replaced by its predecessor, as in the following example: [b c d e] [a c d e] In the transformation involving removal of a redundant letter, the source involved an alphabetically ordered sequence of five letters with one letter repeated, followed by that same sequence with the redundant letter removed, as in the following example: [a b b c d e] [a b c d e] In the transformation involving fixing an alphabetic sequence, the source involved an alphabetically ordered sequence of five letters with one out-of-place letter (not part of the alphabetic sequence), followed by that same sequence with the out-of-place letter replaced, as in the following example: [a b c w e] [a b c d e] In the sorting transformation, the source involved an alphabetically ordered sequence of five letters with the position of two letters swapped, followed by a sorted version of the same sequence, as in the following example: [a d c b e] [a b c d e] 17Problems involved varying degrees of generalization between the source and target. In the zero-generalization problems, the target involved a different instance of the source transformation (instantiated with different letters). Transformation parameters (e.g., the location of the redundant letter) were independently sampled for source and target. Generalization problems involved generalizations sampled from the following set of generalization types: gener- alization from letters to numbers, grouping, generalization to a longer target, reversed order, interleaved distractors, and generalization to a larger interval. In the letter-to-number generalization, target letters were replaced by numbers corresponding to their alphabetic indices, as in the following example: [a b c d] [a b c d e] [7 8 9 10] [ ? ] In the grouping generalization, target letters were replaced by groups with two instances of each letter, as in the following example: [a b c d] [a b c d e] [i i j j k k l l] [ ? ] In the longer target generalization, the target sequence was replaced with a sequence that was twice as long as the source, as in the following example: [a b c d] [a b c d e] [i j k l m n o p] [ ? ] In the reversed order generalization, the order of the target letters was reversed relative to the source, as in the following example: [a b c d] [a b c d e] [l k j i] [ ? ] In the interleaved distractor generalization, the letter ‘x’ was interleaved between each letter in the target sequence, as in the following example: [a b c d] [a b c d e] [i x j x k x l x] [ ? ] In the larger interval generalization, the sequence of target letters was replaced with a sequence involving an interval of size 2, as in the following example: [a b c d] [a b c d e] [i k m o] [ ? ] Each transformation type could be combined with any generalization type. Multiple generalizations could also be combined together. Generalization problems contained between one and three generalizations. We generated a set of 600 zero-generalization problems (involving 100 problems with each transformation type), 600 one-generalization problems (involving 100 problems with each generalization type, with randomly sampled transformation type), and 600 problems each with two and three generalizations (with randomly sampled combinations of transformation and generalization type). We also generated a separate set of problems involving generalization from letters to real-world concepts. In these problems, the source instantiated a transformation using letters, and the target instantiated that same transformation using real-world instances of successorship. These problems involved shorter sequences (maximum length of four), due to the difficulty of identifying real-world instances of successorship with more than four points. The following sequences were used: cold cool warm hot love like dislike hate jack queen king ace penny nickel dime quarter second minute hour day The transformation types included sequence extension, successor, predecessor, and sorting. No other generalizations were applied to these problems. We generated 100 problems with each transformation type. 184.4.2 Evaluating GPT-3 We presented letter string analogies to GPT-3 using the prompt ‘Let’s try to complete the pattern:’, similar to. 70 We also formatted each analogy problem using brackets and line breaks, similar to the presentation format of the Digit Matrices. The presentation format is illustrated in the following example: Let’s try to complete the pattern: \\n\\n[a b c d] [a b c e] \\n[i j k l] [ GPT-3’s generated responses were truncated at the point where a closing bracket was generated. We also evaluated GPT-3 with two alternative problem formats: 1) no prompt, and 2) a sentence format, as in the following example: If a b c d changes to a b c e, then i j k l should change to For this format, GPT-3’s generated responses were truncated at the point where a period was generated. We eval- uated GPT-3 on 300 zero-generalization problems (50 problems for each transformation type), 300 one-generalization problems (50 problems for each generalization type), and 300 problems each with two and three generalizations. We also evaluated GPT-3 on 50 real-world concept generalization problems for each transformation type. 4.4.3 Human behavioral experiment Human behavioral data was collected in an online experiment. The experiment was approved by the UCLA Insti- tutional Review Board (IRB protocol #22-000841, approved May 17, 2022), and all participants provided informed consent. All participants were UCLA undergraduates. Fifty-seven participants (50 female, 18-35 years old, average age = 21.1 years old) completed the experiment. No statistical methods were used to pre-determine sample sizes. Participants received course credit for their participation. Participants were first presented with a set of instructions, and the following example problem (not involving any of the transformations or generalizations employed in the actual experiment): [a a a] [b b b] [c c c] [ ? ] Each participant completed 28 problems, including 6 zero-generalization problems (1 problem for each transfor- mation type), 6 one-generalization problems (1 problem for each generalization type), 6 problems each with two and three generalizations, and 4 real-world concept generalization problems (1 for each transformation type). The specific problem instances were randomly sampled for each participant, and participants received these problems in a random order. Participants generated a free response for each problem. 4.4.4 Statistical analyses Results were analyzed using both regression and correlation analyses. Logistic regression analyses were carried out at the individual trial level, with each data point corresponding to a particular trial from a particular participant (or GPT-3). The dependent variable in all regression analyses was a binary variable coding for whether a particular response was correct or incorrect. Separate analyses were performed for problems that only involved alphanumeric characters vs. those that involved real-world concepts. For problems involving alphanumeric characters, a regression model was fit with two predictors: number of generalizations (zero to three), and a binary predictor coding for GPT-3 vs. human participants. We also fit regression models at each generalization level with a single binary predictor coding for GPT-3 vs. human participants. For real-world concept problems, a regression model was fit with a predictor coding for GPT-3 vs. human participants. For correlation analyses, problem subtypes were defined based on each combination of transformation type and generalization type. The accuracy for each subtype was computed for GPT-3 vs. human participants, and these values were subjected to correlation analysis. There were only a few examples of some problem subtypes (across all participants), especially for problems with more generalizations (the space of possible subtypes grows exponentially with the number of generalizations). We only included subtypes for which there were at least five trials from human participants (across all participants) and five trials from GPT-3. Out of the 252 possible problem subtypes, 41 subtypes met this criterion and were included in the analysis. 194.5 Four-term verbal analogies We evaluated GPT-3 on four separate four-term analogy datasets. 17, 19–21 The UCLA-VAT dataset contains 80 problems, with four relation types: categorical (B/D is a member of the category A/C), functional (A/C is the function of B/D), antonym, and synonym. There are 20 problems for each relation type. Each problem contains two answer choices for the final term (D and D’). We evaluated GPT-3 by presenting the problem along with each possible answer choice (A:B::C:D or A:B::C:D’), using the standard colon notation, and selected the answer choice for which GPT-3 assigned a higher log probability to the final term. The problem and GPT-3’s selected answer were then recursively appended to the prompt for the next problem. The problems were presented in a shuffled order. We compared against human behavioral data from 17 (N=57, UCLA undergraduates). Example problems from each of the four relation categories are shown below: Categorical vegetable : cabbage :: insect : ? 1. beetle 2. frog Function drive : car :: burn : ? 1. wood 2. fire Antonym love : hate :: rich : ? 1. poor 2. wealthy Synonym rob : steal :: cry : ? 1. weep 2. laugh The dataset of Sternberg and Nigro 19 contains 200 problems, including 40 problems for each of five relation types: categorical, functional, antonym, synonym, and linear order. We evaluated GPT-3 in the same way that we did for UCLA-VAT, and compared against human behavioral data from 19 (N=20, Yale undergraduates). An example problem illustrating the linear order relation type is shown below (the categorical, functional, antonym, and synonym problems were similar to those from the UCLA VAT): Linear order month : year :: inch : ? 1. foot 2. length The dataset of SAT problems from Turney et al. 20 contains 374 problems, covering a range of different relation types. Each problem contains five answer choices for both C and D terms (including the correct answer). We evaluated GPT-3 by presenting each of the five possible analogies for each problem, and selecting the choice for which the C and D terms were assigned the highest log probability. The problem, and GPT-3’s choice, were then appended to the prompt for the next problem. We compared against an estimate of the average performance level for high school students taking the SAT (see 40). The dataset of Jones et al. 21 contains 120 problems, including 40 problems for each of three relation types: categorical, causal, and compositional. Half of these problems are categorized as semantically near (A and B are similar to C and D), and half are categorized as semantically far (A and B are dissimilar to C and D). Each problem contains two answer choices. We evaluated GPT-3 in the same way that we did for UCLA-VAT, and compared against human behavioral data from 21 (N=241, Wayne State University undergraduates). Example problems for each of the three relation categories are shown below: Categorical diesel : fuel :: bed : ? 1. furniture 2. pillow Causal motion : sickness :: drought : ? 1. famine 2. rain 20Compositional steel : scissors :: apple : ? 1. cider 2. tree 4.6 Story analogies 4.6.1 Materials All story analogy materials were taken from a problem set created by Gentner et al.18 (from their Experiment 2), and included in a verbal analogy inventory.41 These materials involve 18 source stories. Each source story is accompanied by four potential target stories, forming four conditions: correct and incorrect near analogies (respectively termed ‘literal similarity’ and ‘mere appearance’ by Gentner et al.), both involving similar entities and first-order relations as the source, while differing from each other in higher-order causal relations; and correct and incorrect far analogies (respectively termed ‘true analogy’ and ‘false analogy’ by Gentner et al.), both involving similar first-order relations as the source but distinct entities, while differing from each other in causal relations. An example source story, along with target stories from each condition, is presented below: Source story: Karla, an old hawk, lived at the top of a tall oak tree. One afternoon, she saw a hunter on the ground with a bow and some crude arrows that had no feathers. The hunter took aim and shot at the hawk but missed. Karla knew the hunter wanted her feathers so she glided down to the hunter and offered to give him a few. The hunter was so grateful that he pledged never to shoot at a hawk again. He went off and shot deer instead. Near analogy – correct target story: Once there was an eagle named Zerdia who nested on a rocky cliff. One day she saw a sportsman coming with a crossbow and some bolts that had no feathers. The sportsman attacked but the bolts missed. Zerdia realized that the sportsman wanted her tailfeathers so she flew down and donated a few of her tailfeathers to the sportsman. The sportsman was pleased. He promised never to attack eagles again. Near analogy – incorrect target story: Once there was an eagle named Zerdia who donated a few of her tailfeathers to a sportsman so he would promise never to attack eagles. One day Zerdia was nesting high on a rocky cliff when she saw the sportsman coming with a crossbow. Zerdia flew down to meet the man, but he attacked and felled her with a single bolt. As she fluttered to the ground Zerdia realized that the bolt had her own tailfeathers on it. Far analogy – correct target story: Once there was a small country called Zerdia that learned to make the world’s smartest computer. One day Zerdia was attacked by its warlike neighbor, Gagrach. But the missiles were badly aimed and the attack failed. The Zerdian government realized that Gagrach wanted Zerdian computers so it offered to sell some of its computers to the country. The government of Gagrach was very pleased. It promised never to attack Zerdia again. Far analogy – incorrect target story: Once there was a small country called Zerdia that learned to make the world’s smartest computer. Zerdia sold one of itssupercomputers to its neighbor, Gagrach, so Gagrach would promise never to attack Zerdia. But one day Zerdia was overwhelmed by a surprise attack from Gagrach. As it capitulated the crippled government of Zerdia realized that the attacker’s missiles had been guided by Zerdian supercomputers. 4.6.2 Human behavioral experiment Human behavioral data was collected in an online experiment. The experiment was approved by the UCLA Insti- tutional Review Board (IRB protocol #22-000841, approved May 17, 2022), and all participants provided informed consent. All participants were UCLA undergraduates. Fifty-four participants (47 female, 18-44 years old, average age = 20.7 years old) completed the experiment. No statistical methods were used to pre-determine sample sizes. Participants received course credit for their participation. 21After receiving instructions, participants were presented with 18 trials, each involving a different source story. On each trial, participants were presented with a source story (referred to as ‘Story 1’), followed by two target stories (referred to as ‘Story A’ and ‘Story B’), and asked ‘Which of Story A and Story B is a better analogy to Story 1?’. Participants could select either story A or story B, or could indicate that they were both equally analogous. Accuracy was computed as the proportion of trials for which participants selected the correct target story. On half of the trials, the target stories were from the near analogy conditions. On the other half of the trials, the target stories were from the far analogy conditions. The order of the two target stories was randomly shuffled on all trials. 4.6.3 Evaluating GPT-3 GPT-3 was evaluated by entering stories directly into the OpenAI playground. For each source story, GPT-3 was evaluated on both the near analogy comparison, and the far analogy comparison, and was also evaluated on both possible orderings for each pair of target stories, resulting in 18 ×2×2 = 72 total comparisons. For each comparison, the stories were presented in the following format: Consider the following story: Story 1: ≪ source story text ≫ Now consider two more stories: Story A: ≪ target story A text ≫ Story B: ≪ target story B text ≫ Which of Story A and Story B is a better analogy to Story 1? Is the best answer Story A, Story B, or both are equally analogous? where ≪ source story text ≫, ≪ target story A text ≫, and ≪ target story B text ≫ were replaced by the text for the corresponding stories. In addition to answering the forced-choice question, GPT-3 sometimes spon- taneously produced explanations, but only the forced-choice response was used in our analysis. GPT-3’s context window was cleared after obtaining the results of each comparison. 4.6.4 Evaluating GPT-4 GPT-4 was evaluated by entering stories directly into the ChatGPT web interface. GPT-4 was evaluated on the same 72 problems, using the same format as was used for GPT-3. GPT-4’s context window was cleared after obtaining the results of each comparison. 4.6.5 Statistical analyses The task performed by both GPT-3 and human participants involved a three-choice discrimination (Story A is more analogous, Story B is more analogous, both are equally analogous). Statistical analyses were carried out to determine whether this discrimination was made at a level greater than expected from chance alone. To be conservative, we assumed a chance performance level of 50% accuracy. For GPT-3, a binomial test was performed (using data at the individual trial level). For human participants, a one-sample t-test was performed (using data averaged at the individual subject level). These analyses were carried out separately for the near analogy and far analogy conditions. To compare GPT-3 with human performance, a logistic regression analysis was carried out at the individual trial level. The dependent variable was a binary variable coding for whether a particular response was correct or incorrect. A single binary predictor coded for GPT-3 vs. human responses. 224.7 Analogical problem-solving Problems were entered directly into the OpenAI playground. Materials were taken from 22 and.23 All prompts and responses are shown in Supplementary Section S5. Each subsection shows the results for a single continuous session, with GPT-3’s responses presented in bold text. Responses were not truncated or curated in any way. Data Availability Data for all human behavioral experiments, along with the Digit Matrices, letter string analogy, and UCLA VAT problem sets, can be downloaded from: https://github.com/taylorwwebb/emergent analogies LLM The four-term verbal analogy problem sets from Sternberg and Nigro 19 and Jones et al., 21 and the story analogy materials from Gentner et al. 18 can be downloaded from: http://cvl.psych.ucla.edu/resources/AnalogyInventory.zip Information about the problem set of SAT four-term verbal analogies from Turney et al. 20 can be found at: https://aclweb.org/aclwiki/SAT Analogy Questions (State of the art) Code Availability Code for all simulations can be downloaded from: https://github.com/taylorwwebb/emergent analogies LLM Acknowledgements We would like to thank Bryor Snefjella and Peter Turney for helpful feedback and discussions. Preparation of this paper was supported by NSF grant IIS-1956441 and AFOSR MURI grant FA9550-22-1-0380 to H.L. Author Contributions Statement T.W., K.J.H., and H.L. conceived project and planned experiments. T.W. implemented experiments and analyzed results. T.W., K.J.H., and H.L. drafted manuscript. Competing Interests Statement The authors declare no competing interests. References 1 Keith J Holyoak. Analogy and relational reasoning. In Keith J Holyoak and Robert G Morrison, editors, Oxford handbook of thinking and reasoning , pages 234–259. Oxford University Press, 2012. 2 Miriam Bassok and Laura R Novick. Problem solving. In Keith J Holyoak and Robert G Morrison, editors, Oxford handbook of thinking and reasoning , pages 413–432. Oxford University Press, 2012. 3 Kevin N Dunbar and David Klahr. Scientific thinking and reasoning. In Keith J Holyoak and Robert G Morrison, editors, Oxford handbook of thinking and reasoning , pages 701–718. Oxford University Press, 2012. 234 Raymond B Cattell. Abilities: Their structure, growth, and action . Houghton Mifflin, 1971. 5 Richard E Snow, Patrick C Kyllonen, Brachia Marshalek, et al. The topography of ability and learning correlations. Advances in the psychology of human intelligence , 2(S 47):103, 1984. 6 Melanie Mitchell. Abstraction and analogy-making in artificial intelligence. Annals of the New York Academy of Sciences, 1505(1):79–101, 2021. 7 David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International conference on machine learning , pages 511–520. PMLR, 2018. 8 Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5317–5327, 2019. 9 Felix Hill, Adam Santoro, David G. T. Barrett, Ari S. Morcos, and Timothy P. Lillicrap. Learning to make analogies by contrasting abstract relational structure. In 7th International Conference on Learning Representations, ICLR , 2019. 10 Yuhuai Wu, Honghua Dong, Roger Grosse, and Jimmy Ba. The scattering compositional learner: Discovering objects, attributes, relationships in analogical reasoning. arXiv preprint arXiv:2007.04212 , 2020. 11 Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, and Abbas Rahimi. A neuro-vector-symbolic architecture for solving raven’s progressive matrices. Nature Machine Intelligence , 2023. 12 Shanka Subhra Mondal, Taylor W Webb, and Jonathan D Cohen. Learning to reason over visual objects. In 11th International Conference on Learning Representations, ICLR , 2023. 13 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. 14 Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fe- dorenko. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627, 2023. 15 John C Raven. Progressive matrices: A perceptual test of intelligence, individual form . London: Lewis, 1938. 16 Douglas R Hofstadter and Melanie Mitchell. The copycat project: A model of mental fluidity and analogy-making. In Keith J Holyoak and J A Barnden, editors, Advances in connectionist and neural computation theory, volume 2, page 31–112. Ablex, Norwood, NJ, 1994. 17 Hongjing Lu, Ying Nian Wu, and Keith J Holyoak. Emergence of analogy from relation learning. Proceedings of the National Academy of Sciences , 116(10):4176–4181, 2019. 18 Dedre Gentner, Mary Jo Rattermann, and Kenneth D Forbus. The roles of similarity in transfer: Separating retrievability from inferential soundness. Cognitive psychology, 25(4):524–575, 1993. 19 Robert J Sternberg and Georgia Nigro. Developmental patterns in the solution of verbal analogies. Child Devel- opment, pages 27–38, 1980. 20 Peter D Turney, Michael L Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent modules to solve multiple-choice synonym and analogy problems. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03) , pages 482–489, 2003. 21 Lara L Jones, Matthew J Kmiecik, Jessica L Irwin, and Robert G Morrison. Differential effects of semantic distance, distractor salience, and relations in verbal analogy. Psychonomic bulletin & review , 29(4):1480–1491, 2022. 22 Mary L Gick and Keith J Holyoak. Analogical problem solving. Cognitive psychology, 12(3):306–355, 1980. 2423 Keith J Holyoak, Ellen N Junn, and Dorrit O Billman. Development of analogical problem-solving skill. Child development, pages 2042–2055, 1984. 24 Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022. 25 Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri` a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 , 34:1877—-1901, 2022. 26 Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. 27 Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh, Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In Advances in Neural Information Processing Systems , 2022. 28 Marcel Binz and Eric Schulz. Using cognitive psychology to understand gpt-3.Proceedings of the National Academy of Sciences, 120(6):e2218523120, 2023. 29 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 31:5998–6008, 2017. 30 Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. 31 Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 36:4299–4307, 2022. 32 Laura E Matzen, Zachary O Benz, Kevin R Dixon, Jamie Posey, James K Kroger, and Ann E Speed. Recreat- ing Raven’s: Software for systematically generating large numbers of Raven-like matrix problems with normed properties. Behavior research methods, 42(2):525–541, 2010. 33 Bryan J Matlen, Dedre Gentner, and Steven L Franconeri. Spatial alignment facilitates visual comparison. Journal of Experimental Psychology: Human Perception and Performance , 46(5):443, 2020. 34 James K Kroger, Keith J Holyoak, and John E Hummel. Varieties of sameness: The impact of relational complexity on perceptual comparisons. Cognitive Science, 28(3):335–358, 2004. 35 Graeme S Halford, William H Wilson, and Steven Phillips. Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology. Behavioral and brain sciences, 21(6):803– 831, 1998. 36 David J Chalmers, Robert M French, and Douglas R Hofstadter. High-level perception, representation, and anal- ogy: A critique of artificial intelligence methodology. Journal of Experimental & Theoretical Artificial Intelligence, 4(3):185–211, 1992. 37 Douglas R Hofstadter. Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought. Basic books, 1995. 38 Andrew Lovett and Kenneth Forbus. Modeling visual problem solving as analogical reasoning. Psychological review, 124(1):60, 2017. 39 Melanie Mitchell. Analogy-making as perception: A computer model . MIT Press, 1993. 2540 Peter D Turney and Michael L Littman. Corpus-based learning of analogies and semantic relations. Machine Learning, 60:251–278, 2005. 41 Nicholas Ichien, Hongjing Lu, and Keith J Holyoak. Verbal analogy problem sets: An inventory of testing materials. Behavior research methods, 52:1803–1816, 2020. 42 Peter C Wason. Reasoning about a rule. Quarterly journal of experimental psychology , 20(3):273–281, 1968. 43 Dedre Gentner. Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2):155–170, 1983. 44 OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 45 Karl Duncker. On problem-solving. Psychological monographs, 58(5):i, 1945. 46 Keith J Holyoak and Kyunghee Koh. Surface and structural similarity in analogical transfer. Memory & cognition, 15(4):332–340, 1987. 47 James L McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, and Hinrich Sch¨ utze. Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Proceedings of the National Academy of Sciences , 117(42):25966–25974, 2020. 48 Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science . MIT press, 2001. 49 Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences , 40, 2017. 50 Taylor W Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall O’Reilly, and Jonathan Cohen. Learning representations that support extrapolation. In International conference on machine learning , pages 10136–10146. PMLR, 2020. 51 Brian Falkenhainer, Kenneth D Forbus, and Dedre Gentner. The structure-mapping engine: Algorithm and examples. Artificial intelligence, 41(1):1–63, 1989. 52 Hongjing Lu, Nicholas Ichien, and Keith J Holyoak. Probabilistic analogical mapping with semantic relation networks. Psychological Review, 2022. 53 Taylor W Webb, Shuhao Fu, Trevor Bihl, Keith J Holyoak, and Hongjing Lu. Zero-shot visual reasoning through probabilistic analogical mapping. arXiv preprint arXiv:2209.15087 , 2022. 54 Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159–216, 1990. 55 Keith J Holyoak and John E Hummel. The proper treatment of symbols in a connectionist architecture. Cognitive dynamics: Conceptual change in humans and machines , 229:263, 2000. 56 Trenton Kriete, David C Noelle, Jonathan D Cohen, and Randall C O’Reilly. Indirection and symbol-like processing in the prefrontal cortex and basal ganglia. Proceedings of the National Academy of Sciences, 110(41):16390–16395, 2013. 57 Taylor W Webb, Ishan Sinha, and Jonathan D. Cohen. Emergent symbols through binding in external memory. In 9th International Conference on Learning Representations, ICLR , 2021. 58 Klaus Greff, Sjoerd Van Steenkiste, and J¨ urgen Schmidhuber. On the binding problem in artificial neural networks. arXiv preprint arXiv:2012.05208 , 2020. 59 Thomas L Griffiths. Understanding human intelligence through human limitations. Trends in Cognitive Sciences, 24(11):873–883, 2020. 60 Allen Newell, John Calman Shaw, and Herbert A Simon. Elements of a theory of human problem solving. Psychological review, 65(3):151, 1958. 2661 Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test. Psychological review, 97(3):404, 1990. 62 Derek C Penn, Keith J Holyoak, and Daniel J Povinelli. Darwin’s mistake: Explaining the discontinuity between human and nonhuman minds. Behavioral and brain sciences , 31(2):109–130, 2008. 63 Charles R. Harris, K. Jarrod Millman, St´ efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Courna- peau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern´ andez del R´ ıo, Mark Wiebe, Pearu Peterson, Pierre G´ erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. 64 Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´ efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆ onio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020. 65 Skipper Seabold and Josef Perktold. statsmodels: Econometric and statistical modeling with python. In 9th Python in Science Conference , 2010. 66 J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering , 9(3):90–95, 2007. 67 The pandas development team. pandas-dev/pandas: Pandas, February 2020. 68 R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2021. 69 Joshua R De Leeuw. jspsych: A javascript library for creating behavioral experiments in a web browser. Behavior research methods, 47(1):1–12, 2015. 70 Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022. 27Supplementary Results Supplementary Figure 1: Matrix reasoning results for all GPT-3 variants. Zero-shot results on Digit Matrices for four GPT-3 model variants: davinci, code-davinci-002, text-davinci-002, and text-davinci-003. Results reflect generative accuracy for major problem types, including transformation problems with between one and three rules, and logic problems. Human results reflect average performance for N=40 participants. Black error bars represent standard error of the mean across participants. Summary of human results is plotted here for comparison with GPT-3, individual participant data are shown in Main Text Figure 3a. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. Supplementary Figure 2: Letter string analogy results for all GPT-3 variants. Letter string analogy results for four GPT-3 model variants: davinci, code-davinci-002, text-davinci-002, and text-davinci-003. Results reflect generative accuracy as a function of the number of generalizations between source and target. Human results reflect average performance for N=57 participants. Black error bars represent standard error of the mean across participants. Summary of human results is plotted here for comparison with GPT-3, individual participant data are shown in Main Text Figure 6a. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. S1Supplementary Figure 3: Four-term verbal analogy results for all GPT-3 variants. Results on UCLA Verbal Analogy Test (VAT) for four GPT-3 model variants: davinci, code-davinci-002, text-davinci-002, and text-davinci-003. Results reflect multiple-choice accuracy for problems involving different relation categories. Gray horizontal line represents chance performance. Human results reflect average performance for N=57 participants. Black error bars represent standard error of the mean across participants. Summary of human results is plotted here for comparison with GPT-3, individual participant data are shown in Main Text Figure 7a. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. S1 Solutions to example matrix reasoning problems The solution to the example visual matrix reasoning problem in Main Text Figure 2a is option 5. The problem is defined by a constant rule (applied to the number of shapes in each cell), and two distribution-of-3 rules (one applied to color, and one applied to shape). The solution to the example Digit Matrix problem in Main Text Figure 2b is option 7. This problem is also defined by a constant rule (applied to the digits in the center of each cell), and two distribution-of-3 rules (one applied to the digits on the left side of each cell, and one applied to the digits on the right side). S2 GPT-3 model variants Since the initial release of GPT-3, 13 OpenAI has released a number of updates to the original base model. The largest version (175B parameters) of the base model, davinci, was trained exclusively on next-token prediction using a web-based corpus of text data. Code-davinci-002 was further trained on next-token prediction using a dataset of publicly available code from GitHub. 30 Text-davinci-002 and text-davinci-003 were both fine-tuned to respond appropriately to prompts. 31 Text-davinci-002 was initialized with code-davinci-002, and then fine-tuned using supervised learning based on a set of example prompts and responses. Text-davinci-003 was initialized with text-davinci-002, and then further fine-tuned using reinforcement learning from human feedback (RLHF). In RLHF, a reward model (a separate neural network) is first trained to predict human ratings for pairs of human-generated prompts and language-model responses, and this reward model is then used to fine-tune the primary language model through reinforcement learning. More details on the different model variants and training objectives can be found at https://platform.openai.com/docs/model-index-for-researchers. We evaluated all four of these variants on Digit Matrices (Figure 1), letter string analogies (Figure 2), and four- term verbal analogies (Figure 3). Text-davinci-003 displayed the best overall performance, but other model variants performed well on a subset of tasks. For instance, code-davinci-002 performed well on the Digit Matrices and letter string problems. These task domains both involve simple alphanumeric characters and highly regular relational structure, similar to computer code. It therefore seems likely that code-davinci-002’s strong performance on these S2a  b Supplementary Figure 4: GPT-3 shows human-like contextual effects. In a separate experiment, we presented both GPT-3 and human participants (N=47, UCLA undergraduates) with Digit Matrix problems in order of increasing complexity (easy-to-hard: one-rule problems, followed by two-rule problems, and so on). (a) Both GPT-3 and human participants were able to generalize the structure inferred from few-rule problems to more complex many-rule problems, resulting in very little decrease in performance even for five-rule problems (compare with the decrease in performance from one- to three-rule problems seen in Main Text Figure 3). (b) One-rule problems were also presented in order of increasing complexity, beginning with constant problems, followed by distribution-of-3 problems, followed by progression problems. Interestingly, this actually impaired performance on progression problems relative to zero-shot (or shuffled) presentation. This was likely due to a tendency to mistake the progression rule for the distribution-of-3 rule in the previously presented problems (which only differ in terms of a single digit). Both GPT-3 and human participants showed this effect. Human results reflect average performance for N=47 participants. Black error bars represent standard error of the mean across participants. Each dot represents accuracy for a single participant. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. tasks was a consequence of having been trained on code. By contrast, code-davinci-002 performed very poorly on four-term verbal analogies (near chance performance), whereas the original davinci model performed relatively well on these problems. This suggests that code-davinci-002’s ability to model synthetic code-like structures may have come at the cost of the ability to process more real-world relational concepts. Text-davinci-002, and especially text- davinci-003, seem to have combined both of these abilities, perhaps as a result of prompt training, though these models may have also received additional fine-tuning on the original language modeling task. 31 Finally, it seems likely that prompt training improved text-davinci-002 and text-davinci-003’s ability to perform tasks without the need for few-shot task demonstrations, therefore making it easier to evaluate these models’ latent capabilities in a zero-shot setting. We also performed an initial investigation of GPT-4 on the story analogy problems from Gentner et al.18 (Supple- mentary Figure 6). GPT-4 showed significant improvement on this task relative to GPT-3, more reliably identifying the target story that shared higher-order relations with the source, and providing more precise explanations (Sec- tion S4.3). We were not able to test GPT-4 on the other analogy tasks due to a lack of API access. Very little is known about the details of GPT-4, but it is likely that this improvement stems at least in part from increased scale of both model and training set. 44 S3 Presence of test materials in GPT-3’s training data Given the massive and uncurated nature of GPT-3’s training data, it is important to consider the likelihood that our test materials were included in this training data, and the possibility therefore that GPT-3 may have memorized some of these materials (thus undermining their use as a test of zero-shot reasoning). The Digit Matrices dataset was created specifically for the purposes of our study and therefore certainly was S3a  b Supplementary Figure 5: Letter string analogy results with alternative formatting. (a) GPT-3 performed worse when letter string problems were presented without prompt, in comparison with results (shown in Main Text Figure 6a) when same problems were formatted in the standard way (using brackets to demarcate the analogs, and with the prompt “Let’s try to complete the pattern”). (b) GPT-3 also performed worse when problems were presented in the form of a sentence (e.g., ‘If a b c d changes to a b c e, then i j k l should change to’). Human results reflect average performance for N=57 participants (UCLA undergraduates). Black error bars represent standard error of the mean across participants. Each dot represents accuracy for a single participant. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. not included in GPT-3’s training data. Furthermore, this problem format itself is, to the best of our knowledge, completely novel, and it is thus extremely unlikely that GPT-3 has been trained on similar problems. The letter string problem set that we used was also created specifically for the purposes of our study, and so could not have been included in GPT-3’s training data. It is possible that GPT-3 has been trained on other letter string anal- ogy problems, as these problems are discussed on a number of webpages (e.g., https://cogsci.indiana.edu/lap.html). However, we were not able to obtain any evidence that GPT-3 knows about these problems, e.g., by asking it to describe or give examples of such problems. The UCLA Verbal Analogy Test,17 and the four-term verbal analogy sets from Sternberg and Nigro 19 and Jones et al.21 are all available in a downloadable supplement in a recently published paper, 41 though to our knowledge they are not published directly in the form of webpages, and are thus unlikely to be included in web crawl data such as the Common Crawl dataset on which GPT-3 was trained. The dataset of SAT problems from Turney et al. 20 is not publicly available, but has been distributed to a relatively small number of researchers. It is possible that any of these datasets were deliberately included in GPT-3’s training data (i.e., as a supplement to the web crawl data). However, we were not able to find any evidence that GPT-3 had memorized any of these problems, e.g., by prompting GPT-3 to complete these problems based only on the source analog (the A and B terms alone). The story analogy materials from Gentner et al.18 were also included in the downloadable supplement associated with,41 though to our knowledge have not been published directly to any webpages. We were also not able to obtain any evidence that GPT-3 had memorized any of these stories, e.g., by asking it to complete partial stories. The analogy between the general story and the radiation problem is discussed in the original publication from Gick and Holyoak,22 and is discussed on other webpages as well (e.g., http://cognitivepsychology.wikidot.com/problem- solving:analogy). We found evidence that GPT-3 is familiar with this study (it was able to identify the year of publication, and journal in which it was published), but we did not find any evidence that GPT-3 had memorized the details of the analogy itself. GPT-3 was also able to solve the radiation problem via an analogy with lesser known source stories.46 It seems unlikely that GPT-3’s success on these problems is due to prior exposure, but these S4Supplementary Figure 6: Story analogy results including test of GPT-4. Results for identification of analogies between stories, including both GPT-3 and GPT-4. Whereas GPT-3 was only reliably able to perform the task in the near analogy condition, GPT-4 displayed much more robust performance. GPT-4 showed nearly perfect performance in the near analogy condition, and displayed some sensitivity to higher-order relations in the far analogy condition (two-sided binomial test: p = 0.0039; null hypothesis is chance-level performance of 0.5). Human results reflect average performance for N=54 participants (UCLA undergraduates). Black error bars represent standard error of the mean across participants. Each dot represents accuracy for a single participant. Gray error bars represent 95% binomial confidence intervals for average performance across multiple problems. Gray horizontal line represents chance performance. results should be interpreted with caution. The other materials used to evaluate analogical problem solving are also discussed in the associated publication, 23 but we did not find any evidence that GPT-3 is familiar with this study, or that it had memorized these materials. S4 Story analogy results The following section contains examples of GPT-3’s responses to the story analogy problems. GPT-3’s responses are presented in bold text. S4.1 Near analogy The following contains an example from the near analogy condition. GPT-3 identified the correct answer (Story A). Consider the following story: Story 1: Karla, an old hawk, lived at the top of a tall oak tree. One afternoon, she saw a hunter on the ground with a bow and some crude arrows that had no feathers. The hunter took aim and shot at the hawk but missed. Karla knew the hunter wanted her feathers so she glided down to the hunter and offered to give him a few. The hunter was so grateful that he pledged never to shoot at a hawk again. He went off and shot deer instead. Now consider two more stories: Story A: Once there was an eagle named Zerdia who nested on a rocky cliff. One day she saw a sportsman coming with a crossbow and some bolts that had no feathers. The sportsman attacked but the bolts S5missed. Zerdia realized that the sportsman wanted her tailfeathers so she flew down and donated a few of her tailfeathers to the sportsman. The sportsman was pleased. He promised never to attack eagles again. Story B: Once there was an eagle named Zerdia who donated a few of her tailfeathers to a sportsman so he would promise never to attack eagles. One day Zerdia was nesting high on a rocky cliff when she saw the sportsman coming with a crossbow. Zerdia flew down to meet the man, but he attacked and felled her with a single bolt. As she fluttered to the ground Zerdia realized that the bolt had her own tailfeathers on it. Which of Story A and Story B is a better analogy to Story 1? Is the best answer Story A, Story B, or both are equally analogous? Story A is the better analogy to Story 1. Story B is not as analogous because it has a different ending. In Story 1, the hunter is grateful for the hawk’s feathers and promises not to shoot at hawks again. In Story B, the hunter attacks the eagle and kills her with her own feathers. S4.2 Far analogy The following contains an example from the far analogy condition. GPT-3 identified the correct answer (Story A). Consider the following story: Story 1: Karla, an old hawk, lived at the top of a tall oak tree. One afternoon, she saw a hunter on the ground with a bow and some crude arrows that had no feathers. The hunter took aim and shot at the hawk but missed. Karla knew the hunter wanted her feathers so she glided down to the hunter and offered to give him a few. The hunter was so grateful that he pledged never to shoot at a hawk again. He went off and shot deer instead. Now consider two more stories: Story A: Once there was a small country called Zerdia that learned to make the world’s smartest computer. One day Zerdia was attacked by its warlike neighbor, Gagrach. But the missiles were badly aimed and the attack failed. The Zerdian government realized that Gagrach wanted Zerdian computers so it offered to sell some of its computers to the country. The government of Gagrach was very pleased. It promised never to attack Zerdia again. Story B: Once there was a small country called Zerdia that learned to make the world’s smartest com- puter. Zerdia sold one of itssupercomputers to its neighbor, Gagrach, so Gagrach would promise never to attack Zerdia. But one day Zerdia was overwhelmed by a surprise attack from Gagrach. As it capitu- lated the crippled government of Zerdia realized that the attacker’s missiles had been guided by Zerdian supercomputers. Which of Story A and Story B is a better analogy to Story 1? Is the best answer Story A, Story B, or both are equally analogous? The best answer is Story A. Story A is a better analogy to Story 1 because it follows a similar pattern of a protagonist offering something of value to an antagonist in exchange for a promise of peace. In Story 1, Karla offers her feathers to the hunter in exchange for a promise not to shoot at hawks again. In Story A, Zerdia offers its computers to Gagrach in exchange for a promise not to attack Zerdia again. S6S4.3 GPT-4 The following contains an example response from GPT-4 in the far analogy condition. GPT-4 both identifies the correct answer (Story A), and offers a precise explanation of the analogy. Consider the following story: Story 1: William was a patient in a psychiatric hospital who was confined indoors almost all the time. He could never pass the monthly room inspections so he hated them. He spent most of his time daydreaming about food. A few days before the April inspection William’s room was still a mess since he had done nothing but daydream. To provide William with an incentive, the nurse promised him some gingerbread from the cookie shop if he scrubbed his room and put it in order once and for all. William was overjoyed. But there was no longer enough time for him to put it in order. As a result, he did not pass the inspection and did not get any gingerbread. William sulked all day and slammed his door so hard the plaster cracked, but he still didn’t get any gingerbread. Now consider two more stories: Story A: Karen always did poorly in high school so she despised it. But she loved vacations. She spent most of her time dreaming about going to Hawaii. Not long before the end of her fourth year Karen was not doing at all well in her classes because she had spent all her time daydreaming. To motivate her, Karen’s father promised her that if she did well enough during the next few weeks to graduate from high school he would pay for her trip to Hawaii. This made Karen extremely happy. But she was too far behind in her classes. Consequently she failed too many and did not go to Hawaii. Story B: Karen disliked high school so she always had trouble getting passing grades in her classes. Vacations were her favorite part of the school year. Not long before the end of her fourth year of school, Karen was not doing well at all because she despised school. To motivate her, Karen’s father promised her a trip to Hawaii if she would just pass enough of her classes to graduate. This made Karen extremely happy. During the last few weeks of school she spent most of her time dreaming about Hawaii and preparing for her trip there. Consequently, she failed to graduate and she did not get to go. Which of Story A and Story B is a better analogy to Story 1? Is the best answer Story A, Story B, or both are equally analogous? The best answer is Story A. Both Story A and Story 1 share the following similarities: 1. The main characters (William and Karen) are not successful in their respective situ- ations (room inspection and graduating high school) due to spending too much time daydreaming. 2. An incentive is provided to motivate the main characters (gingerbread for William and a trip to Hawaii for Karen). 3. Despite being motivated by the incentive, both characters fail to achieve the desired outcome because they are too far behind in their tasks (cleaning the room and catching up on classes). Story B is less analogous to Story 1 because Karen spends her time daydreaming and preparing for the trip instead of focusing on her classes, which leads to her failure to graduate. The cause of her failure is different from that in Story 1, where William fails due to the lack of time to complete the task. S5 Analogical problem-solving results The following sections contain all of the materials used to evaluate analogical reasoning in natural language problem- solving. GPT-3’s responses are presented in bold text. S7S5.1 Solution to radiation problem in isolation The following contains GPT-3’s proposed solution to the radiation problem when presented in isolation. Target problem: Suppose you were a doctor faced with a patient suffering from a malignant stomach tumor. The tumor is inoperable, and the patient will die unless it is destroyed. There is a kind of ray that will destroy the tumor if the rays reach it at a high intensity. But unfortunately, such high-intensity rays will also destroy the healthy tissue they pass through on the way to the tumor. At lower intensities the rays will not damage the healthy tissue, but neither will they remove the tumor. How can the doctor use rays to destroy the tumor, while at the same time sparing the healthy tissue? Solution: The doctor can use a technique called brachytherapy, which involves placing a radioactive source directly into or near the tumor. This allows the doctor to deliver a high dose of radiation directly to the tumor, while sparing the healthy tissue around it. The radiation source can be placed in the tumor using a catheter, or it can be placed in a sealed container and placed near the tumor. This technique is often used to treat tumors that are inoperable or too close to vital organs to be safely removed. S5.2 Solution to radiation problem following general story The following contains GPT-3’s proposed solution to the radiation problem when first presented with the general story, along with responses to followup questions that explicitly probe the analogy between the source story and the target problem. Source story: An evil dictator controlled a fortress situated in the center of a small country. Many roads radiated out from the fortress like spokes on a wheel. A general raised an army at the border, vowing to capture the fortress and overthrow the dictator. The general was about to send his entire army down one road to capture the fortress, when he learned that the dictator had mined each road so that although small groups could still pass, a large army would set off an explosion. The general then had a clever idea: he divided his army into small groups and dispatched each group to the head of a different road. Upon his signal, each group charged down a different road. All the groups passed safely to the fortress, where the entire army attacked the fortress in full strength. In this way the general captured the fortress and overthrew the dictator. Target problem: Suppose you were a doctor faced with a patient suffering from a malignant stomach tumor. The tumor is inoperable, and the patient will die unless it is destroyed. There is a kind of ray that will destroy the tumor if the rays reach it at a high intensity. But unfortunately, such high-intensity rays will also destroy the healthy tissue they pass through on the way to the tumor. At lower intensities the rays will not damage the healthy tissue, but neither will they remove the tumor. How can the doctor use rays to destroy the tumor, while at the same time sparing the healthy tissue? Solution: The doctor could divide the rays into small groups and send each group down a different path to the tumor. By using a lower intensity for each group, the rays will not damage the healthy tissue, but when they all reach the tumor at the same time, the combined intensity S8will be enough to destroy it. The doctor could also use imaging technology to map out the paths of the rays and ensure that they all reach the tumor at the same time. Can you identify an analogy between the source story and the target problem? The analogy between the source story and the target problem is that both involve a situation in which a large force needs to be divided into smaller groups in order to achieve a desired outcome. In the source story, the general divides his army into small groups in order to avoid setting off the mines and capture the fortress. In the target problem, the doctor needs to divide the rays into smaller groups in order to destroy the tumor without damaging the healthy tissue. Who or what in the target problem is most analogous to the general in the source story? The doctor in the target problem is most analogous to the general in the source story. Both are faced with a difficult situation and must come up with a clever solution in order to achieve their desired outcome. In the source story, the general divides his army into small groups in order to avoid setting off the mines and capture the fortress. In the target problem, the doctor needs to divide the rays into smaller groups in order to destroy the tumor without damaging the healthy tissue. Who or what in the target problem is most analogous to the evil dictator in the source story? The malignant stomach tumor in the target problem is most analogous to the evil dictator in the source story. Both are obstacles that must be overcome in order to achieve the desired outcome. In the source story, the evil dictator must be overthrown in order for the general to capture the fortress. In the target problem, the malignant stomach tumor must be destroyed in order for the patient to be saved. Who or what in the target problem is most analogous to the army in the source story? The rays in the target problem are most analogous to the army in the source story. Both are forces that must be divided into smaller groups in order to achieve the desired outcome. In the source story, the army is divided into small groups in order to avoid setting off the mines and capture the fortress. In the target problem, the rays must be divided into smaller groups in order to destroy the tumor without damaging the healthy tissue. S5.3 Solution to radiation problem following alternative source analogs We also tested GPT-3 using alternative materials for the source analog taken from a different study. 46 These stories each involve a solution based on convergence, but differ from the general story used in the original study of Gick and Holyoak.22 GPT-3 was also able to identify the convergence solution to the radiation problem when presented with these alternative source analogs. The following contains GPT-3’s response to the radiation problem when first presented with an analogous story about a student in a physics lab who uses lasers to repair a lightbulb. Source story: In a physics lab at a major university, a very expensive lightbulb which would emit precisely controlled quantities of light was being used in some experiments. Ruth was the research assistant responsible for operating the sensitive lightbulb. One morning she came into the lab and found to her dismay that the lightbulb no longer worked. She realized that she had forgotten to turn it off the previous night. As a result the lightbulb overheated, and the filament inside the bulb had broken into two parts. The surrounding glass bulb was completely sealed, so there was no way to open it. Ruth knew that the lightbulb could be repaired if a brief, high-intensity laser beam could be used to fuse the two parts of S9the filament into one. Furthermore, the lab had the necessary equipment to do the job. However, a high-intensity laser beam would also break the fragile glass surrounding the filament. At lower intensities the laser would not break the glass, but neither would it fuse the filament. So it seemed that the lightbulb could not be repaired, and a costly replacement would be required. Ruth was about to give up when she had an idea. She placed several lasers in a circle around the lightbulb, and administered low-intensity laser beams from several directions all at once. The beams all converged on the filament, where their combined effect was enough to fuse it. Since each spot on the surrounding glass received only a low- intensity beam from one laser, the glass was left intact. Ruth was greatly relieved that the lightbulb was repaired, and she then went on to successfully complete the experiment. Target problem: Suppose you were a doctor faced with a patient suffering from a malignant stomach tumor. The tumor is inoperable, and the patient will die unless it is destroyed. There is a kind of ray that will destroy the tumor if the rays reach it at a high intensity. But unfortunately, such high-intensity rays will also destroy the healthy tissue they pass through on the way to the tumor. At lower intensities the rays will not damage the healthy tissue, but neither will they remove the tumor. How can the doctor use rays to destroy the tumor, while at the same time sparing the healthy tissue? Solution: The doctor could use multiple low-intensity rays from several directions to converge on the tumor. The combined effect of the rays would be enough to destroy the tumor, while each spot on the healthy tissue would only receive a low-intensity beam from one ray, thus leaving it intact. The following contains GPT-3’s response to the radiation problem when first presented with a different story involving ultrasound instead of lasers. GPT-3 was still able to use this story to identify the convergence solution, despite the fact that it was designed to have lower surface similarity to the radiation problem. Source story: In a physics lab at a major university, a very expensive lightbulb which would emit precisely controlled quantities of light was being used in some experiments. Ruth was the research assistant responsible for operating the sensitive lightbulb. One morning she came into the lab and found to her dismay that the lightbulb no longer worked. She realized that she had forgotten to turn it off the previous night. As a result the lightbulb overheated, and the two wires in the filament inside the bulb fused together. The surrounding glass bulb was completely sealed, so there was no way to open it. Ruth knew that the lightbulb could be repaired if a brief, high-intensity ultrasound wave could be used to jar apart the fused parts. Furthermore, the lab had the necessary equipment to do the job. However, a high-intensity ultrasound wave would also break the fragile glass surrounding the filament. At lower intensities the ultrasound wave would not break the glass, but neither would it jar apart the fused parts. So it seemed that the lightbulb could not be repaired, and a costly replacement would be required. Ruth was about to give up when she had an idea. She placed several ultrasound machines in a circle around the lightbulb, and administered low-intensity ultrasound waves from several directions all at once. The waves all converged on the filament, where their combined effect was enough to jar apart the fused parts. Since each spot on the surrounding glass received only a low-intensity wave from one ultrasound machine, the glass was left intact. Ruth was greatly relieved that the lightbulb was repaired, and she then went on to successfully complete the experiment. Target problem: Suppose you were a doctor faced with a patient suffering from a malignant stomach tumor. The tumor is inoperable, and the patient will die unless it is destroyed. There is a kind of ray that will destroy S10the tumor if the rays reach it at a high intensity. But unfortunately, such high-intensity rays will also destroy the healthy tissue they pass through on the way to the tumor. At lower intensities the rays will not damage the healthy tissue, but neither will they remove the tumor. How can the doctor use rays to destroy the tumor, while at the same time sparing the healthy tissue? Solution: The doctor could use multiple low-intensity rays from several directions all at once. The rays would converge on the tumor, where their combined effect would be enough to destroy it. Since each spot on the healthy tissue would receive only a low-intensity ray from one source, the healthy tissue would be left intact. S5.4 Solution to radiation problem with distracting stories The following contains GPT-3’s proposed solution to the radiation problem when first presented with both the general story and two distracting (i.e., non-analogous) stories. GPT-3 did not identify the convergence solution in this case, instead proposing the same solution as it did when presented with the radiation problem in isolation. Story #1 – The Wine Merchants: One day a rich man found that his wine cellar was empty. So he sent out messengers to announce a generous offer. The first person to bring the rich man a barrel of wine would be given a brick of solid gold. However, the offer would expire at sundown. Two wine merchants heard the news. Each had a horse-drawn cart loaded with large barrels of wine. They both set out for the duke’s palace at once. An hour before sundown they came to a place where the bridge had been washed out by a raging river. The first merchant drove his horses and cart into the flood in a desperate attempt to reach the other side. But the horses were already exhausted and could not fight the current. The cart overturned, and the horses, wine, and driver were washed away. The second merchant tried a different tactic. He poured the wine out of all but one of his barrels, and lashed them together to form a raft; then he loaded the one full barrel, a horse, and himself on top. He set the raft adrift and floated downstream. In a few minutes the raft came to rest on the shore in front of the town where the rich man lived. The merchant disembarked, loaded the wine barrel on the horse, and led it to the rich man’s house. He arrived just as the sun was setting, and collected the gold brick as a reward for his efforts. Story #2 – The General: An evil dictator controlled a fortress situated in the center of a small country. Many roads radiated out from the fortress like spokes on a wheel. A general raised an army at the border, vowing to capture the fortress and overthrow the dictator. The general was about to send his entire army down one road to capture the fortress, when he learned that the dictator had mined each road so that although small groups could still pass, a large army would set off an explosion. The general then had a clever idea: he divided his army into small groups and dispatched each group to the head of a different road. Upon his signal, each group charged down a different road. All the groups passed safely to the fortress, where the entire army attacked the fortress in full strength. In this way the general captured the fortress and overthrew the dictator. Story #3 – The Identical Twins: Once there were identical twins who were continually playing pranks on their family, friends, and teachers. The annual school picnic was always a big event for the twins. There were races and other athletic events in which the twins won lots of prizes. One year a new student arrived who was a star runner. The twins wanted to win the main event: the 2-mile race through the woods behind the school. So they secretly devised a plan which would enable them to outdo the newcomer. The day of the race arrived. Each runner was to pick his own path through the woods to a clearing, where a teacher stood posted to S11determine the winner. One twin entered the race, while the other excused himself on the grounds that he had hurt his leg in an earlier broadjumping event. The race began and the students rushed into the woods. The twin rushed into the woods and waited until the others had passed out of sight. Then he went back to the school using a path hidden from the picnic area. Shortly after, the other twin, who had been hiding behind a rock near the finish line of the race, burst out and ran into the clearing ahead of the other runners. The teacher named him the winner and marveled at the speed of his running. Next year the twins switched places and thereafter maintained their status on this event. Target problem: Suppose you were a doctor faced with a patient suffering from a malignant stomach tumor. The tumor is inoperable, and the patient will die unless it is destroyed. There is a kind of ray that will destroy the tumor if the rays reach it at a high intensity. But unfortunately, such high-intensity rays will also destroy the healthy tissue they pass through on the way to the tumor. At lower intensities the rays will not damage the healthy tissue, but neither will they remove the tumor. How can the doctor use rays to destroy the tumor, while at the same time sparing the healthy tissue? Solution: The doctor can use a technique called brachytherapy. This involves placing a radioactive source directly into the tumor. The radiation from the source will be of a high enough intensity to destroy the tumor, but the radiation will not travel far enough to damage the healthy tissue. The doctor can also use a technique called stereotactic radiotherapy. This involves using a machine to deliver a high-intensity beam of radiation to the tumor from outside the body. The beam is focused on the tumor, so that the healthy tissue is spared. S5.5 Solution to radiation problem with distracting stories and prompt When presented with distracting stories, GPT-3 did not propose a solution to the radiation problem based on the analogy with the general story. We therefore provided a prompt to encourage GPT-3 to consider the previously presented stories when generating its solution. This same prompt was used in the original human behavioral study,22 where it was found to increase the likelihood that participants would identify the analogous convergence solution. The source stories and target problem were presented in the exact same manner, but the prompt used to elicit GPT- 3’s solution was changed. When presented with this prompt, GPT-3 correctly identified the convergence solution, and specifically identified the general story as the relevant source. Solution (in solving this problem you may find that one of the stories you read before will give you a hint for a solution of this problem): The doctor could use the same tactic as the general in the second story. He could divide the rays into small groups and send each group down a different path to the tumor. The rays would be of low intensity, so they would not damage the healthy tissue, but when they all reached the tumor at the same time, the combined intensity would be enough to destroy it. S5.6 Solution to gumball problem in isolation The following contains GPT-3’s proposed solution to the gumball problem (from 23) when presented in isolation (enumeration is part of the response provided by GPT-3). Target problem: You are seated in front of a table. Two bowls are on the table, one within your reach and one farther away. The closer bowl contains a number of small gumballs, and the farther one is empty. Also on the table are an aluminum walking cane, a large rectangular sheet of heavy paper (posterboard), a hollow S12cardboard tube long enough to reach the farther bowl, scissors, string, tape, paper clips, and rubber bands. Using the materials provided, how can you transfer the balls from the filled to the empty bowl without leaving your seat? Solution: 1. Cut the posterboard into a long strip and tape it to the end of the cardboard tube. 2. Tie one end of the string to the end of the tube and the other end to the handle of the cane. 3. Place the tube in the bowl of gumballs and use the cane to pull the tube and gumballs towards you. 4. Once the tube is close enough, use the paper clips to secure the tube to the edge of the table. 5. Use the rubber bands to secure the tube to the cane. 6. Pull the cane towards you, using the string, and the gumballs will be transferred to the empty bowl. S5.7 Solution to gumball problem following magic staff story The following contains GPT-3’s proposed solution to the gumball problem when first presented with the ‘magic staff story’. This story involves a magical genie who has moved from his old home (a bottle) to a new home (a different bottle), and wants to transport his collection of jewels between the bottles. To do so, he uses a magic staff to pull the new bottle over next to the old bottle and transfers the jewels by hand. When first presented with this story, children most often identify a solution to the gumball problem that involves using the aluminum walking cane to pull the empty bowl over next to the bowl with the gumballs and then transferring the gumballs by hand. GPT-3 was not able to discover this solution, but was able to identify the high-level analogy between the source story and the target problem (though not the specific analogy between the magic staff and the walking cane). Source story: Once upon a time there lived a magical genie. He was a very old, wise, and rich genie indeed. One day while he was polishing his home, which was actually a bottle, he decided he would like to find an even bigger and better home to live in. So he began searching far and wide for another bottle. Finally he found the perfect home. It was larger, prettier, and not too far away from his old bottle. The genie was very excited and began moving his belongings right away. But now the genie had a problem. He had a great many beautiful and very precious jewels in his old home. He had to somehow get all the jewels from his old bottle to the new bottle without dropping or losing a single jewel. After thinking a bit, the genie came up with a wonderful idea. He began searching for his magic staff, or wand. He then commanded his staff to stretch itself from his old home to his new home. Next, the genie tugged and pulled on his magical staff until at last he pulled the new bottle right up next to his old bottle. At once, the genie began gathering his jewels together in his old home and simply dropped them carefully into his new home right next to him. When all his jewels were safely tucked away in his new home, the genie settled in happily. He invited his friend to come in and admire his new home. I’m sure you can still find the genie sitting in his new bigger and better bottle with all his jewels and smiling contentedly even today! Target problem: You are seated in front of a table. Two bowls are on the table, one within your reach and one farther away. The closer bowl contains a number of small gumballs, and the farther one is empty. Also on the S13table are an aluminum walking cane, a large rectangular sheet of heavy paper (posterboard), a hollow cardboard tube long enough to reach the farther bowl, scissors, string, tape, paper clips, and rubber bands. Using the materials provided, how can you transfer the balls from the filled to the empty bowl without leaving your seat? Solution: Take the cardboard tube and cut it in half. Place one half of the tube in the bowl with the gumballs and the other half in the empty bowl. Secure the two halves together with the rubber bands. Take the posterboard and cut it into strips. Tape the strips together to form a long rope. Tie one end of the rope to the handle of the walking cane and the other end to the rubber bands. Place the walking cane on the table and hold it in place. Gently pull the cane towards you, and the gumballs will travel through the tube and into the empty bowl. When all the gumballs have been transferred, remove the tube and rope and enjoy your gumballs! Can you identify an analogy between the source story and the target problem? The source story and the target problem both involve transferring something from one place to another. In the source story, the genie is transferring his jewels from one bottle to another, and in the target problem, the gumballs are being transferred from one bowl to another. In both cases, the characters must use creative problem-solving to figure out how to move the items without losing any. Does this analogy suggest a solution to the target problem? Yes, the analogy suggests a solution to the target problem. The genie used a magical staff to stretch from one bottle to the other, and the target problem suggests using a rope and a walking cane to transfer the gumballs from one bowl to the other. Who or what in the target problem is most analogous to the genie’s old home in the source story? The bowl with the gumballs is most analogous to the genie’s old home in the source story. Both contain something valuable that needs to be transferred to a new location. Who or what in the target problem is most analogous to the genie’s new home in the source story? The empty bowl is most analogous to the genie’s new home in the source story. Both are the destination for something valuable that needs to be transferred from an old location. Who or what in the target problem is most analogous to the jewels in the source story? The gumballs are most analogous to the jewels in the source story. Both are valuable items that need to be transferred from one place to another. Who or what in the target problem is most analogous to the magic staff in the source story? The rope and walking cane are most analogous to the magic staff in the source story. Both are used to transfer something valuable from one place to another. S5.8 Solution to gumball problem following magic carpet story The following contains GPT-3’s proposed solution to the gumball problem when first presented with the ‘magic carpet story’. This story has the same setup as the magic staff story – a magical genie wants to transport his jewels S14from his old bottle to his new bottle – but the genie’s solution in this story is to roll up his magic carpet and use it as a bridge between the two bottles. When first presented with this story, children most often identify a solution to the gumball problem that involves rolling up the posterboard and rolling the gumballs through it into the empty bowl. It is interesting to note that children do this despite having access to an already formed cardboard tube, likely due to the analogy between the rolled up posterboard and the rolled up magic carpet. GPT-3 identified essentially this same solution, but added some unnecessary steps, including first cutting the posterboard in half. GPT-3 correctly identified the analogy between the source story and the target problem. Source story: Once upon a time there lived a magical genie. He was a very old, wise, and rich genie indeed. One day while he was polishing his home, which was actually a bottle, he decided he would like to find an even bigger and better home to live in. So he began searching far and wide for another bottle. Finally he found the perfect home. It was larger, prettier, and not too far away from his old bottle. The genie was very excited and began moving his belongings right away. But now the genie had a problem. He had a great many beautiful and very precious jewels in his old home. He had to somehow get all the jewels from his old bottle to the new bottle without dropping or losing a single jewel. After thinking a bit, the genie came up with a wonderful idea. He searched for his magic carpet. Then he commanded it to roll itself up into a long hollow tube. Next the genie commanded his flying carpet to place one end at his old home and the other end at his new home so that it formed a sort of hollow bridge between the two bottles. Then, the genie very carefully took one jewel from inside his old home and placed it into the opening of his carpet. At once, the jewel began tumbling and rolling through the carpet tube until it reached his new home and plopped safely inside. The genie grinned happily and began rolling all his jewels through the carpet into his new home. In fact, I’m sure you can still find him sitting in his new, bigger and better bottle with all his jewels and smiling contentedly even today! Target problem: You are seated in front of a table. Two bowls are on the table, one within your reach and one farther away. The closer bowl contains a number of small gumballs, and the farther one is empty. Also on the table are an aluminum walking cane, a large rectangular sheet of heavy paper (posterboard), a hollow cardboard tube long enough to reach the farther bowl, scissors, string, tape, paper clips, and rubber bands. Using the materials provided, how can you transfer the balls from the filled to the empty bowl without leaving your seat? Solution: Take the posterboard and cut it into two pieces. Take one of the pieces and roll it up into a tube. Secure the tube with the rubber bands, tape, and paper clips. Place one end of the tube into the bowl with the gumballs and the other end into the empty bowl. Then, use the walking cane to push the gumballs through the tube until they reach the empty bowl. Can you identify an analogy between the source story and the target problem? The source story and the target problem both involve transferring items from one place to another. In the source story, the genie transfers jewels from one bottle to another using a flying carpet. In the target problem, the person transfers gumballs from one bowl to another using a posterboard tube. In both cases, the items are transferred without leaving the original location. Does this analogy suggest a solution to the target problem? S15Yes, the analogy suggests a solution to the target problem. The solution is to use a poster- board tube to transfer the gumballs from one bowl to the other. This is similar to how the genie used a flying carpet to transfer the jewels from one bottle to the other. Who or what in the target problem is most analogous to the genie’s old home in the source story? The bowl with the gumballs is most analogous to the genie’s old home in the source story. Both contain items that need to be transferred to a new location. Who or what in the target problem is most analogous to the genie’s new home in the source story? The empty bowl is most analogous to the genie’s new home in the source story. Both are the destination for items that need to be transferred from an old location. Who or what in the target problem is most analogous to the jewels in the source story? The gumballs in the target problem are most analogous to the jewels in the source story. Both are items that need to be transferred from one location to another. Who or what in the target problem is most analogous to the magic staff in the source story? The posterboard tube in the target problem is most analogous to the magic carpet in the source story. Both are used to transfer items from one location to another. S16",
      "references": [
        "Analogy and relational reasoning.",
        "Problem solving.",
        "Scientific thinking and reasoning.",
        "Abilities: Their structure, growth, and action .",
        "The topography of ability and learning correlations.",
        "Abstraction and analogy-making in artificial intelligence.",
        "Measuring abstract reasoning in neural networks.",
        "Raven: A dataset for relational and analogical visual reasoning.",
        "Learning to make analogies by contrasting abstract relational structure.",
        "The scattering compositional learner: Discovering objects, attributes, relationships in analogical reasoning.",
        "A neuro-vector-symbolic architecture for solving raven’s progressive matrices.",
        "Learning to reason over visual objects.",
        "Language models are few-shot learners.",
        "Dissociating language and thought in large language models: a cognitive perspective.",
        "Progressive matrices: A perceptual test of intelligence, individual form .",
        "The copycat project: A model of mental fluidity and analogy-making.",
        "Emergence of analogy from relation learning.",
        "The roles of similarity in transfer: Separating retrievability from inferential soundness.",
        "Developmental patterns in the solution of verbal analogies.",
        "Combining independent modules to solve multiple-choice synonym and analogy problems.",
        "Differential effects of semantic distance, distractor salience, and relations in verbal analogy.",
        "Analogical problem solving.",
        "Development of analogical problem-solving skill.",
        "Language models show human-like content effects on reasoning.",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.",
        "Emergent abilities of large language models.",
        "Data distributional properties drive emergent in-context learning in transformers.",
        "Using cognitive psychology to understand gpt-3.",
        "Attention is all you need.",
        "Evaluating large language models trained on code.",
        "Training language models to follow instructions with human feedback.",
        "Recreating Raven’s: Software for systematically generating large numbers of Raven-like matrix problems with normed properties.",
        "Spatial alignment facilitates visual comparison.",
        "Varieties of sameness: The impact of relational complexity on perceptual comparisons.",
        "Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology.",
        "High-level perception, representation, and anal- ogy: A critique of artificial intelligence methodology.",
        "Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought.",
        "Modeling visual problem solving as analogical reasoning.",
        "Analogy-making as perception: A computer model .",
        "Corpus-based learning of analogies and semantic relations.",
        "Verbal analogy problem sets: An inventory of testing materials.",
        "Reasoning about a rule.",
        "Structure-mapping: A theoretical framework for analogy.",
        "Gpt-4 technical report.",
        "On problem-solving.",
        "Surface and structural similarity in analogical transfer.",
        "Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models.",
        "The algebraic mind: Integrating connectionism and cognitive science .",
        "Building machines that learn and think like people.",
        "Learning representations that support extrapolation.",
        "The structure-mapping engine: Algorithm and examples.",
        "Probabilistic analogical mapping with semantic relation networks.",
        "Zero-shot visual reasoning through probabilistic analogical mapping.",
        "Tensor product variable binding and the representation of symbolic structures in connectionist systems.",
        "The proper treatment of symbols in a connectionist architecture.",
        "Indirection and symbol-like processing in the prefrontal cortex and basal ganglia.",
        "Emergent symbols through binding in external memory.",
        "On the binding problem in artificial neural networks.",
        "Understanding human intelligence through human limitations.",
        "Elements of a theory of human problem solving.",
        "What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test.",
        "Darwin’s mistake: Explaining the discontinuity between human and nonhuman minds.",
        "Array programming with NumPy.",
        "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.",
        "statsmodels: Econometric and statistical modeling with python.",
        "Matplotlib: A 2d graphics environment.",
        "pandas-dev/pandas: Pandas, February 2020.",
        "R: A Language and Environment for Statistical Computing.",
        "jspsych: A javascript library for creating behavioral experiments in a web browser.",
        "Large language models are zero-shot reasoners."
      ],
      "meta_data": {
        "arxiv_id": "2212.09196v3",
        "authors": [
          "Taylor Webb",
          "Keith J. Holyoak",
          "Hongjing Lu"
        ],
        "published_date": "2022-12-19T00:04:56Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Directly tests whether zero-shot analogical reasoning emerges in large language models by comparing GPT-3 (text-davinci-003) to humans across multiple analogy domains. Introduces Digit Matrices, a novel text-only Raven’s SPM–like matrix reasoning benchmark guaranteed to be novel to both humans and LLMs, enabling a clean zero-shot evaluation of abstract rule induction. Finds GPT-3 matches or exceeds college-student accuracy on Digit Matrices (generative and multiple-choice) and on letter-string and several verbal analogy datasets, and shows human-like difficulty signatures (relational complexity, progression-rule difficulty, mapping alignment effects, semantic-distance effects). Demonstrates partial sensitivity to higher-order (causal) relations in story analogies, with a notable gap on far/cross-domain analogies; preliminary GPT-4 tests show improvements. Provides qualitative evidence that GPT-3 can leverage provided source analogs to solve classic transfer problems (radiation problem), while failing in tool/physical-reasoning analogical transfer (gumball task).",
        "methodology": "Evaluates a 175B-parameter transformer LLM (GPT-3 text-davinci-003) in strictly zero-shot settings (no fine-tuning; typically no in-context examples; temperature=0). Uses standardized prompting formats for generative tasks (matrix completion and letter-string completion) and a log-probability scoring method for multiple-choice tasks: concatenate each candidate answer to the prompt, compute average token log-probability for the choice, and select the argmax. Constructs Digit Matrices procedurally using transformation rules (constant, distribution-of-3, progression) and logic/set rules (OR/AND/XOR) with controlled relational complexity and alignment manipulations; generates distractors via rule-based perturbations and subset sampling. Analyzes behavior with trial-level logistic regressions (odds ratios, CIs) and correlations of subtype accuracies between GPT-3 and humans to assess similarity of error patterns; also performs qualitative case studies of analogical problem solving with and without distractors/prompts.",
        "experimental_setup": "Four main evaluation domains with matched human comparison: (1) Digit Matrices: 3x3 text matrices; GPT-3 tested on 40 instances of each of 32 subtypes (one-, two-, three-rule transformations and logic problems), both generative completion and 8-choice selection; human online study with UCLA undergrads N=40 (free response then multiple-choice); additional easy-to-hard ordering study with humans N=47 and GPT-3 repeated runs with growing context until context-window truncation. (2) Letter-string analogies: novel problem set spanning 6 transformation types (extend, successor, predecessor, remove redundant, fix sequence, sort) and up to 3 generalizations (letter→number, grouping, longer target, reversed order, interleaved distractors, larger interval), plus letter→real-world ‘successorship’ variants; GPT-3 evaluated on hundreds of procedurally generated problems under different formatting prompts; humans N=57 (28 problems each, free response). (3) Four-term verbal analogies: four datasets with existing human data—UCLA VAT (80 items; humans N=57), Sternberg & Nigro (200; humans N=20), SAT analogies (374; compared to estimated SAT-taker average), Jones et al. (120; humans N=241); GPT-3 solved via log-prob multiple-choice scoring with sequential-context appending. (4) Story analogies: Gentner et al. materials (18 sources; near vs far comparisons, causal higher-order relation manipulation); new human study N=54; GPT-3 evaluated on 72 comparisons (order counterbalancing) with cleared context; preliminary GPT-4 tested similarly. Statistics: binomial tests/t-tests for above-chance; logistic regression for model-vs-human contrasts and condition effects; subtype correlation analyses.",
        "limitations": "Digit Matrices are text/pseudo-symbolic and bypass key perceptual demands of Raven’s (object segmentation, visual attribute disentangling), so results do not establish visual analogical reasoning from pixels. LLM evaluation is sensitive to prompting/format (notably letter-string tasks) and to contextual ordering, making robustness an open issue. GPT-3 lacks episodic long-term memory and cannot retrieve source analogs autonomously; analogical transfer only works when relevant sources are placed in the context window, and it does not ‘learn’ enduringly from analogies once context is cleared. Weaknesses appear in far story analogies emphasizing cross-domain causal structure, and in tasks requiring grounded physical/tool reasoning (gumball transfer), suggesting missing world-model/embodiment. Human samples are largely WEIRD (UCLA undergrads) and task interfaces differ from real-world settings. Potential (though argued unlikely) contamination from training data exists for some established verbal/story analogy materials; comparisons across GPT-3 variants and GPT-4 are incomplete due to access constraints.",
        "future_research_directions": "Extend to multimodal models to test full Raven-like visual analogy (pixels→objects→relations) and study how perceptual grounding interacts with analogical mapping. Develop methods for autonomous analog retrieval and persistent memory/learning so models can search past experiences and refine concepts across multiple analogs (beyond in-context). Probe mechanisms: interpret transformer attention/representations as analogical mapping/variable binding; design controlled interventions/diagnostics to distinguish genuine relational reasoning from pattern heuristics. Improve and benchmark higher-order causal and far/cross-domain analogies, including compositional narrative structure and counterfactual/causal reasoning. Integrate physical reasoning benchmarks (tool use, mechanics) with analogy tasks, possibly via simulation or embodied training. Study prompt/format invariance, calibration, and adversarial robustness; create standardized evaluation protocols minimizing prompt sensitivity. Broaden human comparisons (diverse populations; matched time/effort constraints) and compare across model scales/architectures and RLHF variants to isolate what training factors drive emergent analogy.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Reasoning with Language Model Prompting: A Survey",
      "full_text": "COPD-FlowNet: Elevating Non-invasive COPD Diagnosis with CFD Simulations (Student Abstract) Aryan Tyagi*, Aryaman Rao*, Shubhanshu Rao, Raj Kumar Singh Delhi Technological University, New Delhi, India tyagiaryan82@gmail.com Abstract Chronic Obstructive Pulmonary Disorder (COPD) is a preva- lent respiratory disease that significantly impacts the quality of life of affected individuals. This paper presents COPD- FlowNet, a novel deep-learning framework that leverages a custom Generative Adversarial Network (GAN) to gener- ate synthetic Computational Fluid Dynamics (CFD) veloc- ity flow field images specific to the trachea of COPD pa- tients. These synthetic images serve as a valuable resource for data augmentation and model training. Additionally, COPD- FlowNet incorporates a custom Convolutional Neural Net- work (CNN) architecture to predict the location of the ob- struction site. Introduction Chronic Obstructive Pulmonary Disorder (COPD) is the third leading cause of death worldwide, causing over 3 mil- lion deaths in 2019 alone (Momtazmanesh et al. 2023). COPD is caused by smoking and exposure to air pollution that can hinder lung growth. Patients with COPD suffer from symptoms that include breathing difficulties and fatigue due to increased stiffness and blockage of the airways. Before starting treatment, it is imperative to accurately detect the location of the obstruction sites to devise a better treatment plan for the patient. Unfortunately, conventional diagnos- tic methods like bronchoscopy often prove impractical due to their invasiveness and high costs. Computational Fluid Dynamics (CFD) has emerged as a promising tool for air- flow and blood flow simulations in patients (Xi and Zhao 2019; Arzani et al. 2022), largely successful due to its non- invasive nature and cost-effectiveness. Hu et al. (2022) pro- posed a novel methodology for detecting the location of obstruction sites in COPD patients using CFD and Convo- lutional Neural Networks (CNN). However, it is not feasi- ble to generate thousands of images using CFD simulations since they require a significant amount of computational re- sources. Moreover, datasets often suffer from imbalance, re- flecting the prevalence of specific cases in the population. Thus, our study centers its efforts on augmenting and bal- ancing the dataset created by Hu et al. (2022). Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. *These authors contributed equally to this work. Methodology Data Augmentation In the present study, a novel Generative Adversarial Net- work (COPD-FlowNet) is introduced to generate realistic velocity flow field images at a resolution of 128x128 pix- els. The generator in this network employs a series of CNN layers integrated with BatchNorm layers and leaky-ReLU activation functions to achieve the desired image dimen- sions. Meanwhile, the discriminator incorporates traditional Conv2D layers, along with fully connected layers, includ- ing Dropout and Dense layers, featuring several innovative adaptations. Classification A custom CNN network for classifying the location of ob- struction sites in the patient’s lungs is also introduced. The proposed classifier takes an input image of 128x128 pix- els as the input layer. It is followed by a sequence of con- volution blocks, with the corresponding depth continuously increasing from 8 filters to 128 filters. With each convolu- tion block, the kernel size is decreased, which allows the network to capture finer details in the data. The padding is set to ”valid,” which ensures that the size of the output fea- ture maps is reduced. The ReLU activation function intro- duces non-linearity, while MaxPooling2D layers and batch normalization enhance feature extraction and model conver- gence. The Batch normalization is inserted to mitigate the ”covariate shift” problem, thereby enhancing convergence during training. Dataset Distribution Left Right Both Total Original Dataset 45 113 137 295 After Augmentation 400 400 400 1200 Table 1: Distribution of dataset Experimental Setup The dataset used consisted of 295 images of velocity flow fields at the trachea of patients with COPD. These images were obtained by CFD simulations of airflow in patient- specific lung geometries. The dataset is divided into three arXiv:2312.11561v1  [cs.LG]  17 Dec 2023Figure 1: Overall Architecture of COPD-FlowNet classes named left, right, and both, depending on the loca- tion of the obstruction in the lung. The exact distribution of the dataset before and after augmentation is given in Table 1. Experiment Analysis and Results Our classification model outperforms well-established benchmark algorithms such as ResNet-50 and EfficientNet. Notably, before augmentation, our CNN model achieved an accuracy of 73.94%. However, after implementing augmen- tation techniques, this accuracy increased to 96.35%. Conse- quently, our approach surpassed the performance of state-of- the-art (SOTA) algorithms in the conventional performance metrics such as Accuracy and F1-score, as illustrated in Ta- ble 2. The normalized confusion matrix and ROC curve dis- played in Figure 2, depict the predicted test sample distri- bution towards each class and the classification model’s per- formance at different classification thresholds respectively. Method Accuracy Class Precision Recall F1 Score Proposed 96.35% Left 0.98 0.92 0.95 Right 0.97 1.00 0.98 Both 0.94 0.97 0.96 ResNet-50 96% Left 0.94 0.94 0.94 Right 0.96 0.98 0.97 Both 0.98 0.97 0.97 EfficientNet 95.00% Left 0.94 0.92 0.93 Right 0.96 0.96 0.96 Both 0.94 0.97 0.95 Table 2: Experimental Analysis Conclusion Our proposed approach represents an integration of deep learning and CFD. It demonstrates the potential use of CFD for medical diagnosis and its enhancement using genera- tive networks. Further, the proposed CNN outperforms well- established models such as ResNet-50 and EfficientNet. Fu- ture works will include generating datasets that can detect more specific locations of the obstruction sites since cur- rently the model only predicts whether the obstruction is in the left lung, right lung, or both lungs. (a)  (b) Figure 2: (a) Normalized confusion matrix, (b) Receiver Op- erating Characteristic (ROC) curve References Arzani, A.; Wang, J.-X.; Sacks, M.; and Shadden, S. 2022. Machine Learning for Cardiovascular Biomechanics Mod- eling: Challenges and Beyond. Annals of Biomedical Engi- neering, 50: 1–13. Hu, P.; Cai, C.; Yi, H.; Zhao, J.; Feng, Y .; and Wang, Q. 2022. Aiding Airway Obstruction Diagnosis With Compu- tational Fluid Dynamics and Convolutional Neural Network: A New Perspective and Numerical Case Study. Journal of Fluids Engineering, 144(8): 081206. Momtazmanesh, S.; Saeedi Moghaddam, S.; Ghamari, S.- H.; Rad, E.; Rezaei, N.; Shobeiri, P.; Aali, A.; Abbasi- Kangevari, M.; Abbasi-Kangevari, Z.; Abdelmasseh, M.; Abdoun, M.; Abdulah, D.; Abdullah, A. Y . M.; Abedi, A.; Abolhassani, H.; Abrehdari-Tafreshi, Z.; Achappa, B.; Adane, D.; Adane, T.; and Farzadfar, F. 2023. Global burden of chronic respiratory diseases and risk factors, 1990–2019: an update from the Global Burden of Disease Study 2019. The Lancet, 59. Xi, J.; and Zhao, W. 2019. Correlating exhaled aerosol im- ages to small airway obstructive diseases: A study with dy- namic mode decomposition and machine learning. PLOS ONE, 14(1): 1–22.",
      "references": [
        "Machine Learning for Cardiovascular Biomechanics Mod- eling: Challenges and Beyond.",
        "Aiding Airway Obstruction Diagnosis With Compu- tational Fluid Dynamics and Convolutional Neural Network: A New Perspective and Numerical Case Study.",
        "Global burden of chronic respiratory diseases and risk factors, 1990–2019: an update from the Global Burden of Disease Study 2019.",
        "Correlating exhaled aerosol im- ages to small airway obstructive diseases: A study with dy- namic mode decomposition and machine learning."
      ],
      "meta_data": {
        "arxiv_id": "2312.11561v1",
        "authors": [
          "Aryan Tyagi",
          "Aryaman Rao",
          "Shubhanshu Rao",
          "Raj Kumar Singh"
        ],
        "published_date": "2023-12-17T15:09:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses non-invasive, cost-effective localization of airway obstruction in COPD by combining CFD-derived airflow velocity fields with deep learning while mitigating limited/imbalanced CFD datasets. Contributions: (1) COPD-FlowNet, a custom GAN to generate realistic 128×128 tracheal velocity flow-field images for data augmentation and class balancing; (2) a custom CNN classifier to predict obstruction laterality (left, right, both); (3) demonstrates large performance gains from GAN-based augmentation, achieving 96.35% accuracy and outperforming ResNet-50 and EfficientNet baselines on the task.",
        "methodology": "Data augmentation via a custom GAN: generator built from stacked CNN layers with BatchNorm and leaky-ReLU to synthesize 128×128 velocity-field images; discriminator uses Conv2D plus fully connected layers with Dropout/Dense and custom adaptations. Classification via a bespoke CNN: input 128×128 image → multiple convolution blocks with increasing channels (8→128) and decreasing kernel sizes, valid padding, ReLU activations, MaxPooling2D, and BatchNorm to improve convergence and feature extraction. Training leverages augmented balanced data; evaluation reports accuracy, per-class precision/recall/F1, ROC, and confusion matrix, with comparisons to ResNet-50 and EfficientNet.",
        "experimental_setup": "Dataset: 295 CFD-simulated airflow velocity flow-field images at the trachea from patient-specific lung geometries; labeled into three classes by obstruction location: left (45), right (113), both (137). Augmentation: GAN-generated synthetic samples to balance classes to 400 each (total 1200). Benchmarks: ResNet-50 and EfficientNet. Validation/analysis: reports pre-augmentation accuracy (73.94%) vs post-augmentation (96.35%); provides per-class precision/recall/F1, normalized confusion matrix, and ROC curve (exact split protocol not specified in the abstract).",
        "limitations": "Relies on a small original dataset and synthetic GAN-generated images; risk of overfitting or learning GAN artifacts and uncertain generalization to unseen patients/centers. Only coarse obstruction localization (left/right/both) rather than segment-level sites. Uses 2D 128×128 velocity-field images from CFD at the trachea, which may discard 3D spatiotemporal airflow information and depends on CFD modeling assumptions (geometry reconstruction, boundary conditions). Experimental details are limited (e.g., train/val/test split, cross-validation, hyperparameters, statistical significance), making reproducibility and robustness harder to assess.",
        "future_research_directions": "Extend labels to finer, anatomically specific obstruction locations (lobar/segmental airways) and potentially multi-obstruction severity grading. Validate on larger, multi-center cohorts with strict external test sets and ablation studies on GAN augmentation impact. Incorporate 3D and/or time-resolved flow representations, multi-plane features, or physics-informed/conditional generative models tied to boundary conditions and anatomy. Combine CFD features with clinical variables or imaging modalities (CT, spirometry) for improved diagnostic utility and calibration/uncertainty estimates; add interpretability methods to relate flow patterns to predicted obstruction sites.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "The Power of Prompt Tuning for Low-Resource Semantic Parsing",
      "full_text": "arXiv:2301.08752v1  [eess.IV]  20 Jan 2023 OPTIMIZED LEARNED ENTROPY CODING P ARAMETERS FOR PRACTICAL NEURAL-BASED IMAGE AND VIDEO COMPRESSION Amir Said Reza P ourreza Hoang Le Qualcomm AI Research, ∗ San Diego, CA, USA ABSTRA CT Neural-based image and video codecs are signiﬁcantly more power-efﬁcient when weights and activations are quantized to low-precision integers. While there are general-purpos e techniques for reducing quantization effects, large losse s can occur when speciﬁc entropy coding properties are not con- sidered. This work analyzes how entropy coding is affected by parameter quantizations, and provides a method to min- imize losses. It is shown that, by using a certain type of coding parameters to be learned, uniform quantization be- comes practically optimal, also simplifying the minimizat ion of code memory requirements. The mathematical properties of the new representation are presented, and its effectiven ess is demonstrated by coding experiments, showing that good results can be obtained with precision as low as 4 bits per network output, and practically no loss with 8 bits. Index T erms— Learned image and video compression, neural network quantization, entropy coding. 1. INTRODUCTION After years of research showing the advantages of neural- based techniques for video coding [1, 2], and recent demon- strations of real-time decoding in mobile devices [3, 4], th ere is growing interest in their standardization and commercia l deployment [5], and a main practical implementation chal- lenge is the minimization of decoding power requirements. Implementing neural network (NN) computations with low-precision integers signiﬁcantly reduces power needs, but requires minimizing the effects of quantizing networks weights and activations [6, 7, 8]. For neural image and video codecs, efﬁcacy is strongly deﬁned by the entropy coding stage and related networks, and can be severely degraded. Analyzing quantization effects in NNs can be difﬁcult because they implement non-linear processes, and learned variables have unknown properties. T echniques based on re-training under quantization constraints help to reduce the losses, but have limited efﬁcacy. In this work it is shown that NN outputs used for entropy coding have well-deﬁned meaning and properties, which can be used for predicting quantization effects. Furthermore, those properties can be used for modifying the loss function ∗ Qualcomm AI Research is an initiative of Qualcomm T echnolog ies, Inc. Fig. 1. Quantized values from normal distribution with std. dev . σ , commonly used in neural-based codecs. used while learning, and it is demonstrated that networks ca n be trained to be minimally sensitive to quantization. Further analysis shows that this optimized representa- tion is remarkably independent of the quantization, and thu s networks do not need to be retrained even after signiﬁcant changes to entropy coding design choices. Thus, in those networks the uniform quantization can also be extended be- yond the hardware requirements, and used to minimize the memory needed for storing entropy coding tables. Experimental results show that the proposed solution can guarantee good performance ( ≈ 2% redundancy) with pre- cision as low as 4 bits, and with relative redundancy reduc- tion by a factor of 4 for each additional bit, reaching less than 0.01% for 8 bits. 2. ENTROPY CODING FOR NN-BASED CODECS Entropy coding in conventional codecs is based on estimat- ing the probabilities of variables to be coded. For example, in video standards like HEVC and VVC [9, 10, 11], adap- tive binary arithmetic coding contexts are used for estimat ing probabilities related to data elements. Neural-based codecs use a different principle: a param- eterized probability distribution (pdf) is chosen for trai ning, and a learned scheme (table or NN) is used to determine the parameter value to be used for coding a quantized random variable. Fig. 1 shows an example of the commonly used nor- mal distribution, and unit-step quantization. The sequence of probabilities of random variable y ∼ N (0,σ2), quantized to value n∈ Z, is pn(σ) = 1 2 [ erfc (2n− 1√ 8σ ) − erfc (2n+ 1√ 8σ )] , (1)Fig. 2 . Example of a neural-based codec architecture, including scale hyperprior networks for entropy coding. with entropy H(σ) def = − ∞∑ n=−∞ pn(σ) log2(pn(σ)) , (2) where the complementary error function is erfc(x) def = 2 √ π ∫ ∞ x e− t2 dt. (3) In practical implementations special symbols are used to code values in underﬂow and overﬂow inﬁnite sets, which have very small but nonzero probabilities, as shown in Fig. 1 . Within those sets the values are sub-optimally coded with, f or example, Elias universal codes [12]. Also shown in Fig. 1 are the elements of code vector c(σ) (CV), which contain the probabilities to be used for arithme tic coding (AC), within a given range R ci(σ) =      ∑ − R n=−∞ pn(σ), i = 0 , pi− R(σ), i = 1 ,2,..., 2R− 1,∑ ∞ n=R pn(σ), i = 2 R. (4) T o simplify the notation we ignore implementation de- tails like conversions to integer-valued cumulative sums f or AC [13, 14]. It is assumed that Rand AC precision are cho- sen to obtain compression very close to entropy. This type of entropy coding is used within a NN-based codec as shown in Fig. 2, corresponding to the scale hyper- prior architecture [15]. There are other codec conﬁguratio ns, but our main interest is in the per-element coding stages shown in Fig. 3, which is used with all factorized priors. The latent y to be coded is quantized with Qy→ q, which is taken into account during ﬂoating-point training. The pr ob- lems addressed here are caused by the quantization stage Qσ → ˆσ , representing the use of low-precision network outputs during coding, but not during training. Since code vectors must be bitwise identical at the en- coder and decoder [16], given a range U = [ σmin,σmax], we assume they share a monotonic sequence {tk}N k=0 parti- tioning U, and that latent y is coded using c(ρ∗ k) whenever ˆσ∈ [tk,tk+1). T o determine the optimal values of ρ∗ k for code vector computation we measure the average redundancy, deﬁned by Fig. 3. Per-element entropy coding, including quantization of pdf parameter σ . Fig. 4 . Example of nonuniform quantization of parameter σ , de- signed for constant maximum relative redundancy . the Kullback-Leibler divergence R(σ,ρ) def = ∞∑ n=−∞ pn(σ) log2 (pn(σ) pn(ρ) ) . (5) Since practical image and video codecs are meant to be efﬁcient in a wide range of bit rates, it is interesting to hav e entropy coding designed to minimize relative redundancy L(σ,ρ) def = R(σ,ρ) H(σ) . (6) and to enable good performance for all use cases use ρ∗ k = arg min ρ ∫ tk+1 tk L(σ,ρ) dσ. (7) 3. NEW PDF P ARAMETERIZA TION From these deﬁnitions it is possible to optimize non-unifor m quantization schemes as shown in the example of Fig. 4, where similarly to other quantization solutions, all inter vals have the same maximum relative redundancy and averages ¯Lk def = 1 tk+1 − tk ∫ tk+1 tk L(σ,ρ∗ k) dσ. (8) With sufﬁciently ﬁne quantization of σ, L(σ,ρ∗ k) is ap- proximately quadratic, and we can use ρ∗ k ≈ { ρ: L(tk,ρ) = L(tk+1,ρ)} , (9) ¯Lk ≈ L(tk,ρ∗ k) 3 = L(tk+1,ρ∗ k) 3 . (10) Neural-based codecs using precise ﬂoating-point com- putations can be implemented with arrays of pre-computedFig. 5 . Proposed modiﬁcation of codec in Fig. 3, replacing pdf parameter σ with parameter u optimized for quantization. Fig. 6. Mapping from non-uniform to uniform quantization. code vectors, and the approach shown in the example of Fig. 4. However, for practical codecs, which must handle a wide range of bit rates, it is necessary to support, for exam- ple, σ ∈ [0.1,1000] with the precision of hyperprior network output ˆσconstrained to only 8 bits. This means that {tk}N k=0 must also use only 8 bits, severely constraining the design and resulting in signiﬁcant losses. T o solve this problem we can exploit the fact that σis only used for rate estimation during training, and while computi ng code vectors. The proposed solution is shown in Fig. 5: hy- perprior networks are trained to compute a new pdf parameter u∈ [0,1] that is converted as σ = T(u). For analysis we de- ﬁne u ∈ R, but note that during actual coding the quantized networks only need to generate integer k. This approach is used in [16], where the conversion is chosen to be in the form σ= eαu +β . Fig. 6 shows how optimal T(u,N) can be deﬁned, for a given N, to obtain constant maximum relative redundancy, and the following algorithm can be used for its computation. 1. Given N and [σmin,σmax], choose initial maximum re- dundancy ǫ; initialize t0 = σmin. 2. For n= 1 ,2,...,N : (a) Set ρ∗ n− 1 = {ρ>t n− 1 : L(tn− 1,ρ) = ǫ}; (b) Set tn = {t>ρ ∗ n− 1 : L(t,ρ∗ n− 1) = ǫ}. 3. If tN ≈ σmax then stop. 4. If tN < σmax then increase ǫ, otherwise decrease ǫ, using a method for unidimensional search. 5. Go to step 2. The surprising conclusion from computing T(u,N) for several values of N, and interpolating between the sampled values, is that all results are practically identical. This means that if we compute T(u) = lim N→∞ T(u,N), (11) we obtain a single reference function that can be used for all network trainings, and the value of Ncan be chosen and mod- iﬁed later, without the need for re-training. The rationale for determining T(u) can be seen from the lower part of Fig. 6. For increasing values of N the redun- dancy curves in all intervals should approximate quadratic equations on u, with same second derivatives. This can be achieved if we can ﬁnd T(u) and constant αsuch that ∂2 ∂u2 L(T(u),ρ) ⏐ ⏐ ⏐ ⏐ ρ=T (u) = α, u ∈ [0,1]. (12) Deﬁning ψ(σ) def = ∂2 ∂σ2 L(σ,ρ) ⏐ ⏐ ⏐ ⏐ ρ=σ (13) it can be shown that, thanks to properties of L(σ,ρ), eq. (12) is equivalent to d duT(u) = √ α ψ(T(u)), (14) and thus function T(u) can be determined by solving this or- dinary differential equation (ODE) using boundary conditi ons T(0) = σmin, T(1) = σmax. For any sequence of differentiable parameterized proba- bilities it can be shown that ψ(σ) = 1 ln(2)H(σ) ∞∑ n=−∞ 1 pn(σ) [ dpn(σ) dσ ] 2 , (15) and for normal pdf we have ψ(σ) = 1 ln(2)H(σ) { [φ1(σ)]2 1 − ω1(σ) + (16) ∞∑ n=1 [φ2n− 1(σ) − φ2n+1(σ)]2 ω2n− 1(σ) − ω2n+1(σ) } , where ωk(σ) def = erfc ( k√ 8σ ) , φ k(σ) def = ke− k2/ (8σ 2) √ 2πσ2 . (17) 4. EXPERIMENT AL RESUL TS The ODE of eq. (14) was solved using the 4th-order Runge- Kutta method [17, §17.1], and boundary conditions σ ∈ [0.1,1000]. The solution T(u) is shown in Fig. 7. A quick observation may lead to the incorrect conclusion that it can be approximated by a function like 104u2− 1 (red line). Since T(u) is deﬁned by a differential equation, the qual- ity of the approximation must be measured by relative errors of derivatives. Those are shown in Fig. 8, where we canFig. 7 . The optimal pdf parameter conversion function T (u) and two approximations. Fig. 8. Derivatives of pdf parameter conversion function T (u) and its approximations. also see a much better approximation Tπ (u) = 10 π (u), where polynomial π(u) = 2 .49284 u3 + 0.93703 u2 + 0.57013 u− 1, (18) minimizes the maximum relative derivative error. Fig. 9 shows how the average relative redundancy ¯L(σ) varies with σwhen uniform quantization is applied to pdf pa- rameter u, and N code vectors are used for entropy coding latent variables y, as shown in Fig. 5. Following the condi- tion deﬁned by eq. (13), ¯L(σ) does not change when T(u) is used for pdf parameter conversion. On the other hand, there are relatively small variations when approximation Tπ (u) is used instead, and those deviations correspond to errors in a p- proximating the derivatives (cf. Fig. 8). W e also observe that the curves are nearly perfectly paral- lel, conﬁrming the predominance of the quadratic term in the T aylor series of the relative redundancy. This is also veriﬁ ed by the fact that ¯L(σ) quadruples when the N is halved. Non-uniform quantizations deﬁned by transformation T(u) were used for entropy coding values from a ﬂoating- point implementation of the scale hyperprior codec [15], in the Kodak image dataset [18]. Redundancy results are shown in T able 1, for average bit rates between 0.25 and 2.5 bits/pixel. For values of N larger than about 128 (7 bit represen- tation) the redundancy from quantization is 0.01% or below , which is within the experimental variation. For smaller val ues of N we can observe that Fig. 9. A verage relative redundancies from T (u) (dashed) and ap- proximation 10π (u) (solid), for different number of quantization in- tervals N. T able 1. Coding results obtained using scale hyperprior codec ap- plied to 24 images of Kodak dataset. N CV mem. Rel. redundancy (%) @ bit rate (Kbytes) 0.25 0.50 1.00 1.50 2.50 256 51.6 0.00 0.00 0.00 0.00 0.01 192 38.7 0.00 0.00 0.00 0.01 0.01 128 25.8 0.04 0.04 0.04 0.03 0.03 96 19.4 0.03 0.04 0.04 0.04 0.05 64 12.9 0.13 0.12 0.12 0.12 0.11 48 9.7 0.24 0.23 0.22 0.22 0.20 32 6.5 0.38 0.38 0.40 0.41 0.42 24 4.9 0.85 0.81 0.81 0.82 0.79 16 3.4 1.79 1.76 1.76 1.78 1.75 • The redundancies are practically constant for all bit rates, meaning that the proposed coding method achieves the design objective of having constant relative redun- dancies. • The average redundancy levels match those of Fig. 9, showing that features predicted by theory match the practical implementation. The second column of T able 1 contains the total amount of memory needed to store the code vectors, when using 16 bits/element. This shows that the proposed method can be directly used to minimize memory. For example, even if the network is able to support 8 bit outputs, and redundancy below 0.5% is acceptable, the pdf parameter can be further quantized to 5 bits, by simply discarding the 3 least signiﬁc ant bits, to reduce CV memory from 51.6 to 6.5 Kbytes. 5. CONCLUSIONS It is shown that the effects of quantization on the entropy co d- ing can be analyzed by evaluating coding redundancy, and this can be used for changing the pdf parameter to be learned so that average relative redundancy becomes practically co n- stant. In consequence, networks do not need to be re-trained when the pdf parameter quantization is changed, which we show can also be done intentionally to minimize memory for code tables.6. REFERENCES [1] S. Ma, X. Zhang, C. Jia, Z. Zhao, S. W ang, and S. W ang, “Image and video compression with neural networks: a review , ” IEEE T rans. Circuits Syst. V ideo T echnol., vol. 30, no. 6, pp. 1683–1698, June 2020, arXiv:1904.03567v2. [2] D. Ding, Z. Ma, D. Chen, Q. Chen, Z. Liu, and F . Zhu, “ Advances in video compression system us- ing deep neural network: a review and case studies, ” Proc. IEEE , vol. 109, no. 9, pp. 1494–1520, Mar. 2021, arXiv:2101.06341v1. [3] Qualcomm AI Research, “W orld’s ﬁrst software- based neural video decoder running HD format in real-time on a commercial smartphone, ” URL: https://www .qualcomm.com/news/onq/2021/06/17/worlds- ﬁrst-software-based-neural-video-decoder-running-hd - format-real-time, June. 2021. [4] Qualcomm AI Research, “Demonstration of real time neural video decoding on a mobile device, ” URL: https://www .youtube.com/watch?v=WUnlSHenr08, Dec. 2021. [5] J. Ascenso, P . Akyazi, F . Pereira, and T . Ebrahimi, “Learning-based image coding: early solutions review- ing and subjective quality evaluation, ” in Proc. SPIE 11353, Optics, Photonics and Digital T ech. for Imaging Applicat. VI , Apr. 2020, pp. 164–176. [6] M. Nagel, M. van Baalen, T . Blankevoort, and M. W elling, “Data-free quantization through weight equalization and bias correction, ” in Proc. IEEE/CVF Int. Conf. Comput. V ision , Oct. 2019, pp. 1325–1334, arXiv:1906.04721v3. [7] M. Nagel, M. Fournarakis, R. A. Amjad, Y . Bon- darenko, M. van Baalen, and T . Blankevoort, “ A white paper on neural network quantization, ” arXiv:2106.08295v1, June 2021. [8] T . Liang, J. Glossner, L. W ang, S. Shi, and X. Zhang, “Pruning and quantization for deep neural network ac- celeration: a survey, ” Neurocomputing, vol. 461, pp. 370–403, Oct. 2021, arXiv:2101.09671. [9] G. J. Sullivan, J.-R. Ohm, W .-J. Han, and T . Wie- gand, “Overview of the High Efﬁciency V ideo Coding (HEVC) Standard, ” IEEE T rans. Circuits Syst. V ideo T echnol., vol. 22, no. 12, pp. 1649–1668, Dec. 2012. [10] V . Sze and D. Marpe, “Entropy coding in HEVC, ” in High Efﬁciency V ideo Coding (HEVC): Algorithms and Architectures, V . Sze, M. Budagavi, and G. J. Sullivan, Eds., chapter 8, pp. 209–274. Springer, 2014. [11] B. Bross, Y .-K. W ang, Y . Y e, S. Liu, J. Chen, G. J. Sulli- van, and J.-R. Ohm, “Overview of the versatile video coding (VVC) standard and its applications, ” IEEE T rans. Circuits Syst. V ideo T echnol. , vol. 31, no. 10, pp. 3736–3764, Oct. 2021. [12] P . Elias, “Universal codeword sets and representation s of the integers, ” IEEE T rans. Inf. Theory , vol. 21, no. 2, pp. 194–203, Mar. 1975. [13] A. Said, “ Arithmetic coding, ” in Lossless Compres- sion Handbook , K. Sayood, Ed., chapter 5, pp. 101–152. Academic Press, San Diego, CA, 2003. [14] A. Said, “Introduction to arithmetic coding – theory and practice, ” T echnical Report HPL-2004-76, Hewlett Packard Laboratories, Palo Alto, CA, USA, Apr. 2004, (http://www .hpl.hp.com/techreports/2004/HPL- 2004-76.pdf). [15] J. Ball´ e, D. Minnen, S. Singh, S. J. Hwang, and N. John- ston, “V ariational image compression with a scale hy- perprior, ” in Sixth Int. Conf. Learning Representa- tions, V ancouver, Canada, Apr. 2018, arXiv preprint arXiv:1802.01436v2. [16] J. Ball´ e, N. Johnston, and D. Minnen, “Integer networks for data compression with latent-variable models, ” in Int. Conf. Learning Representations , New Orleans, Louisiana, USA, May 2019, URL: https://openreview .net/forum?id=S1zz2i0cY7. [17] W . H. Press, S. A. T eukolsky, W . T . V etterling, and B. P . Flannery, Numerical Recipes: The Art of Scientiﬁc Computing, Cambridge University Press, Cambridge, UK, third edition, 2007. [18] Kodak Image Dataset, URL: http://www .cs.albany .edu/ xy- pan/research/snr/Kodak.html.",
      "references": [
        "Image and video compression with neural networks: a review",
        " Advances in video compression system us- ing deep neural network: a review and case studies,",
        "W orld’s ﬁrst software- based neural video decoder running HD format in real-time on a commercial smartphone,",
        "Demonstration of real time neural video decoding on a mobile device,",
        "Learning-based image coding: early solutions review- ing and subjective quality evaluation,",
        "Data-free quantization through weight equalization and bias correction,",
        " A white paper on neural network quantization,",
        "Pruning and quantization for deep neural network ac- celeration: a survey,",
        "Overview of the High Efﬁciency V ideo Coding (HEVC) Standard,",
        "Entropy coding in HEVC,",
        "Overview of the versatile video coding (VVC) standard and its applications,",
        "Universal codeword sets and representation s of the integers,",
        " Arithmetic coding,",
        "Introduction to arithmetic coding – theory and practice,",
        "V ariational image compression with a scale hy- perprior,",
        "Integer networks for data compression with latent-variable models,",
        "Numerical Recipes: The Art of Scientiﬁc Computing"
      ],
      "meta_data": {
        "arxiv_id": "2301.08752v1",
        "doi": "10.1109/ICIP46576.2022.9897505",
        "authors": [
          "Amir Said",
          "Reza Pourreza",
          "Hoang Le"
        ],
        "published_date": "2023-01-20T18:01:31Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses degradation in neural image/video codecs when entropy-model network outputs (e.g., scale σ) are quantized to low-bit integers for power-efficient inference. Analyzes entropy-coding redundancy induced by parameter quantization and proposes a new learned pdf-parameter representation u∈[0,1] with a conversion σ=T(u) such that uniform quantization of u yields (approximately) constant, minimal relative redundancy across a wide σ range. Shows that the optimal mapping T(u) is essentially independent of the number of quantization levels N, enabling changing output precision or code-table memory without retraining. Demonstrates good coding with ~4-bit outputs (~2% redundancy) and negligible loss at 8 bits (<0.01% redundancy).",
        "methodology": "Models entropy coding for quantized latents y using a parametric prior (normal distribution with scale σ) and arithmetic coding with precomputed code vectors c(σ). Defines redundancy as KL divergence R(σ,ρ) between true quantized distribution pn(σ) and coding distribution pn(ρ), and optimizes relative redundancy L(σ,ρ)=R/H. Derives conditions for quantization intervals minimizing average relative redundancy and shows for fine quantization L behaves quadratically. Introduces reparameterization: hyperprior network predicts u, then σ=T(u). Chooses T(u) so that the second derivative of L w.r.t. u is constant when ρ=σ, leading to an ODE dT/du = sqrt(α/ψ(T(u))) where ψ(σ)=∂²L/∂σ²|_{ρ=σ} expressed via pn(σ) and dpn/dσ; provides closed-form components for normal prior. Solves ODE numerically (Runge–Kutta) for σ∈[0.1,1000]; gives polynomial approximation Tπ(u). Uniformly quantizes u to N levels to index code vectors; can further truncate bits to reduce table memory.",
        "experimental_setup": "Computes T(u) by solving the derived ODE with 4th-order Runge–Kutta using boundary conditions σmin=0.1, σmax=1000; evaluates average relative redundancy curves versus σ for various N under uniform quantization in u, comparing exact T(u) and polynomial approximation Tπ(u). End-to-end coding validation uses a floating-point implementation of the scale hyperprior image codec (Ballé et al., 2018) on the 24-image Kodak dataset. Tests multiple codebook sizes N (16 to 256) corresponding to 4–8 bits, across average bit rates 0.25–2.5 bpp; reports relative redundancy (%) and code-vector table memory (Kbytes, assuming 16-bit elements).",
        "limitations": "Analysis and experiments focus on a specific entropy model: unit-step quantized zero-mean normal prior parameterized by scale σ and factorized per-element coding; extensions to other priors (e.g., logistic mixtures, context models, mean/scale, or autoregressive dependencies) are not empirically demonstrated. Assumes arithmetic coding precision and range R are sufficient to make coding near-entropy and ignores some implementation details (integer CDFs, under/overflow handling) in theory. Mapping T(u) is derived for a fixed σ dynamic range [0.1,1000]; different ranges may require recomputation. Experimental validation is limited to Kodak and a floating-point codec run, measuring redundancy from parameter quantization rather than full RD (PSNR/MS-SSIM) impacts under quantized NN inference for complete codec pipelines (including weights/activations quantization).",
        "future_research_directions": "Generalize the reparameterization/ODE approach to other entropy models used in learned compression (e.g., Laplace/logistic, Gaussian with learned mean, mixtures, discretized distributions) and to context/adaptive models (autoregressive, checkerboard, transformer-based priors). Jointly optimize parameter mappings for multiple parameters (mean, scale, mixture weights) and for vector-valued outputs. Integrate the mapping into quantization-aware training pipelines to co-optimize network quantization (weights/activations) and entropy-model robustness, and validate on video codecs and larger datasets with full RD metrics and real-time integer inference. Explore hardware- and memory-aware design: adaptive selection of N per layer/feature, dynamic range control, and bit-truncation strategies for codebook storage with guaranteed redundancy bounds; extend to integer CDF generation without stored tables.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning",
      "full_text": "β-Ga2O3 Trench Schottky Diodes by Novel Low-Damage Ga- Flux Etching  Sushovan Dharaa, Nidhin Kurian Kalarickala, Ashok Dheenana, Sheikh Ifatur Rahmana,  Chandan Joishia, Siddharth Rajana,b  a Department of Electrical and Computer Engineering, The Ohio State University, USA,  b Department of Materials Science and Engineering, The Ohio State University, USA.   Email: dhara.2@osu.edu, rajan@ece.osu.edu    Abstract: 𝛽-Ga2O3 trench Schottky barrier diodes fabricated through a Gallium atomic beam  etching technique, with excellent field strength and power device figure of merit, are  demonstrated. Trench formation was accomplished by a low-damage Ga flux etch that enables  near-ideal forward operating characteristics that are independent of fin orientation. The reverse  breakdown field strength of greater than 5.10 MV/cm is demonstrated at breakdown voltage as  of 1.45 kV. This result demonstrates the potential for Ga atomic beam etching and high-quality  dielectric layers for improved performance in 𝛽-Ga2O3 vertical power devices.  𝛽-Ga2O3 is a promising material for kV class power devices due to its high theoretical  breakdown field (8 MV/cm) and power switching figure of merit 1–3. The availability of melt- based native substrates can provide semiconductor devices with lower defects and greater  reliability at a lower cost 4,5. Unlike other vertical power devices based on SiC and GaN, the  unavailability of p-type 𝛽-Ga2O36 makes power device design challenging. Without the ability  to form p-n homojunctions, diode architectures can only be realized using Schottky junctions.  However, this also poses a limitation in device breakdown performance , since the Schottky  barrier used for rectification cannot sustain high fields in reverse bias . The narrow triangular  tunnelling barrier near the metal -semiconductor interface under high field results in high  tunnelling current into the semiconductor and premature breakdown . This Schottky junction  limits the material's potential to be used in high-voltage diode applications. The use of alloyed  interlayers such as AlGaO7, which possesses a higher Schottky barrier , or the use of high-k  dielectric such as BaTiO 38 heterojunction to maintain a wider tunneling barrier showed  significant improvement to in sustaining high fields. Another method to design efficient diodes  is through formation of three-dimensional ‘trench’ device geometries, where the peak electric  field at the metal-semiconductor interface is pushed away into the bulk of the semiconductor. This approach can provide an excellent reverse breakdown field without sacrificing a forward  operating voltage close to the Schottky turn-on9,10.  To realize the trench Schottky barrier diode , it is critical to reduce damage to the  semiconductor in the fin etching process module. Additionally, an appropriate dielectric with a  high breakdown strength needs to be integrated.  It is well-known that ICP/RIE etching causes  sub-surface damage and depletion in 𝛽-Ga2O311,12. This report demonstrates a novel approach  to realize high aspect ratio trenches through a combination of low -damage Ga atomic flux - based etching process and a n Al2O3 dielectric layer deposited by plasma-assisted MBE to  fabricate trench Schottky diodes. Exposing 𝛽-Ga2O3 to Ga flux in vacuum results in a purely  chemical, damage -free etching process that significantly improves the on -resistance when  compared to devices with trenches formed by ICP/RIE processes.      Figure 1: Process flow for the Ga-flux etched trench SBD  The devices discussed here were fabricated on c ommercially obtained HVPE-grown  𝛽-Ga2O3 epitaxial layers (10 𝜇𝑚 thick epitaxial layer of doping (N D) ~ 5x10 16 cm-3) sourced  from Novel Crystal Technologies, Japan. The process flow (figure 1) began with the definition  of the 100 m x 1.8 m stripes to define the trench geometries. 100 nm of SiO2 was blanket  deposited for use as a hard-mask using plasma-enhanced chemical vapor deposition (PECVD)  at 250℃ and then patterned for fin etching using photolithography, followed by dry and wet  Substrate Al2O3 1) SiO2 etching mask SiO2 Substrate Al2O3 Substrate SiO2 Substrate β-Ga2O3 (001) ~5x1016 cm-3 2) Fin etching using  Ga flux 92 nm 4) Top dielectric layers  removal Ga 5) Top and bottom metallization SiO2 1.5m1.8 m 3) Cleaning Ga droplets  and MBE Al2O3 deposition Substrate Ti/Au Ni/Au Trench SBD Ni/Au MOSCAP Ni/Au Planar SBDetching of SiO2. The 𝛽-Ga2O3 fins were then defined by exposing the sample to atomic gallium  flux13, where reaction of Ga with the 𝛽-Ga2O3 leads to the formation of volatile sub -oxide  (Ga2O) which desorbs at temperatures above 500℃, resulting in chemical etching of the sample.  A two-step etching process (step 1- Tsub=700 ℃, BEP = 5x10-7 Torr, time = 120 minutes and  step 2- Tsub=550 ℃, BEP = 1.5x10-7 Torr, time = 35 minutes ) was carried out in a Riber M7  molecular beam epitaxy (MBE) chamber . This two-step process helps to avoid the formation  of a Si-rich layer near the top of the etched m aterial13. The process results in sharply defined  features with smooth sidewalls. The trench depth was measured by atomic force microscopy to  be ~1.2 𝜇𝑚. After the Ga-flux etching process, the sample was taken out of the MBE chamber  and dipped in HCl for 5 minutes to remove any excess Ga droplets from the surface. After the  etching step, the sample was reintroduced into the MBE chamber, a a 92 nm thick Al2O3 layer  was deposited over the entire sample by exposing the sample to atomic Al -flux and oxygen  plasma (Al flux  = 2x10 -8 Torr, O 2 flow=2.5 sccm, plasma power=250 W) at a substrate  temperature of 400°C in the same MBE chamber. Windows to the top of each fin were then  patterned by photlithography and the SiO2 and Al2O3 dielectric layers were removed by a dry  etching, followed by wet etching (dilute buffered oxide etch (BOE) ). Top Schottky contacts  were then formed by sputtering 50 nm of Ni for side wall coverage followed by electron beam  evaporation of Ni/Au (30/100 nm). Finally, the ohmic back contact (Ti/Au, 30/70 nm) was  deposited by electron -beam evaporation.  MOS-capacitors were fabricated on the sample  sample on the Ga-flux etched 𝛽-Ga2O3 surface. Additionally,  a planar SBD on the un-etched  𝛽-Ga2O3 was fabricated for comparison.    Figure 2 : SEM imaging of (a) [100]-direction and, (b) [010]-direction-oriented fin structure  Fin structures were fabricated along both the [100] and [010] directions to investigate  the effect of Ga flux-based etching on different crystal planes. Figure 2 shows the tilted SEM  of the fin structures and the top view of the fins oriented along the [010] and [100] directions.  Both the fins were etched using a 1.8 m wide SiO 2 mask, and 1.2 µm of vertical etching  resulted in sub-lithographic fin widths of 1.58 m and 0.96 m along [010] and [100]  directions, respectively.    Figure 3: Forward current-voltage characteristics of the planar and trench SBDs (TSBD) in  (a) log and (b) linear scale with capacitance-voltage characteristics of the MOS capacitor on  the etched surface in the inset.  Two terminal e lectrical characterization was carried out using a Keysight  B1500A  semiconductor parameter analyzer. Room temperature current-voltage (J-V) measurements of  the two types of fin device structures and large-area planar Schottky diodes are shown in figure  3(a-b). The forward J-V characteristics of both the trench Schottky barrier diodes  showed a  turn-on voltage of ~1.0 V, similar to the turn-on voltage of a planar Schottky diode fabricated  on the same epitaxial structure. The trench SBDs oriented along [100] and [010] directions  showed similar turn-on voltage and and resistance. In comparison with fins reported previously  that were fabricated using dry etching exhibited depletion of carriers in the drift layer  from  plasma damage and strong dependence on fin orientation11,12. The specific on-resistance of 50  m circular planar device was ~1.7 mΩ-cm2, while trench diodes showed on-resistance close  to 0.5 mΩ-cm2 at 2 V of forward biasing. This lower resistance of fin structures is due to current  spreading. When we account for the current spreading, the on-resistance is computed to be 1.01  mΩ-cm2 and 1. 20 mΩ-cm2 for fins oriented in [100] and [010] direction  (details in   supplementary section). The reverse leakage current was found to be close to the noise floor  margin of the measurement system  for both the planar and trench SBDs . To understand the  properties of the Al 2O3 dielectric layers and the interface with the semiconductor, MOS  capacitor structures on the same sample were electrically character ized. The capacitance - voltage (CV) characteristics of the MOS -capacitor showed relatively low voltage hysteresis  (figure 3(b)-inset), indicating a good quality interface. When compared with the simulated ideal  CV of this MOS structure, a positive flat band voltage shift was observed.  This confirms the  presence of negative fixed charge either at the Al 2O3/𝛽-Ga2O3 interface or in the bulk of the  Al2O3.     Figure 4: (a) Reverse breakdown measurement of the MOSCAP, planar, and trench SBDs. (b)  Simulated 2-dimensional electric field profiles of 0.98 µm wide fin at 1.45 kV . Electric field  profile along (c) AD (vertical cutline) and (d) B’C’C”B” (lateral cutline) for 0.96 µm wide fin.    The devices whose forward characteristics had been measured were selected for  measurement of higher reverse voltage breakdown using a Agilent B1505A semiconductor  parameter analyzer. The reverse J -V measurements show hard (destructive) breakdown  characteristics for the highest obtained reverse breakdown votlage from planar Schottky diodes  (250 V), trench SBD (1.45 kV),  and a planar MOS capacitor (1.52 kV) (figure 4(a)). To  investigate the electrostatics of the trench SBD, 2D simulations  of the device structures were  done using the Silvaco ATLAS package. Results for the 0.96 m-width fin are shown for an  applied voltage of 1.45 kV in figure 4(b). The vertical cutline A -D along the structure shows  the electric field profile along the cente r of the fin (figure 4(c)).  The electric field at the  metal/𝛽-Ga2O3 Schottky interface is <2 MV/cm. Along the vertical cutline, the peak electric  field was observed in the bulk at a depth of ~2 m, as expected from the trench architecture.  The lateral cutline as shown in figure 4(d) along B’-C’-C”-B” shows the presence of a peak in  the electric field near the anode edge at points B’ and B”.  This suggests that peak fields at the  metal edge (approaching > 8 MV/cm) are likely the cause of the breakdown.  We estimated the parallel plate fields in the 𝛽-Ga2O3 at the reverse breakdown  condition to be 2.10 MV/cm, 5.10 MV/cm, and 5.82 MV/cm for the planar SBD, trench SBDs,  and the MOSCAP respectively. For planar structures, the breakdown field was estimated using  one-dimensional electrostatics, 𝐸𝑓𝑖𝑒𝑙𝑑 =  √𝑞𝑁𝐷𝑉𝐵𝑅 𝜀⁄ , where 𝑞 is charge of electron, 𝑁𝐷  is the  doping of the semiconductor, 𝑉𝐵𝑅 is the breakdown voltage , and 𝜀 is the permitivity of the  semiconductor. For the trench structure, we estimated the field from simulation , as shown in  Figure 4(d). Observation of the devices post-breakdown suggest catastrophic breakdown near  the anode edge, which matches the peak field position expected from simulations.         Figure 5. (a) Parallel-plate field vs breakdown voltage voltage of Ga-flux etched trench SBD  in comparison to previously reported devices. (b) Plot of on-resistance vs. breakdown     The breakdown voltage and parallel plate  electric field of the trench Schottky diode  reported here is compared with previously reported 𝛽-Ga2O3 vertical diodes in Figure 5. 10,14– 20. The device demonstrated here shows one of the highest combinations of breakdown electric  field strength and breakdown voltage . Furthermore, t he damage-free atomic beam  etching  method adopted here leads to excellent forward conduction characteristics when compared to  standard ICP/RIE processes. The evaluated Baliga power device figure of merit (BFOM) is also  benchmarked with other reports in figure 5(b) and estimated to be higher than 2 GW/cm2, which  is one of the best values reported for 𝛽-Ga2O3, and compares favorably with state-of-art SiC  vertical devices. Improved edge termination is expected to greatly improve the performance of  the devices.  In summary, we have demonstrated a novel process to realize 𝛽-Ga2O3 trench Schottky  diodes using Ga atomic beam etching technique. A parallel-plate electric field strength of 5.12  MV/cm with an associated breakdown voltage of ~1.45 kV combined with a low on-resistance  results in a BFOM of greater than 2 GW/cm2. This result shows the potential  of 𝛽-Ga2O3  vertical device structures  for future high voltage electronics . We acknowledge funding from  Department of Energy / National Nuclear Se curity Administration under Award Number(s)  DE-NA0003921, and AFOSR GAME MURI (Award No. FA9550-18-1-0479, project manager  Dr. Ali Sayir). The content of the information does not necessarily reflect the position or the  policy of the federal government, and no official endorsement should be inferred.      Reference:  1 M. Higashiwaki, K. Sasaki, H. Murakami, Y. Kumagai, A. Koukitu, A. Kuramata, T.  Masui, and S. Yamakoshi, Semicond. Sci. Technol. 31, 034001 (2016).  2 M. Higashiwaki and G.H. Jessen, Appl. Phys. Lett. 112, 060401 (2018).  3 S.J. Pearton, J. Yang, P.H. Cary, F. Ren, J. Kim, M.J. Tadjer, and M.A. Mastro, Appl.  Phys. Rev. 5, 011301 (2018).  4 H.F. Mohamed, C. Xia, Q. Sai, H. Cui, M. Pan, and H. Qi, J. Semicond. 40, 011801  (2019).  5 B. Fu, Z. Jia, W. Mu, Y. Yin, J. Zhang, and X. Tao, J. Semicond. 40, 011804 (2019).  6 H. Peelaers, J.L. Lyons, J.B. Varley, and C.G. Van de Walle, APL Mater. 7, 022519  (2019).  7 P. P. Sundaram, F. Alema, A. Osinsky, and S.J. Koester, J. Vac. Sci. Technol. A 40,  043211 (2022).  8 Z. Xia, H. Chandrasekar, W. Moore, C. Wang, A.J. Lee, J. McGlone, N.K. Kalarickal,  A. Arehart, S. Ringel, F. Yang, and S. Rajan, Appl. Phys. Lett. 115, 252104 (2019).  9 W. Li, Z. Hu, K. Nomoto, R. Jinno, Z. Zhang, T.Q. Tu, K. Sasaki, A. Kuramata, D.  Jena, and H.G. Xing, in 2018 IEEE Int. Electron Devices Meet. IEDM (2018), p. 8.5.1- 8.5.4.  10 W. Li, K. Nomoto, Z. Hu, D. Jena, and H.G. Xing, IEEE Electron Device Lett. 41, 107  (2020).  11 W. Li, K. Nomoto, Z. Hu, D. Jena, and H.G. Xing, Appl. Phys. Express 12, 061007  (2019).  12 S. Dhara, N.K. Kalarickal, A. Dheenan, C. Joishi, and S. Rajan, Appl. Phys. Lett. 121,  203501 (2022).  13 N.K. Kalarickal, A. Fiedler, S. Dhara, H.-L. Huang, A.F.M.A.U. Bhuiyan, M.W.  Rahman, T. Kim, Z. Xia, Z.J. Eddine, A. Dheenan, M. Brenner, H. Zhao, J. Hwang, and  S. Rajan, Appl. Phys. Lett. 119, 123503 (2021).  14 S. Roy, A. Bhattacharyya, P. Ranga, H. Splawn, J. Leach, and S. Krishnamoorthy,  IEEE Electron Device Lett. 42, 1140 (2021).  15 C. Joishi, S. Rafique, Z. Xia, L. Han, S. Krishnamoorthy, Y. Zhang, S. Lodha, H. Zhao,  and S. Rajan, Appl. Phys. Express 11, 031101 (2018).  16 J. Yang, F. Ren, M. Tadjer, S.J. Pearton, and A. Kuramata, ECS J. Solid State Sci.  Technol. 7, Q92 (2018).  17 Z. (Ashley) Jian, S. Mohanty, and E. Ahmadi, Appl. Phys. Lett. 116, 152104 (2020).  18 Q. He, W. Hao, X. Zhou, Y. Li, K. Zhou, C. Chen, W. Xiong, G. Jian, G. Xu, X. Zhao,  X. Wu, J. Zhu, and S. Long, IEEE Electron Device Lett. 43, 264 (2022).  19 Y. Wang, H. Gong, Y. Lv, X. Fu, S. Dun, T. Han, H. Liu, X. Zhou, S. Liang, J. Ye, R.  Zhang, A. Bu, S. Cai, and Z. Feng, IEEE Trans. Power Electron. 37, 3743 (2022).  20 Y. Lv, Y. Wang, X. Fu, S. Dun, Z. Sun, H. Liu, X. Zhou, X. Song, K. Dang, S. Liang,  J. Zhang, H. Zhou, Z. Feng, S. Cai, and Y. Hao, IEEE Trans. Power Electron. 36, 6179  (2021).            Supporting Information    β-Ga2O3 Trench Schottky Diodes by Novel Low-Damage Ga- Flux Etching  Sushovan Dharaa, Nidhin Kurian Kalarickala, Ashok Dheenana, Sheikh Ifatur Rahman,  Chandan Joishia, Siddharth Rajana,b  a Department of Electrical and Computer Engineering, The Ohio State University, USA,   b Department of Materials Science and Engineering, The Ohio State University, USA.   Email: dhara.2@osu.edu, rajan@ece.osu.edu    Supporting Information 1: (Estimation of On-resistance)    Device schematic for On-resistance estimation.  𝑅𝑒𝑠𝑖𝑠𝑡𝑖𝑣𝑖𝑡𝑦 (𝜌) =  1 𝑞µ𝑁𝐷  .   Where, ND is Doping of the semiconductor, q is electron charge, and µ is mobility  Thus,    𝜌𝑒𝑝𝑖. = 1.25 𝛺𝑐𝑚 (𝑁𝐷 = 5𝐸16 𝑐𝑚3 ,  µ = 100 𝑐𝑚2 𝑉𝑠 )     and,  𝜌𝑠𝑢𝑏𝑠𝑡𝑟𝑎𝑡𝑒 = 10.8 𝑚𝛺𝑐𝑚 (𝑁𝐷 = 5.8𝐸18 𝑐𝑚3 ,  µ = 100 𝑐𝑚2 𝑉𝑠 )    𝑅𝑠𝑢𝑏𝑠𝑡𝑟𝑎𝑡𝑒 =𝜌𝑠𝑢𝑏𝑠𝑡𝑟𝑎𝑡𝑒 × 𝑠𝑢𝑏𝑠𝑡𝑟𝑎𝑡𝑒 𝑡ℎ𝑖𝑐𝑘𝑛𝑒𝑠 = 0.68 𝑚𝛺𝑐𝑚2  𝑅𝑒𝑝𝑖. = 𝜌𝑒𝑝𝑖. × wFin × (( hFin wFin )  +  1 2  ln (1 + 2hdrift wFin )), by assuming current spreading angle of 45°  The total device resistance can be estimated as,  𝑅𝐷𝑒𝑣𝑖𝑐𝑒 =  𝑅𝑠𝑢𝑏𝑠𝑡𝑟𝑎𝑡𝑒 + 𝑅𝑒𝑝𝑖.  𝑅𝑒𝑝𝑖.[100]= 0.33 𝑚𝛺𝑐𝑚2 → 𝑅𝐷𝑒𝑣𝑖𝑐𝑒[100]= 1.01 𝑚𝛺𝑐𝑚2  𝑅𝑒𝑝𝑖.[010]= 0.52 𝑚𝛺𝑐𝑚2 → 𝑅𝐷𝑒𝑣𝑖𝑐𝑒[010]= 1.20 𝑚𝛺𝑐𝑚2  𝑅𝑒𝑝𝑖(50 µ𝑚)= 1.05 𝑚𝛺𝑐𝑚2 → 𝑅𝐷𝑒𝑣𝑖𝑐𝑒(50 µ𝑚)= 1.73 𝑚𝛺𝑐𝑚2  This estimated resistance of the 50 m device is close to the measured resistance. Thus, we  can consider this resistance estimation assumption for trench devices.",
      "references": [],
      "meta_data": {
        "arxiv_id": "2303.04870v1",
        "doi": "10.1063/5.0151808",
        "authors": [
          "Sushovan Dhara",
          "Nidhin Kurian Kalarickal",
          "Ashok Dheenan",
          "Sheikh Ifatur Rahman",
          "Chandan Joishi",
          "Siddharth Rajan"
        ],
        "published_date": "2023-03-08T20:22:30Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Demonstrates β-Ga2O3 vertical trench Schottky barrier diodes (TSBDs) enabled by a novel low-damage gallium atomic-beam (Ga-flux) chemical etch for high-aspect-ratio trenches, yielding near-ideal forward conduction independent of fin orientation and markedly improved reverse blocking. Achieves destructive breakdown at ~1.45 kV with an extracted parallel-plate breakdown field >5.1 MV/cm and BFOM >2 GW/cm^2, among the best reported for β-Ga2O3 vertical diodes; shows that peak field is moved from the Schottky interface into the bulk, with remaining breakdown driven by anode-edge field crowding.",
        "methodology": "Process integration of (1) patterned SiO2 hard mask, (2) two-step Ga atomic flux etching in MBE (chemical formation/desorption of volatile Ga2O suboxide) to form ~1.2 µm deep trenches with smooth sidewalls while avoiding Si-rich layer, (3) HCl dip to remove Ga droplets, (4) in-situ plasma-assisted MBE deposition of 92 nm Al2O3 dielectric, (5) dielectric window opening and metallization to form trench Schottky contacts (sputtered Ni for sidewall coverage + Ni/Au) and Ti/Au backside ohmic. Electrical characterization via J–V and MOS C–V; electrostatic analysis via 2D Silvaco ATLAS simulations and field extraction/estimation formulas for planar devices.",
        "experimental_setup": "Substrates: commercial HVPE-grown β-Ga2O3 (001) epilayers, 10 µm thick, ND≈5×10^16 cm^-3 on conductive substrate (ND≈5.8×10^18 cm^-3 implied by resistance calc). Trench geometry: 100 µm × 1.8 µm stripes; trench depth ~1.2 µm (AFM). Fins oriented along [100] and [010]; sub-lithographic fin widths after etch ~0.96 µm ([100]) and ~1.58 µm ([010]). Etch conditions: step1 Tsub=700°C, BEP=5×10^-7 Torr, 120 min; step2 Tsub=550°C, BEP=1.5×10^-7 Torr, 35 min. Al2O3 deposition: 400°C, Al flux 2×10^-8 Torr, O2 2.5 sccm, plasma 250 W. Devices compared: Ga-flux-etched TSBDs, planar SBD on unetched surface, and planar MOS capacitors on etched surface. Measurements: room-temperature two-terminal J–V (Keysight B1500A) for forward/reverse leakage; high-voltage reverse breakdown (Agilent B1505A). Validation includes SEM/AFM morphology, MOS C–V hysteresis/flatband shift, destructive breakdown voltage; simulations of E-field profiles at 1.45 kV to locate peak fields and explain breakdown.",
        "limitations": "Breakdown is destructive and appears limited by electric-field crowding at the anode metal edge (simulated peaks approaching/exceeding ~8 MV/cm) rather than the Schottky interface; indicates insufficient edge termination/field management. Reported field values for trench devices rely on 2D simulation-based extraction and ‘parallel-plate’ estimates, which depend on geometry/doping assumptions. No long-term reliability, thermal, switching, or surge-current robustness data; forward metrics influenced by current spreading assumptions (45°) and may not directly translate across layouts. Device-to-device statistics/yield and sensitivity to dielectric fixed charge (observed negative fixed charge via positive flatband shift) are not fully quantified.",
        "future_research_directions": "Implement robust edge termination (guard rings, field plates, junction termination extensions, beveled edges) and dielectric/metal optimization to suppress anode-edge field peaks and push BV toward the material limit. Optimize fin/trench geometry (width, depth, pitch) and dielectric thickness/permittivity to balance Ron–BV and reduce field crowding; explore alternative high-k or multi-layer dielectrics and interface passivation to control fixed charge and traps. Extend Ga-flux etch integration to larger-area devices and other β-Ga2O3 orientations; assess process uniformity, scalability, and compatibility with manufacturing. Perform reliability studies (TDDB of Al2O3, bias-temperature stress, avalanche capability), temperature-dependent conduction/leakage, and switching characterization; investigate defect/trap spectroscopy to correlate etch/dielectric quality with leakage and breakdown.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning",
      "full_text": "Atomic diﬀusion due to hyperatomic ﬂuctuation for quasicrystals Yuki Nagai,1, 2,∗Yutaka Iwasaki,3, 4,†Koichi Kitahara,5 Yoshiki Takagiwa,3 Kaoru Kimura,3, 4and Motoyuki Shiga1 1CCSE, Japan Atomic Energy Agency, 178-4-4, Wakashiba, Kashiwa, Chiba 277-0871, Japan 2Mathematical Science Team, RIKEN Center for Advanced Intelligence Project (AIP), 1-4-1 Nihonbashi, Chuo-ku, Tokyo 103-0027, Japan 3National Institute for Materials Science (NIMS), 1-2-1 Sengen, Tsukuba, Ibaraki 305-0047, Japan 4Department of Advanced Materials Science, The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba 277-8561, Japan 5Department of Materials Science and Engineering, National Defense Academy, 1-10-20 Hashirimizu, Yokosuka, 239-8686, Kanagawa, Japan. (Dated: March 1, 2023) A quasicrystal is an ordered but non-periodic structure understood as a projection from a higher dimensional periodic structure. Some physical properties of quasicrystals are diﬀerent from those of conventional solids. An anomalous increase in heat capacity at high temperatures has been discussed for over two decades as a manifestation of a hidden high dimensionality of quasicrystals. A plausible candidate for this origin has been phason, which has excitation modes originating from additional degrees of freedom in the higher-dimensional lattice. However, most theoretical studies on phasons have used toy models. A theoretical study of the heat capacity of realistic quasicrystals or their approximants has yet to be conducted because of the huge computational complexity. To bridge this gap between experiment and theory, we show experiments and molecular simulations on the same material, an Al–Pd–Ru quasicrystal, and its approximants. We show that at high temperatures, aluminum atoms diﬀuse with discontinuous-like jumps, and the diﬀusion paths of the aluminum can be understood in terms of jumps corresponding to hyperatomic ﬂuctuations in six-dimensional space. It is concluded that the anomaly in the heat capacity of quasicrystals arises from extra degrees of freedom due to hyperatomic ﬂuctuations that play a role in diﬀusive Nambu–Goldstone modes. INTRODUCTION Quasicrystals (QCs)[1, 2] are solids with quasiperi- odic atomic structures. Mathematically, a quasiperiodic structure is constructed by a projection from a higher- dimensional space [3]: Every quasiperiodic structure in three-dimensional physical space can be described as a projection from a hypothetical higher-dimensional peri- odic crystal structure called “hyperlattice”. The hyper- lattice concept is commonly used to explain the static structures of QCs[4, 5]. QCs have unique physical properties that are not ob- served in conventional solids[6–8]. In particular, several quasicrystals at high temperatures show signiﬁcant in- creases in heat capacity. The heat capacity per atom at a constant volume cV for icosahedral Al–Pd–Mn [9– 11], Al–Cu–Fe [12], Al–Cu–Ru [13], and decagonal Al– Cu–Co [9, 10] QCs show large upward deviations from the Dulong–Petit limit 3 kB, where kB is the Boltzmann constant. Such anomalous heat capacity at high tem- peratures has been debated for over two decades, and a potential connection to the hidden higher-dimensional properties of QCs has been discussed. The phason, which contains additional degrees of free- dom because of the hyperlattice structure [14, 15], has been suggested as the origin of the high-temperature ∗nagai.yuki@jaea.go.jp †IWASAKI.Yutaka@nims.go.jp anomalous heat capacity [11, 16]. The phason modes originate from the fact that the free energy of the system is invariant under a rigid translation of the parallel space in the perpendicular dimension, where the hyperlattice decomposes into the parallel (physical) and the perpen- dicular space. In many solids, heat capacity at high tem- peratures approaches the Dulong–Petit limit 3 kB, be- cause full vibrational-mode degrees of freedom amount to three degrees of freedom per atom, each correspond- ing to a quadratic kinetic energy term and a harmonic potential energy term. However, the vibrations in QCs are beyond harmonic oscillations along the phason de- grees of freedom, which may contribute to the increased heat capacity. Inelastic neutron scattering [17] and coher- ent X-ray scattering [18] experiments on Al–Pd–Mn QC have observed characteristic phason excitation and dif- fusive modes above approximately 700 K. These results suggest that phason is excited at high temperatures with anomalous atomic motion, consistent with the tempera- ture range where anomalous heat capacity is observed. In addition, experimental evidence of anomalous atomic vi- brations at speciﬁc atomic sites in the structure at 1100 K has been observed in decagonal Al–Ni–Co QCs [19]. Further evidence that phason may be the cause of the anomalous heat capacity is seen in the relationship be- tween the degree of approximation and the heat capacity in quasicrystalline approximant crystals (ACs), which are periodic crystals that have the same local structure as the corresponding QC. The structure of ACs is classiﬁed by the degree of approximation represented by two consecu- tive numbers in the Fibonacci sequence, such as 1/0, 1/1, arXiv:2302.14441v1  [cond-mat.mtrl-sci]  28 Feb 20232 2/1, ···, qn/qn−1. A larger qn in an AC corresponds to a larger unit cell and a structure that is closer to that of a QC. As the degree of approximation increases, the AC be- comes closer in structure to the QC. The AC-speciﬁc heat is expected to become more anomalous as it approaches the QC limit because the phason degrees of freedom may increase[11, 13]. So far, most theoretical studies on phasons have used toy models consisting of a single atomic species in one or two dimensions [20, 21]. The three-dimensional atomic motion of multiple-element QCs has been studied exper- imentally in decagonal phases such as Al–Ni–Co QCs [19]. In a recent molecular dynamics (MD) simulation by Mihalkoviˇ c and Widom [22], an AC of Al–Cu–Fe qua- sicrystals containing 9846 atoms was studied, focusing on its energetic stability. However, it is not clear what contributes to the high-temperature anomalous heat ca- pacity in actual materials, although phason is a plausible candidate. Additionally, a direct computation of heat ca- pacity has never been done before because to adequately describe the dynamic behavior of QCs accounting for a huge number of atoms and complex atomic interactions is required. To bridge this large gap between experiment and the- ory, we study the same material from both theoretical and experimental approaches. First, we synthesized Al– Pd–Ru icosahedral QC and its ACs and observed the high-temperature anomalous heat capacity. We then per- formed a machine-learning molecular dynamics (MLMD) simulation for Al–Pd–Ru ACs and qualitatively repro- duced the increase of the heat capacity. We herein show that Al atoms diﬀuse with discontinuous jumps at the temperature range where anomalous heat capac- ity is observed. The diﬀusion path of Al atoms can be understood in terms of hyperatomic ﬂuctuations in six-dimensional space. Considering this atomic diﬀusion due to hyperatomic ﬂuctuations to be the “phason” dif- fusion, this diﬀusion can be understood as a diﬀusive Nambu-Goldsone mode. Based on this observation, we conclude that atomic diﬀusion due to hyperatomic ﬂuc- tuations in hyperdimensional space, which consists of a series of discontinuous atomic jumps, is the origin of the high-temperature anomalous heat capacity. HEAT CAPACITY: EXPERIMENT AND SIMULATION Experiment Fig. 1(a) shows the experimental results of the constant-volume heat capacity, CV , of Al–Pd–Ru icosa- hedral QC and its ACs. It can be seen thatCV largely de- viates from Dulon–Petit’s law, 3 kB with increasing tem- perature. The deviation from the 3kB is the largest for QCs, fol- lowed by the 2/1 AC and 1/0 AC. Thus, theCV anomaly of the ACs increases as the AC structure becomes similar to the QC. This systematical trend is consistent with previous work [11] that reported CV values for various aluminum– transition metal QCs and ACs. Simulation: machine-learning molecular dynamics The machine-learning molecular dynamics (MLMD) simulations of Al–Pd–Ru ACs are conducted under pe- riodic boundary conditions. We used a potential en- ergy function from an artiﬁcial neural network (ANN) that imitates the Born-Oppenheimer energies obtained by ﬁrst-principles density-functional theory (DFT). The ANN was trained by a self-learning hybrid Monte Carlo (SLHMC) method[23, 24] with a combination of PIMD[23, 25], the Vienna Ab initio Simulation Package ( VASP) [26, 27] and Atomic Energy Network ( aenet) [28] soft- ware. The computational details are provided in the Sup- porting Information. We show the heat capacity directly calculated by the ensemble average of the energy ﬂuctuation as CV = ⟨(∆E)2⟩/(kBT2). Figure 1(b) shows the calculated heat capacity for Al13Ru4, 1/0, 1/1, and 2/1 Al–Pd–Ru ACs. The simulation of Al 13Ru4 reproduces the conventional Dulong—Petit limit, 3 kB, as observed in experiments [13]. For ACs, we found that the calculated heat ca- pacity depends on temperature and becomes larger than the Dulong–Petit limit at high temperature. Here, the diﬀerence between 1/1 AC and 1/1 AC with 2 ×2 ×2 face-centered cubic primitive cells in Fig. 1(b) is the dif- ference of the number of atoms in a unit cell, 128 and 256, respectively. There is some system size dependence in the simulated results for the 1/1 AC case. To make a qualitative comparison with experiment, one needs to add more atoms in the unit cell, although such a compu- tation is too expensive at present. ATOMIC STRUCTURE AND PHASONS The local atomic structures of the QCs and ACs are similar, the main diﬀerence being how the cluster struc- tures are connected. Therefore, the cause of the anoma- lous heat capacity in QCs can be revealed by calculating the corresponding ACs. To understand high-temperature anomalous heat capacity, we introduce the crystal struc- ture of ACs in the Al–Pd–Ru system. Figure 2 shows the 1/0 AC, 1/1 AC, and 2/1 AC model structures generated by the modiﬁed Katz–Gratias– Boudard model of the Al–Pd–Ru system[29–31]. The structure of Al–Pd–Ru ACs is described as a dense packing of two types of clusters[32], the so-called mini- Bergman and pseudo-Mackay clusters shown in Fig. 2(d) and (e). These structures were obtained assuming the smallest size of the hyperatoms in six dimensions, called ”the occupation domain”. Note, however, that these3 400 600 800 1000 2 2.5 3 3.5 4 4.5 5  T (K) Cv /kB         calc. 2/1 1/1: 2 fcc 1/1 1/0 Ru4Al13 400 600 800 1000 2 3 4 5  T (K) Cv /kB  QC  2/1  1/0 (a) (b) FIG. 1. (Color online) Temperature dependence of heat capacity at constant volume normalized by Boltzmann constant CV /kB (a) in the experiment for Al–Pd–Ru QC (red), 2/1 AC (green), and 1/0 AC (blue), and (b) in the calculation for 2/1 AC (orange circle), 1/1 AC (blue triangle), 1/1 AC with 2 × 2 × 2 face-centered cubic primitive supercell (green triangle), 1/0 AC (purple square), and Ru 4Al13 (cross), where the thick light-colored line on each measured line stands for the standard deviation. model structures are not necessarily energetically favor- able and may not represent the actual structure of the AC at ﬁnite temperatures. The actual structural ensem- ble in thermal equilibrium can only be obtained after MD simulations. In the next section, we will show the results of the MD simulations starting from these model structures. MD ANALYSIS Atom jumping MD simulations of Al–Pd–Ru AC show several ener- getically favorable structures apart from the prototype shown in Fig. 2. Those structures are diﬀerent at the Al sites. It was found that some Al atoms are mobile in the ACs and randomly jump from one site to another dur- ing MD simulations in thermal equilibrium. This jump is detected at high temperatures where the heat capacity becomes anomalous. Figures 5(a)–(c) show the trajectories of some Al atoms for 1/1 AC at a temperature of 1000 K. Pd, Ru, and some Al atoms composing the inner shell of the mini- Bergman cluster are immobile and oscillate at their re- spective positions, as shown in Fig. 5(a). However, some other Al atoms composing the inner shell and edge of the pseudo-Mackay cluster are moving with almost dis- continuous jumps, as shown in Fig. 5(b)–(c). This may correspond to what was predicted as ”phason ﬂips” in the QC model system [21, 33], but detecting atomic jumps in real-time from MD simulations is a new ﬁnding. At 1000 K, the atomic jumps occur at about 100 ps, which is much longer than conventional phonon oscillations; an analysis of trajectories up to 2 ns suggests that the mov- ing Al atoms diﬀuse across the AC, as shown in the next subsection. Atom diﬀusion Figure 5(d) shows the diﬀusion coeﬃcientsD(T) of the Al atoms in 1/1 AC, which are obtained from the slope of the mean square displacement of the moving Al atoms; see Supplementary Material for the computational de- tails. By a linear ﬁtting of the Arrhenius plot of D(T), the free energy barrier of the Al jumps is estimated as ∆E ∼ 6000 K (∼ 0.52 eV), which is reachable at the high-temperature range exhibiting the anomalous heat capacity. Here it is worth noting that the diﬀusion of4 (a) 1/0 approximant(b) 1/1 approximant(c) 2/1 approximant (d) mini-Bergman cluster (mBC) (e) pseudo-Mackay cluster (pMC)1/0 1/1, 2/1or Al PdRu FIG. 2. (Color online) Atomic structures of Al–Pd–Ru ACs, (a): 1/0 AC, (b): 1/1 AC, (c): 2/1 AC, (d): mini-Bergman cluster and (e): the pseudo-Mackay cluster of each AC. Here, the mini-Bergman and pseudo-Mackay clusters have an inner shell. Al atoms in ACs (and presumably in QCs) occurs with- out any vacancy formation, as opposed to most diﬀusion mechanisms in perfect crystals. Figure 5(e) visualizes the trajectory of the Al diﬀusion. One can see that the diﬀu- sive paths are restricted within the locations at the edge of the mini-Bergman and pseudo-Mackay clusters shown in Figure 2. DISCUSSIONS Origin of Al diﬀusion How can we understand the diﬀusion pathways? As mentioned earlier, the static conﬁgurations of the QCs and ACs correspond to the projection of hyperatoms on a periodic lattice in a higher dimensional space. Fig- ures 4(a) and 4(b) show the positions of the hyperatomic sites and atoms in the real space of the Al–Pd–Ru icosa- hedrons QC and AC, respectively. Here, the hyperatomic sites of the Pd and Ru atoms are located inside the hyper- atomic shell, while the Al atoms are located outside. The hyperatomic sites shown in Figure 4(a) are schematic. In this case, the QC and AC static conﬁgurations corre- spond to the most compactly occupied hyperatomic sites in six-dimensional space. However, there are many other accessible hyperatomic Al sites outside the shell. The ac- cessibility of each of these sites is energy-dependent and can only be determined after MD simulations. The move- ment between the diﬀerent hyperatomic sites corresponds to a phason ﬂip, and a series of phason ﬂips were ob- served at ﬁnite temperatures in the MD simulations. At higher temperatures, the occupancy of Al hyperatomic sites beyond the outer shell increases, resulting in the Al coordination shown in Figure 4(b). This corresponds ex- actly to the diﬀusion path of Al atoms as found in the MD simulation shown in Figure 5(f). Therefore, the Al diﬀusion in Al–Pd–Ru QC and AC can be regarded as a six-dimensional hyperatomic ﬂuctuation. Origin of anomalous heat capacity Why does the heat capacity become anomalous in the presence of phason ﬂips? The heat capacity follows the Dulong–Petit rule assuming harmonic vibrations of con- ventional phonons. The anharmonicity of the potential5 -25 -20 -15 -10 -5  0  5  10  15  800  1000 1200 1400 1600 1800 2000 position [Angstrom] t [ps] Al-89,xAl-89,yAl-89,z -30 -25 -20 -15 -10 -5  0  5  10  800  1000 1200 1400 1600 1800 2000 position [Angstrom] t [ps] Al-88,xAl-88,yAl-88,z-10 -5  0  5  10  800  1000 1200 1400 1600 1800 2000 position [Angstrom] t [ps] Al-56,xAl-56,yAl-56,z -0.001  0  0.001  0.002  0.003  0.004  0.005  0.006  0.007  0.008  0.009  0.01  400  500  600  700  800  900  1000 D(T) T [K] AlRuPd Al88 Al89 Al56 Al RuPd Al (high-T) (a) (b) (c) (d) (e) (f) FIG. 3. (a)–(c) Real-time dynamics of the Al atoms in the 1/1 AC Al 92Pd20Ru16 at 1000 K. (d) Temperature depen- dence of the diﬀusion coeﬃcient. Visualization of the diﬀusive Al atoms in (e) the initial atomic structure and (f) atoms gen- erated by the hyperatom in the six-dimensional space. Here, the yellow isosurface and the blue and red polyhedra repre- sent the three-dimensional distribution of the Al atoms for 1/1 AC at T = 1000 K from t = 800 ps to 2000 ps, and the inner shell of the mini-Bergman, and pseudo-Mackay clusters, respectively. energy surface is the origin of the anomaly. According to QC hydrodynamics, phason ﬂips correspond to diﬀu- sive Nambu–Goldstone modes[16, 34]. Their dispersion relation is expressed as ω= −iD(T)q2 + ··· , (1) where q denotes the momentum of the phason mode. In this case, the imaginary frequency of the diﬀusive mode, iω, is proportional to the diﬀusion coeﬃcient D(T). The magnitude of the heat capacity is determined by how much energy a material absorbs when heat is applied from the outside. Because the applied heat is partially used to excite the diﬀusive Nambu–Goldstone mode, the heat capacity becomes larger with increasing tempera- ture. Al Pd, Ru thermally fluctuated  Al sites (a) (b) Temperature fluctuation Temperature fluctuation3D space atomic sites 6D space hyper-atomic sites  Al Pd  thermally fluctuated  Al sites FIG. 4. (a) Schematic ﬁgure of the eﬀect of temperature ﬂuctuation of the hyperatom in six-dimensional space. Al atoms are located around the surface. (b) Schematic ﬁgure of the eﬀect of a temperature ﬂuctuation of the corresponding atoms composing the inner shell of the pseudo-Mackay cluster in three-dimensional space. SUMMARY The anomalous heat capacity observed in icosahedral AC in the Al–Pd–Ru system was well reproduced by machine-learning MD simulations. Some of the Al atoms were found to diﬀuse through the crystal in the absence of vacancies via nearly discontinuous jumps that can be regarded as phason ﬂips. The restricted pathways of the phason ﬂips can be understood as thermal ﬂuctuations of hyperatoms in six-dimensional space. From this re- sult, we conclude that anomalous heat capacity is caused by atomic diﬀusion due to thermal ﬂuctuations of hyper- atoms in the higher-dimensional space in QCs and ACs. This study suggests that the high dimensionality of QC structure may aﬀect physical properties other than heat capacity. Acknowledgements Y.N. was partially supported by JSPS KAKENHI Grant Numbers 18K11345, 20H05278, 22H04602, and 22K03539. The calculations were performed using the su- percomputing system HPE SGI8600 at the Japan Atomic Energy Agency. Y.I. was supported by JSPS KAK- ENHI under Grant 19H05818, 19H05821, and 19J21779. M.S. was supported by JSPS KAKENHI (18H05519, 21H01603) and the Fugaku Battery & Fuel Cell Project. We thank Edanz (https://jp.edanz.com/ac) for editing a draft of this manuscript. Y.N. and Y.I. contributed equally to this work.6 [1] D. Shechtman, I. Blech, D. Gratias, and J. W. Cahn, Metallic phase with long-range orientational order and no translational symmetry, Phys. Rev. Lett.53, 1951 (1984). [2] D. Levine and P. J. Steinhardt, Quasicrystals: A new class of ordered structures, Phys. Rev. Lett. 53, 2477 (1984). [3] A. Yamamoto, Crystallography of Quasiperiodic Crys- tals, Acta Crystallographica Section A 52, 509 (1996). [4] M. Boudard, M. de Boissieu, C. Janot, G. Heger, C. Beeli, H. U. Nissen, H. Vincent, R. Ibberson, M. Au- dier, and J. M. Dubois, Neutron and x-ray single- crystal study of the alpdmn icosahedral phase, Journal of Physics: Condensed Matter 4, 10149 (1992). [5] H. Takakura, C. P. G´ omez, A. Yamamoto, M. De Boissieu, and A. P. Tsai, Atomic structure of the binary icosahedral yb–cd quasicrystal, Nature Materials 6, 58 (2007). [6] K. Deguchi, S. Matsukawa, N. K. Sato, T. Hattori, K. Ishida, H. Takakura, and T. Ishimasa, Quantum crit- ical state in a magnetic quasicrystal, Nature Materials 11, 1013 (2012). [7] K. Kamiya, T. Takeuchi, N. Kabeya, N. Wada, T. Ishi- masa, A. Ochiai, K. Deguchi, K. Imura, and N. K. Sato, Discovery of superconductivity in quasicrystal, Nature Communications 9, 154 (2018). [8] R. Tamura, A. Ishikawa, S. Suzuki, T. Kotajima, Y. Tanaka, T. Seki, N. Shibata, T. Yamada, T. Fujii, C.-W. Wang, M. Avdeev, K. Nawa, D. Okuyama, and T. J. Sato, Experimental observation of long-range mag- netic order in icosahedral quasicrystals, Journal of the American Chemical Society 143, 19938 (2021), pMID: 34786934, https://doi.org/10.1021/jacs.1c09954. [9] K. Edagawa and K. Kajiyama, High temperature speciﬁc heat of al–pd–mn and al–cu–co quasicrystals, Materials Science and Engineering: A 294-296, 646 (2000). [10] K. Edagawa, K. Kajiyama, R. Tamura, and S. Takeuchi, High-temperature speciﬁc heat of quasicrystals and a crystal approximant, Materials Science and Engineering: A 312, 293 (2001). [11] K. Fukushima, H. Suyama, Y. Tokumoto, Y. Kamimura, Y. Takagiwa, and K. Edagawa, Comparative study of high-temperature speciﬁc heat for al–pd–mn icosahe- dral quasicrystals and crystal approximants, Journal of Physics Communications 5, 085002 (2021). [12] A. F. Prekul, V. A. Kazantsev, N. I. Shchegolikhina, R. I. Gulyaeva, and K. Edagawa, High-temperature heat ca- pacity of the al63cu25fe12 quasicrystal, Physics of the Solid State 50, 2013 (2008). [13] S. Tamura, K. Fukushima, Y. Tokumoto, Y. Takagiwa, and K. Edagawa, High-temperature speciﬁc heat of al–cu–ru icosahedral quasicrystals and 1/1 crystal ap- proximants, MATERIALS TRANSACTIONS 62, 356 (2021). [14] D. Levine, T. C. Lubensky, S. Ostlund, S. Ramaswamy, P. J. Steinhardt, and J. Toner, Elasticity and dislocations in pentagonal and icosahedral quasicrystals, Phys. Rev. Lett. 54, 1520 (1985). [15] J. E. S. Socolar, T. C. Lubensky, and P. J. Stein- hardt, Phonons, phasons, and dislocations in quasicrys- tals, Phys. Rev. B 34, 3345 (1986). [16] M. Baggioli and M. Landry, Eﬀective ﬁeld theory for qua- sicrystals and phasons dynamics, SciPost Phys. 9, 062 (2020). [17] G. Coddens, S. Lyonnard, B. Hennion, and Y. Calvayrac, Triple-axis neutron-scattering study of phason dynamics in al-mn-pd quasicrystals, Phys. Rev. B 62, 6268 (2000). [18] S. Francoual, F. Livet, M. de Boissieu, F. Yakhou, F. Bley, A. L´ etoublon, R. Caudron, and J. Gastaldi, Dynamics of phason ﬂuctuations in the i−AlPdMn qua- sicrystal, Phys. Rev. Lett. 91, 225501 (2003). [19] E. Abe, S. J. Pennycook, and A. P. Tsai, Direct observa- tion of a local thermal vibration anomaly in a quasicrys- tal, Nature 421, 347 (2003). [20] M. Engel, S. Sonntag, H. Lipp, and H.-R. Trebin, Struc- ture factors of harmonic and anharmonic ﬁbonacci chains by molecular dynamics simulations, Phys. Rev. B 75, 144203 (2007). [21] M. Engel, M. Umezaki, H.-R. Trebin, and T. Odagaki, Dynamics of particle ﬂips in two-dimensional quasicrys- tals, Phys. Rev. B 82, 134206 (2010). [22] M. Mihalkoviˇ c and M. Widom, Spontaneous formation of thermodynamically stable al-cu-fe icosahedral quasicrys- tal from realistic atomistic simulations, Phys. Rev. Res. 2, 013196 (2020). [23] Y. Nagai, M. Okumura, K. Kobayashi, and M. Shiga, Self-learning hybrid monte carlo: A ﬁrst-principles ap- proach, Phys. Rev. B 102, 041124 (2020). [24] K. Kobayashi, Y. Nagai, M. Itakura, and M. Shiga, Self-learning hybrid monte carlo method for isother- mal–isobaric ensemble: Application to liquid silica, The Journal of Chemical Physics 155, 034106 (2021), https://doi.org/10.1063/5.0055341. [25] M. Shiga, Pimd: An open-source software for paral- lel molecular simulations, https://ccse.jaea.go.jp/ software/PIMD/index.en.html. [26] G. Kresse and J. Hafner, Ab initio molecular dynamics for liquid metals, Phys. Rev. B 47, 558 (1993). [27] G. Kresse and J. Furthm¨ uller, Eﬃcient iterative schemes for ab initio total-energy calculations using a plane-wave basis set, Phys. Rev. B 54, 11169 (1996). [28] N. Artrith and A. Urban, An implementation of artiﬁcial neural-network potentials for atomistic materials simu- lations: Performance for tio2, Computational Materials Science 114, 135 (2016). [29] M. Krajˇ c´ ı, M. Windisch, J. Hafner, G. Kresse, and M. Mi- halkoviˇ c, Atomic and electronic structure of icosahedral al-pd-mn alloys and approximant phases, Phys. Rev. B 51, 17355 (1995). [30] M. Krajˇ c´ ı and J. Hafner, Semiconducting al–transition- metal quasicrystals, Phys. Rev. B 68, 165202 (2003). [31] M. Krajˇ c´ ı and J. Hafner, Topologically induced semicon- ductivity in icosahedral al-pd-re and its approximants, Phys. Rev. B 75, 024116 (2007). [32] N. Fujita, H. Takano, A. Yamamoto, and A.-P. Tsai, Cluster-packing geometry for Al-based F-type icosahe- dral alloys, Acta Crystallographica Section A 69, 322 (2013). [33] M. Widom, Discussion of phasons in quasicrystals and their dynamics, Philosophical Magazine 88, 2339 (2008), https://doi.org/10.1080/14786430802247163. [34] M. Baggioli, Homogeneous holographic viscoelastic mod- els and quasicrystals, Phys. Rev. Res. 2, 022022 (2020).7 [35] K. Tanaka, Y. Mitara, and M. Koiwa, Elas- tic constants of al-based icosahedral quasicrys- tals, Philosophical Magazine A 73, 1715 (1996), https://doi.org/10.1080/01418619608243008. [36] J. B. Wachtman, W. E. Teﬀt, D. G. Lam, and C. S. Apstein, Exponential temperature dependence of young’s modulus for several oxides, Phys. Rev. 122, 1754 (1961). [37] J. P. Perdew, K. Burke, and M. Ernzerhof, Generalized gradient approximation made simple, Phys. Rev. Lett. 77, 3865 (1996). [38] P. E. Bl¨ ochl, Projector augmented-wave method, Phys. Rev. B 50, 17953 (1994). [39] G. Kresse and D. Joubert, From ultrasoft pseudopoten- tials to the projector augmented-wave method, Phys. Rev. B 59, 1758 (1999). [40] N. Artrith, A. Urban, and G. Ceder, Eﬃcient and accu- rate machine-learning interpolation of atomic energies in compositions with many species, Phys. Rev. B96, 014112 (2017). [41] The ANN ﬁngerprint parameters were set as radial Rc = 8.0, radial N = 16, angular Rc = 6.5, and angular N = 4 for all the atomic elements. We used two hidden layers with 15 units and the tanh-type activation function. SUPPLEMENTAL MATERIALS Methods Sample preparation Synthesis of Al–Pd–Ru QC, 2/1 AC, 1/0 AC was per- formed from commercial element powders, Al (3N purity; Kojundo Chemical Laboratory Co., Ltd., Japan), Pd (3N purity; TANAKA Kikinzoku Kogyo K.K, Japan), and Ru (3 N purity; TANAKA Kikinzoku Kogyo K.K, Japan) with nominal compositions of Al71.5Pd19Ru9.5 for QC, Al71Pd20Ru9 for 2/1 AC, Al 68Pd20.5Ru11.5 for 1/0 AC, respectively. The mixed powders were pressed un- der 200 MPa into disc-shaped pellets. Each pellet was melted in an arc-melting furnace under an argon atmo- sphere. The ingots wrapped in tantalum foil were an- nealed at 1173 K for 72 h under argon atmosphere. The annealed ingots were reground into powder and then sin- tered by spark plasma sintering. The temperatures of the specimens were increased from room temperature to 1173 K, and then the samples were kept for 20 min at a uniaxial pressure of 90 MPa under an argon atmosphere. The sintered samples were annealed again under the same conditions as above. Characterization Phase identiﬁcation and determination of lattice con- stants were performed by the powder X-ray diﬀraction (XRD) method with Cu K–L 2,3 radiation (MiniFlex; Rigaku Co., Japan). Each XRD pattern is shown in the supplemental materials. The longitudinal and transverse speeds of sound vl and vt were measured by the ultra- sonic pulse-echo method using an echometer (Echometer 1062; Karl Deutsch Co., Germany). The heat capacity at constant pressure Cp was measured by diﬀerential scan- ning calorimetry using a diﬀerential scanning calorimeter (DSC 404 F1 Pegasus; NETZSCH Japan K.K.). Physical properties analysis CV /kB is converted by using the following thermody- namic relation: CV = (1 + 9VBSα2TC−1 p )−1Cp (2) where V, BS, and α are the average volume per atom, the adiabatic bulk modulus, and the linear thermal ex- pansion coeﬃcient, respectively. BS(300 K) is calculated by using the density ρ, vl and vt as BS = ρ(v2 l −4 3 v2 t ). BS(300 K)/BS(T) where T > 300 K for Al–Pd–Ru QC and the ACs are assumed to be the same as that of Al– Pd–Mn QC measured by Tanaka et al.[35]. Here, BS(T) is ﬁtted to the functionBS = B0−ATexp (−T0 T )[36] with B0 = 127 GPa, A = 0.044 GPa/K and T0 = 486 K. α is8 extracted from the previously reported value for Al–Pd– Mn QC measured by Fukushima et al. [11]. We note that the trend of α is almost independent of chemical composition and the degree of approximation according to previous work [10]. The maximum diﬀerence between Cp/kB and CV /kB is up to 0.3kB at T = 1050 K in QCs. Simulation setups We consider a 2 ×2 ×2 supercell of the 1/0 AC Al23Pd2Ru6, the 1/1 AC Al92Pd20Ru16, and the 2/1 AC Al396Pd80Ru68. We also consider Al 13Ru4 as a refer- ence material where the heat capacity is similar to the Dulong–Petit limit and has weak temperature depen- dence. We construct artiﬁcial neural networks (ANNs) to reproduce the energy calculated by the DFT calcu- lations. The DFT-MD, ML-MD and SLHMC were im- plemented in PIMD software [23–25], which supports the interface of both the Vienna Ab initio Simulation Package (VASP) [26, 27] and Atomic Energy Network (aenet) soft- ware [28]. The DFT calculations were conducted using the Perdew–Burke–Ernzerhof functional [37]. The pro- jector augmented wave method [38, 39] was employed, while the cutoﬀ energy was 500 eV and the sampling point was only the Γ point. In aenet, we adopt the Chebyshev basis set as a descriptor for atomic environ- ments [40], and the corresponding parameters are given in Ref. 41. SLHMC is used for improving the ANN po- tentials [23, 24]. The heat capacity is calculated by the NVT ensemble average of the energy ﬂuctuation. The massive Nos´ e–Hoover chain method is used for generat- ing the NVT ensemble for heat capacity calculations. In calculations of the mean-square displacement and acti- vation energy, the Nos´ e–Hoover thermostat is used to study real-time dynamics. Using the 1 fs time step, we do simulations with up to 2 ×106 fs. The quality of the ANN potentials is conﬁrmed with SLHMC calculations (see supplemental materials) because the SLHMC gen- erates the same ensemble as that generated by the ﬁrst- principle MD simulation [23]. Quality of the machine-learning potential The quality of the machine-learning potential can be checked with the self-learning hybrid Monte Carlo method (SLHMC). The ensemble generated by the SLHMC is equivalent to that of the DFT calculations. As shown in Fig. 5, the radial distribution functions ob- tained in the MLMD simulations are similar to that ob- tained in the SLHMC simulations. This ensures that our machine learning potential performs close to the accuracy of DFT and is suﬃcient to study the physical properties of ACs. Mean square displacement The mean square displacement is deﬁned as MSD(t) = 1 N N∑ j=1 φj(rj(t)), (3) where φj(rj(t)) ≡⟨[rj(t) −rj(0)]2⟩. (4) We call φj(rj(t)) the MSD for j-th atom. As shown in Fig. 6, some of Al atoms become diﬀusive with in- creasing the temperature. We calculate the temper- ature dependence of the diﬀusion coeﬃcient D(T) ≡ (1/6) limt→∞MSD(t)/t. We can ﬁt the diﬀusion coef- ﬁcient by an Arrhenius law, D(T) = D0 exp (−∆E/kBT) . (5) X-RAY DIFFRACTION PATTERNS Figure 7 shows the experimental X-ray diﬀraction pat- terns of Al-Ru-Pd quasicrystal, 2/1 approximant, and 1/0 approximant.9 (a)  0  0.5  1  1.5  2  2.5  3  3.5  0  2  4  6  8  10 r [angstrom] 1000K,SLHMC 1000K,MLMD Radial distribution function Al-Al (b)  0  1  2  3  4  5  6  0  2  4  6  8  10 r [angstrom] 1000K,SLHMC 1000K,MLMD Radial distribution function Pd-Al (c)  0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  0  2  4  6  8  10 r [angstrom] 1000K,SLHMC 1000K,MLMD Radial distribution function Pd-Pd (d)  0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5  0  2  4  6  8  10 r [angstrom] 1000K,SLHMC 1000K,MLMD Radial distribution function Pd-Ru (e)  0  1  2  3  4  5  6  7  0  2  4  6  8  10 r [angstrom] 1000K,SLHMC 1000K,MLMD Radial distribution function Ru-Al (f)  0  1  2  3  4  5  6  0  2  4  6  8  10 r [angstrom] 1000K,SLHMC 1000K,MLMD Radial distribution function Ru-Ru FIG. 5. Radial distribution functions for 1/1 approximant.10  0  2  4  6  8  10  0  10  20  30  40  50 (a) 59-th atom, Al-22  0  0.2  0.4  0.6  0.8  1  0  10  20  30  40  50 (b) 92-th atom, Al-56  0  2  4  6  8  10  0  10  20  30  40  50 t[ps] 400K 500K 600K 700K 750K 850K 950K 1000K 1100K (c) 126-th atom, Al-90 FIG. 6. (Color online) Mean square displacement of (a) Al-22, (b) Al-56, and (c) Al-90 atoms for 1/1 approximant. 20 30 40 50 60 70 80 QC 2/1 1/0 2 (degree) Al68Pd20.5Ru11.5 Al71Pd20Ru9 Al71.5Pd19Ru9.5 Intensity(arb. unit) FIG. 7. (Color online) The X-ray diﬀraction patterns of Al- Ru-Pd QC (RED), 2/1 AC (GREEN), and 1/0 AC (BLUE), respectively.",
      "references": [
        "Metallic phase with long-range orientational order and no translational symmetry",
        "Quasicrystals: A new class of ordered structures",
        "Crystallography of Quasiperiodic Crys- tals",
        "Neutron and x-ray single- crystal study of the alpdmn icosahedral phase",
        "Atomic structure of the binary icosahedral yb–cd quasicrystal",
        "Quantum crit- ical state in a magnetic quasicrystal",
        "Discovery of superconductivity in quasicrystal",
        "Experimental observation of long-range mag- netic order in icosahedral quasicrystals",
        "High temperature speciﬁc heat of al–pd–mn and al–cu–co quasicrystals",
        "High-temperature speciﬁc heat of quasicrystals and a crystal approximant",
        "Comparative study of high-temperature speciﬁc heat for al–pd–mn icosahe- dral quasicrystals and crystal approximants",
        "High-temperature heat ca- pacity of the al63cu25fe12 quasicrystal",
        "High-temperature speciﬁc heat of al–cu–ru icosahedral quasicrystals and 1/1 crystal ap- proximants",
        "Elasticity and dislocations in pentagonal and icosahedral quasicrystals",
        "Phonons, phasons, and dislocations in quasicrys- tals",
        "Eﬀective ﬁeld theory for qua- sicrystals and phasons dynamics",
        "Triple-axis neutron-scattering study of phason dynamics in al-mn-pd quasicrystals",
        "Dynamics of phason ﬂuctuations in the i−AlPdMn qua- sicrystal",
        "Direct observa- tion of a local thermal vibration anomaly in a quasicrys- tal",
        "Struc- ture factors of harmonic and anharmonic ﬁbonacci chains by molecular dynamics simulations",
        "Dynamics of particle ﬂips in two-dimensional quasicrys- tals",
        "Spontaneous formation of thermodynamically stable al-cu-fe icosahedral quasicrys- tal from realistic atomistic simulations",
        "Self-learning hybrid monte carlo: A ﬁrst-principles ap- proach",
        "Self-learning hybrid monte carlo method for isother- mal–isobaric ensemble: Application to liquid silica",
        "Pimd: An open-source software for paral- lel molecular simulations",
        "Ab initio molecular dynamics for liquid metals",
        "Eﬃcient iterative schemes for ab initio total-energy calculations using a plane-wave basis set",
        "An implementation of artiﬁcial neural-network potentials for atomistic materials simu- lations: Performance for tio2",
        "Atomic and electronic structure of icosahedral al-pd-mn alloys and approximant phases",
        "Semiconducting al–transition- metal quasicrystals",
        "Topologically induced semicon- ductivity in icosahedral al-pd-re and its approximants",
        "Cluster-packing geometry for Al-based F-type icosahe- dral alloys",
        "Discussion of phasons in quasicrystals and their dynamics",
        "Homogeneous holographic viscoelastic mod- els and quasicrystals",
        "Elas- tic constants of al-based icosahedral quasicrys- tals",
        "Exponential temperature dependence of young’s modulus for several oxides",
        "Generalized gradient approximation made simple",
        "Projector augmented-wave method",
        "From ultrasoft pseudopoten- tials to the projector augmented-wave method",
        "Eﬃcient and accu- rate machine-learning interpolation of atomic energies in compositions with many species"
      ],
      "meta_data": {
        "arxiv_id": "2302.14441v1",
        "doi": "10.1103/PhysRevLett.132.196301",
        "authors": [
          "Yuki Nagai",
          "Yutaka Iwasaki",
          "Koichi Kitahara",
          "Yoshiki Takagiwa",
          "Kaoru Kimura",
          "Motoyuki Shiga"
        ],
        "published_date": "2023-02-28T09:39:25Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the long-standing origin of anomalously large high-temperature heat capacity (Cv exceeding the Dulong–Petit limit) in realistic quasicrystals. Using the Al–Pd–Ru icosahedral quasicrystal (QC) and a series of periodic approximant crystals (ACs), the study links the Cv anomaly to aluminum atomic diffusion that occurs via nearly discontinuous, vacancy-free jumps (“phason flips”). These jumps are interpreted as transitions between accessible hyperatomic sites in the 6D hyperlattice description, i.e., thermal hyperatomic fluctuations. The work concludes that these hyperatomic-fluctuation-driven phason flips correspond to diffusive Nambu–Goldstone modes that provide extra effective degrees of freedom and thereby increase heat capacity at high temperatures. Experimentally, the Cv anomaly strengthens systematically as ACs approach the QC limit (QC > 2/1 AC > 1/0 AC), and simulations qualitatively reproduce this trend and reveal the microscopic jump mechanism.",
        "methodology": "Combines (i) high-temperature calorimetry and elastic/thermal-expansion-based thermodynamic conversion to obtain constant-volume heat capacity, and (ii) large-scale machine-learning molecular dynamics (MLMD) to compute Cv from energy fluctuations and to resolve atomic trajectories. The ML potential is an artificial neural network (ANN) trained to reproduce DFT Born–Oppenheimer energies using a self-learning hybrid Monte Carlo (SLHMC) loop coupled to DFT (VASP) and aenet descriptors (Chebyshev/basis fingerprints with specified cutoffs and network architecture). MLMD is run in NVT with Nosé–Hoover chains; Cv is computed as Cv = <(ΔE)^2>/(kB T^2). Atomic diffusion is quantified via mean-square displacement and Arrhenius fits to diffusion coefficients to extract activation barriers. Structural interpretation uses the 6D hyperlattice/hyperatom (occupation domain) framework to map observed 3D jump pathways to hyperatomic-site fluctuations and to connect dynamics to diffusive phason (Nambu–Goldstone) modes with ω = −i D(T) q^2 + … .",
        "experimental_setup": "Materials: synthesized Al–Pd–Ru icosahedral QC and ACs (2/1 and 1/0) with nominal compositions Al71.5Pd19Ru9.5 (QC), Al71Pd20Ru9 (2/1 AC), Al68Pd20.5Ru11.5 (1/0 AC). Preparation: powder mixing, pressing (200 MPa), arc melting under Ar, annealing at 1173 K for 72 h (Ta-foil wrapped), regrinding, spark plasma sintering to 1173 K (20 min, 90 MPa, Ar), followed by re-anneal. Characterization: powder XRD (Cu Kα) for phase identification and lattice constants; ultrasonic pulse-echo to measure longitudinal/transverse sound velocities to infer adiabatic bulk modulus; DSC to measure Cp(T). Conversion to Cv(T): uses Cv = (1 + 9 V Bs α^2 T / Cp)^−1 Cp with V from composition/density, Bs from sound velocities; assumes Bs(T) scaling and α(T) from literature for Al–Pd–Mn QC (composition/approximant-insensitive assumption). Simulation: periodic-boundary MLMD on supercells—1/0 AC Al23Pd2Ru6 (2×2×2), 1/1 AC Al92Pd20Ru16 (and size-variant cells), 2/1 AC Al396Pd80Ru68 (2×2×2), plus reference Ru4Al13. DFT settings for training: PBE, PAW, 500 eV cutoff, Γ-point sampling. Validation: compares radial distribution functions from MLMD vs SLHMC at 1000 K to ensure near-DFT ensemble fidelity; detects jump events in real-time trajectories; computes diffusion coefficients from MSD and fits Arrhenius law to estimate barriers (~0.52 eV).",
        "limitations": "Quantitative agreement with experimental Cv is limited: simulations are stated to be qualitative, with noticeable system-size dependence (e.g., 1/1 AC with 128 vs 256 atoms per cell). Direct QC simulations are not performed; conclusions about QCs rely on AC behavior and 6D interpretation. Initial structural models (modified Katz–Gratias–Boudard) are acknowledged as not necessarily energetically favorable at finite T; equilibrium structures emerge only after MD. Experimental Cv extraction depends on assumptions/inputs borrowed from Al–Pd–Mn (temperature dependence of Bs and thermal expansion α), which may introduce systematic uncertainty for Al–Pd–Ru. Phason/Nambu–Goldstone interpretation is largely phenomenological: the work does not compute phason dispersion, mode densities, or directly decompose Cv contributions into phonon vs phason components. Time scales accessible to MD (up to ~2 ns) may miss rarer events or long-time collective dynamics; diffusion analysis focuses on a subset of mobile Al atoms.",
        "future_research_directions": "(1) Scale up MLMD/accelerated dynamics to larger approximants and, ultimately, true QC models to reduce finite-size effects and enable quantitative Cv comparisons. (2) Compute phason spectra/transport explicitly—e.g., extract D(T) and q-dependent relaxation from dynamic structure factors S(q,ω) and compare with neutron/X-ray scattering. (3) Separate thermodynamic contributions: develop models or constrained simulations to partition Cv into phonon, anharmonic, and phason (diffusive NG) components and link to hyperlattice degrees of freedom. (4) Improve experimental Cv determination using direct measurements of α(T) and Bs(T) for Al–Pd–Ru QC/ACs rather than borrowing from Al–Pd–Mn. (5) Map hyperatomic-site energetics: quantify free-energy landscapes in perpendicular space, occupation-domain growth with T, and correlate with activation barriers and percolation of diffusion pathways. (6) Explore generality across compositions and symmetries (other icosahedral/decagonal QCs) and assess impacts on other properties (thermal conductivity, mechanical damping, creep, diffusion-mediated phase stability). (7) Investigate coupling between phason flips and electronic/elastic responses, including how hyperatomic fluctuations affect elastic constants and potential functional properties.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
      "full_text": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time Zichang Liu Department of Computer Science Rice University Houston, TX 77005 zichangliu@rice.edu Aditya Desai Department of Computer Science Rice University Houston, TX 77005 Aditya.P.Desai@rice.edu Fangshuo Liao Department of Computer Science Rice University Houston, TX 77005 Fangshuo.Liao@rice.edu Weitao Wang Department of Computer Science Rice University Houston, TX 77005 wtwang@rice.edu Victor Xie Department of Computer Science Rice University Houston, TX 77005 vyx2@rice.edu Zhaozhuo Xu Department of Computer Science Rice University Houston, TX 77005 zhaozhuoxu@gmail.com Anastasios Kyrillidis Department of Computer Science Rice University Houston, TX 77005 anastasios@rice.edu Anshumali Shrivastava Department of Computer Science Rice University Houston, TX 77025 anshumali@rice.edu Abstract Large language models(LLMs) have sparked a new wave of exciting AI applica- tions. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose SCIS - SORHANDS , a system that maintains the memory usage of KV cache under a fixed budget without finetuning the model. We validate that SCISSORHANDS reduces the inference memory usage of the KV cache by up to 5× without compromising model quality. We further demonstrate that SCISSORHANDS can be combined with 4-bit quantization for further compression Preprint. Under review. arXiv:2305.17118v2  [cs.LG]  28 Aug 20231 Introduction Large language models(LLMs), trained on immense amounts of text data, have demonstrated an incredible ability to generate text that is both logically connected and contextually relevant [ 1–5]. LLM inference follows an autoregressive fashion, generating one token at each step conditioned on the previous steps. At each step, the key-value embedding in attention is stored in memory to avoid repetitive key-value projection computation at future steps. Unfortunately, the memory of the key-value cache( KV cache), including prompts and previously generated tokens, can be surprisingly large. Using OPT-175B as an example, the impressive 175 billion parameters consume around 325 GB of memory. At the same time, at batch size 128 and sequence length 2048, the KV cache requires around 950 GB of memory, three times larger than the model weights. Considering that 8 Nvidia A100-80GB offers 640GB GPU memory, the memory usage of the KV cache is truly concerning. LLMs are typically deployed on fixed memory hardware, and the size of model weights is also fixed once deployed. Apart from a small memory buffer typically reserved for communication and computation, the rest of the available memory is for the KV cache. The size of the KV cache depends on batch size, sequence length, and model dimension. Thus, at a given inference sequence length, compression in the KV cache memory translates almost linearly into an increase in the batch size. And any increase in batch size is significant for high-throughput inference systems [6, 7]. 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (a) Attention map at position 178 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (b) Attention map at position 228 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (c) Attention map at position 278 Figure 1: Repetitive Attention Pattern. We plot the attention map at three token positions in a sentence. Only five attention heads are plotted for a clearer presentation. We discretize the attention score such that the high score is dark green, and the low score is light green. In Figure 1(a), the token at position 178 pays heavy attention to positions 27, 63, 98, etc. This pattern is also present in the attention maps of position 228 and position 278. Quantization and sparsity ap- proaches [8–14] have been stud- ied in LLMs to reduce the model sizes. However, compressing the KV cache remains an open but challenging problem. First, training models at the scale of hundreds of billions of parame- ters on a large amount of data is prohibitively expensive. Thus, an ideal compression algorithm should be applicable without training. Second, emerging ap- plications such as dialogue sys- tems require an extremely long context window. The maximum sequence length of LLMs is grow- ing to over 32K [15]. The size of the KV cache also grows linearly with sequence length. For scala- bility, an ideal compression algo- rithm should reduce the memory from the sequence length dimen- sion. At last, compression should preserve LLMs’ quality and in- context learning ability. We go beyond the traditional model compression techniques to achieve such demanding requirements. We envision that not all tokens must be stored in memory for LLM to understand the context. Just like humans can skim through an article and grasp the main idea, LLMs may also be able to skim and comprehend. It is commonly observed that the attention score from one token follows a strong power law distribution [16–20], meaning that one token will only heavily attend to a small number of tokens. More importantly, we observe Repetitive Attention Pattern from different tokens in the sequence in a trained LLM( Figure 1). Certain tokens are more important throughout the paragraph. Specifically, for two different tokens, there are similarities between who they are heavily attending to and similarities between who they are ignoring. Inspired by the above observation, we articulate the Persistence of Importance Hypothesis: Only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step. This hypothesis, if true, suggests that it is possible to foresee which token is likely to be important for future generations. Fortunately, we empirically verify that later tokens in the 2sentence mostly only attend to tokens that were heavily attended from the early tokens in a sentence. And the overlapping ratio is surprisingly high, over 90% in most of the transformer layers (Figure 2). Based on the above two findings, we present SCISSORHANDS that exploits persistence of importance hypothesis to realize LLM inference with a compressed KV cache. In Section 4, we present an efficient algorithm such that the size of KV cache is always less than a predetermined budget. And a theoretical guarantee justifies that such a compressed KV cache can approximate the attention output. In Section 5, we empirically evaluateSCISSORHANDS and show that SCISSORHANDS reduces the memory usage of KV cache up to 5× without degradation on model quality. Reduction in the KV cache can directly result in a larger batch size. Further, we adopt quantization and show its compatibility with SCISSORHANDS . 2 Problem Description and Related Work This paper considers the LLM inference workflow, specifically focusing on the memory usage for storing the keys and values in attention. Let d be the hidden dimension of the model, b be the batch size, and s be the length of prompt sentences. We are given the trained model weights, Wi K ∈ Rd×d, Wi V ∈ Rd×d for the key and value projection matrix at the ith transformer layer. The standard LLM inference consists of two stages: prompting and token generation. In the prompting stage, the model takes the prompt sentences as the input, and the key/value embedding in attention is stored as a cache to reduce repetitive computation. Denote xi prompt = [xi 1, ..., xi p], xi prompt ∈ Rb×p×d as the input to attention at the ith transformer layer. Denote the key cache and value cache at layer i as Ki, Vi ∈ Rb×p×d, Ki 0 = xi promptWi K, Vi 0 = xi promptWi V . In the generation stage, the model starts with the stored KV cache in the prompting stage and generates one token at each step. At each step, the KV cache gets updated. Given the input to attention at step t in the ith transformer layer xi t ∈ Rb×1×d. Ki t+1 = [Ki t, xi tWi K], Vi t+1 = [Vi t, xi tWi V ]. 2.1 LLM Inference Memory Breakdown In this section, we provide the memory consumption breakdown of LLMs. The memory footprint consists of three parts: model weights, KV cache, and activation buffer. The size of model weights depends on model configuration, such as the number of transformer layers and hidden size. The size of the KV cache depends on model configurations, sequence length, and batch size. The size of the activation buffer depends on parallel strategy, model configurations, and implementation. The size of the activation buffer is considerably smaller than the previous two. As shown in Table 1, the size of the KV cache, 2.5×-5× larger than model weights, can quickly become the bottleneck in memory consumption. At the same time, much research has been spent on extending the length of the context window. GPT-4-32K can process up to 32,768 tokens [15]. Longer sequence length would make the KV cache memory problem even more severe. Assuming LLM generates until its maximum sequence length, we summarize the maximum batch size before going out of GPU memory on a box of 8 A100 80GB GPU in Table 2.1. At the GPT-3 scale with a maximum sequence length of 2048, batch size cannot exceed 35 without offloading. Small batch size limits the model inference throughput. 2.2 Efficient Attention Computing the attention matrix necessitates a time complexity of O(n2), where n is the sequence length. As a result, a line of work has been proposed to mitigate the computation burden of the attention mechanism [16–20]. These approaches exploit low-rank or sparsification to approximate the attention output. Besides, [21] realized exact efficient attention with wall-clock speed by optimizing Table 1: The memory consumption of model weights and KV cache for three different LLMs at batch size 128 and sequence length 2048 shows that the KV cache dominates the memory consumption. Model # of Layer Hidden Size Weights (GB) KV cache (GB) OPT-175B 96 12288 325 1152 LLaMA-65B 80 8192 130 640 BLOOM 70 14336 352 950 3Table 2: This table summarizes the maximum batch size before hitting out of memory on a box of 8 A100 80GB GPU when models are deployed with its maximum sequence length. Model OPT-175B LLaMA-65B BLOOM Maximum Batch Size 34 102 36 the number of memory reads and writes. However, these approaches were evaluated mostly for training, focused on computation complexity, and did not address the KV-Cache memory usage introduced by auto-regressive language models. Recently, there is active research attempting to apply quantization or pruning in LLM [ 8–14]. However, they mostly focus on reducing the size of model weights. Flexgen [7] applies quantization and sparsification to the KV cache; however, the memory of the KV cache is not reduced regarding sequence lengths. It stores the quantized KV cache for all tokens in CPU memory and loads all attention keys from CPU memory to compute attention scores. 3 The Persistence of Importance Hypothesis We first present one interesting observation upon which the persistence of importance hypothesis is derived in Section 3.1. In Section 3.2, we discuss the hypothesis in detail with empirical verification. Then, in Section 3.3, we provide theoretical intuition on the reason behind such model behaviors. 3.1 Repetitive Attention Pattern. Observation. We are interested in the attention score from the position t over all the words that come before it in the sentence. In Figure 1, we provide three attention maps of a sentence randomly drawn from C4 [22] using OPT-6B. Each attention map is a discretized attention score calculated at a randomly decided position. We consider a score larger than 1 t as significant as 1 t indicates an averaging mixing score. High attention scores are marked with dark green. Result. High attention scores are observed at the same set of tokens from various positions in the sentence. In all three plots, we see dark green at sequence positions 27, 63, 98, 121, 152, and 177, suggesting that these tokens received high attention at all three positions. We observe similar model behavior at different transformer layers with different text inputs. More plots are in Appendix A. Implication. Even though small differences exist, repetitive attention patterns are evident in the attention maps. There exist specific tokens that keep receiving high attention. Meanwhile, these attention maps show sparsity: only a few tokens have high attention scores. 3.2 The Persistence of Importance Hypothesis The repetitive attention pattern suggests that specific tokens are influential throughout the sequence. A stricter claim is that these tokens are the only ones that could be significant for a future step. Thus, we articulate the persistence of importance hypothesis. The Persistence of Importance Hypothesis. With a trained autoregressive language model, only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step. If true, this hypothesis indicates the possibility of foreseeing what information in the previous sequences could be vital for future steps. This hypothesis is trivial when pivotal tokens include all tokens in the entire sentences. However, a much more interesting case is when pivotal tokens are a subset of previous words. This would enable us to reduce the size of the KV cache by throwing away the embedding of non-important tokens. Pivotal Token. One natural indication of a token’s influence is the attention score. We consider a token pivotal for position t if this token receives an attention score larger than threshold α from the token at position t. Let St denote the set of pivotal tokens for position t. Sa→b denote the set of pivotal tokens for every position from a to b. Sa→b = ∪t=b t=aSt Verification. We measure persistence ratio as an empirical test the hypothesis. Persistence ratio measures how many tokens in the pivotal token sets of the later part of the sentence are also in the 40 10 20 30 40 50 60 Transformer Layer 0.5 0.6 0.7 0.8 0.9 1.0Pesistence Ratio OPT-6B OPT-13B OPT-30B OPT-66B (a) Persistence Ratio 0 10 20 30 40 50 60 Transformer Layer 0.0 0.2 0.4 0.6 0.8 1.0 |S0 t| t OPT-6B OPT-13B OPT-30B OPT-66B (b) Size of S0→t Figure 2: In this figure, we plot the persistence ratio and the corresponding size of the pivotal token set. The persistence ratio is over 95% in most of the layers, with decreases at the later layers. Meanwhile, the number of pivotal tokens is considerably smaller than the sequence length. This suggests that the pivotal tokens of later half sentences are almost all included in the set of first halves. pivotal token sets of the initial part of the sentence. Let l denote the length of the sentence. We record S1→t ∈ {x1, ...xt}, tokens in {x1, ..., xt} who received high attention from every position until t. Then, we record St+1→l ∈ {x1, ...xt}, tokens in {x1, ..., xt} who received high attention from position after t. The persistence ratio is the intersection divided by the size of St+1→l. Formally, Persistence Ratio = |St+1→l ∩ S0→t| |{x|x ∈ St+1→l, x∈ {x1, ..., xt}}| At the same time, we measure |S0→t| t . |S0→t| = t indicates that every token substantially impacted at least one position, which is the trivial case of persistence of importance hypothesis . Our test is performed with OPT models [ 23] with different datasets such as OpenBookQA [ 24] and Wiki- Text [25]. In our verification, we set t = l 2 , which measures the overlapping between the first and later half of the sentences. Same as in Section 3.1, we set α = 1 t , which suggests an average score. Result. We present our main results in Figure 2. First, given the current criterion of pivotal token and t value, the size of S0→t is considerably smaller than half of the sentence length. This verifies that we are not considering the trivial case of our hypothesis. Second, the persistence ratio is generally over 95%, with dips in the later transformer layers. The pivotal token set of the later half sentences is mostly included in the set of the first half sentences. Combining these two pieces of empirical evidence, we see positive evidence for our hypothesis test. Implication. The hypothesis provides insights for understanding the behavior of LLMs and opens up new opportunities for reducing the KV cache memory. The hypothesis suggests the possibility of predicting the potentially influential tokens for future steps. The non-influential tokens are unnecessary to store in the memory, as they are unlikely to have high attention scores. This reduces the number of tokens stored in the KV cache and the computation required at the attention. 3.3 Attention Weights Decides the Pivotal Tokens In the previous section, we verified that the significant tokens would continue to be significant. In this section, we try to understand the reasons for such phenomena. We consider the token generation process of a simplified model: a single-layer transformer model with single-head attention. xt+1 = F (at) , where at = softmax \u0000 1/t · xtWQW⊤ K X⊤ t−1 \u0001 Xt−1WV WO (1) xt ∈ R1×d is a row vector. Xt−1 ∈ R(t−1)×d denotes the aggregation of x1, . . . , xt−1, where the jth row is xj. WQ, WK, WV ∈ Rd×p and WO ∈ Rp×d are the attention weights. Lastly, F : R1×d → R1×d denotes the MLP block following attention block, a two-layer MLP with skip connections, given by F(x) = x + W2relu(W1x) (2) We are interested in the attention scoresαt = softmax(1/t ·xtWQW⊤ K X⊤ t−1). Notice that αt,j scales with xtWQW⊤ K x⊤ j . The following theorem characterizes the behavior of xtWQW⊤ K x⊤ j Theorem 3.1. Let A = WV WOWQW⊤ K and let λK, λQ, λV , λO denote the largest singular values of WK, WQ, WV , WO, respectively. Consider the transformer in (1) with normalized inputs ∥xt∥2 = 1 5Algorithm 1 Inference with Budget KV cache Input: Memory Budget B, Maximum Sequence Length Tmax Key Cache ¯K ∈Rn×d, Value Cache ¯V ∈Rn×d, where n = 0 while t < Tmax do Model update ¯K, ¯V such that n ← n + 1 if n > Bthen: Compress KV cache using Algorithm 2 such that n ≤ B. end if t ← t + 1 end while Algorithm 2 Compress KV Cache Input: Key Cache ¯K ∈Rn×d, Value Cache ¯V ∈Rn×d, History Window Size w, Recent Window Size r, Drop Amount m, Generation Step t, Importance Record I ← ⃗0 ∈ Rt for i ∈ [t − w, t] do ▷ Consider tokens within history window I ← I + αi < 1 t ▷ Increment the counter for low score token end for I[: −r] ← 0 ▷ Keep cache within the recent window Keep set St ← Argsort (I) [: −m] Keep everything in St in ¯K ∈Rn×d, ¯V ∈Rn×d such that n ← n − m for all t. Let c, ϵ >0 be constants. Assume that atx⊤ t+1 ≥ (1 − δ) ∥at∥2 with δ ≤ \u0010 cϵ λQλKλV λO \u00112 . Then for all xℓ satisfying xℓAx⊤ ℓ ≥ c and xℓAxℓ ≥ ϵ−1 maxj∈[t],j̸=ℓ xjAx⊤ ℓ , it holds that xℓAx⊤ ℓ ∥at∥2 (αt,ℓ − 3ϵ) ≤ xt+1WQW⊤ K x⊤ j ≤ xℓAx⊤ ℓ ∥at∥2 (αt,ℓ + 3ϵ) (3) The proof is provided in Appendix B. Theorem 3.1 shows that under an assumption on the MLP in (2), for all xℓ such that xℓAx⊤ ℓ is large enough, xt+1WQW⊤ K x⊤ j satisfies Equation (3). The assumption on the MLP atx⊤ t+1 ≥ (1−δ) ∥at∥2 essentially requires a large cosine similarity between the input and output of F. This behavior can be empirically verified in Appendix A. Essentially, skip connection dominates the output because ∥x∥2 ≫ ∥W2relu(W1x)∥2, resulting in a cosine similarity close to one between input and output. Equation (3) shows that despite a factor of xℓAx⊤ ℓ ∥at∥2 , xt+1WQW⊤ K x⊤ j almost scales with αt,ℓ. Since xt+1WQW⊤ K x⊤ j directly affects αt+1,ℓ, this property shows that a larger αt,ℓ will potentially imply a large αt+1,ℓ. Our theorem shows that the property in Equation (3) property only holds for xℓ such that xℓAx⊤ ℓ is large. A are trained attention weights. This condition may suggest that the trained weights A selects xℓ as a pivotal token. Each attention is learned to identify some subspace. Only those tokens embedded inside these regions are pivotal for this attention. This would explain why only some specific tokens are always relevant. 4 Sequential Token Generation Under budget In this section, we present SCISSORHANDS , which reduces the KV cache memory from the sequence length dimension without fine-tuning the model. In Section 4.1, we describe how SCISSORHANDS maintains the KV cache under a given budget. Section 4.2 provides a theoretical analysis of the algorithm and the approximation error. 4.1 Budget KV Cache for Single Attention Head In this section, for the sake of the discussion, we drop the layer number notation i and batch size dimension. Kt, Vt ∈ Rt×d denote for the KV cache until step t. xt ∈ R1×d is a row vector that 6denotes the input to attention at step t. The output of an attention head at step t can be written as, at = tX i=1 αt,iV[i]t, where αt,i = exp(⟨xtWK, Kt[i]⟩)Pt i=1 exp(⟨xtWK, Kt[i]⟩) Intuition. As shown in Section 3, the attention scores αt,i follow a strong power-law distribution. For the autoregressive generation process, if there exists an oracle such that we can identify the heavy score tokens before the future generation step, then the memory of the KV cache can be significantly reduced by only storing the heavy score tokens. Fortunately, the persistence of importance hypothesis provides us with such an oracle. It states that only historical tokens with significant contributions toward previous generated tokens will have significant contributions toward future tokens. Challenges. LLMs are deployed on hardware with a fixed memory. The algorithm should maintain the cache under fixed memory to meet the hard requirement. Further, LLMs are already computationally intensive. The algorithm should avoid introducing much extra burden on computation. A fixed memory budget for one attention head is B tokens. In other words, we can store key and value embedding for B previous tokens. We describe the problem as follows, Definition 4.1 (Sequential generation at an attention head under budget B). Given a stream of token embedding, including prompt and previously generated tokens, denotes their input to the head as {x1, . . . , xt, . . .}. The problem of sequential generation at an attention head under budget B is maintaining a key cache ¯Kt and value cache ¯Vt such that ¯Kt, ¯Vt ∈ Rn×d and n < B. Approach. Inspired by the textbook solution of reservoir sampling and the Least Recent Usage cache replacement algorithm, SCISSORHANDS reserves a fixed memory buffer for the KV cache. When the buffer is full, SCISSORHANDS drops stored but non-influential tokens from the cache. Naturally, attention scores are used as indicators of influence. We present the main algorithm in Algorithm 1 and Algorithm 2. The influence measure is collected over a history window to reduce variance. And recent tokens are always kept because of the lack of information on their importance. In practice, w and r are quite robust. We use r = 10 and w = 400 in all our experiments. With a sampled KV cache, attention output can be computed by the following estimator ˆat = nX i=1 ˆαt,i ¯Vt[i], where ˆαt,i = exp(⟨xtWK, ¯Kt[i]⟩)Pn i=1 exp(⟨xtWK, ¯Kt[i]⟩) Overhead Tradeoff At the compression step, an extra attention computation is introduced to collect the importance measurements over a history window. However, such compression is not required at every generation step. m controls the frequency, and we use m = 0.5B in our experiment. Further, steps after the compression have reduced attention computation because of the reduction in the KV cache. On the other hand, one can trade a tiny amount of memory to avoid the overhead by maintaining the importance record during generation steps in Algorithm 1. Allocating Budgets Across Attention Heads. An LLM typically consists of L transformer layers where each layer has H heads. A total memory budget has to be distributed over layers and heads. Within each transformer layer, the budget is distributed evenly across heads. Within the entire model, we distributed the budget according to Figure 2. The rule of thumb is to allocate more budget to later layers to compensate for the lower persistence ratio. 4.2 Theoretical Analysis. We study how much the tokens generated by the compressed KV cache deviate from the tokens generated by the original transformer using our simplified model in (1). Let {˜xt}T t=0 denote the tokens generated by the transformer with budget KV cache as in Algorithm 2 with m = 1: ˜xt+1 = F (˜at) , where ˜at = softmax \u0010 1/t · ˜xtWQ ˜K⊤ t \u0011 ˜V⊤ t WO Notice that when m = 1, i.e., in each iteration, we drop one token with the lowest score, the cache will always maintain B tokens. If the ranking of the attention scores does not change in each iteration, Algorithm 2 will always drop tokens with the smallest attention scores. 71X 2X 3X 4X 5X Compression 7.5 8.0 8.5 9.0 9.5 10.0Perlexity OPT-13B OPT-30B OPT-66B (a) Language Modeling 1X 2X 3X 4X 5X Compression 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Hellaswag MathQA PIQA Winogrande (b) OPT-6B Five shot 1X 2X 3X 4X 5X Compression 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Hellaswag MathQA PIQA Winogrande (c) OPT-13B Five shot 1X 2X 3X 4X 5X Compression 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Hellaswag MathQA PIQA Winogrande (d) OPT-30B Five shot Figure 3: This figure shows the accuracy trend of SCISSORHANDS on language modeling dataset and downstream tasks with different KV cache compression. In general, SCISSORHANDS incurs no accuracy drop until 5× compression on OPT-66B. For reference purposes, let {xt}T t=0 denote the tokens generated by a vanilla transformer defined in (1). We will bound the difference ∥xt − ˜xt∥2. Theorem 4.1. Let λ1, λ2 denote the largest singular values of W1 and W2 in (2). Let βt,j = exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ j \u0001 Pt−1 i=1 exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ i \u0001 and assume that each βt,j = cvt,j, where vt,j are sampled from a power-law distribution with pdf f(x) = c(x + b)−k. Suppose that λV λO(1 + λ1λ2)(1 + λQλK) ≤ 1 2 . Let Tmin and Tmax denote the starting and maximum sequence lengths, respectively, and let B ≤ Tmax denote the budget as in Algorithm 2. If for all t ∈ [Tmin, Tmax], St contains only tokes with at most the largest B values of βt,j, that is, |St| = B and minj∈St βt,j ≥ maxj /∈ˆSt βt,j, then for all ϵ ∈ (0, 1), with probability at least 1 − Tmax exp \u0010 − ϵ2b2(Tmin−1) (k−2)2(u−b)2 \u0011 − Tmax exp \u0010 −2(Tmin−1)(1−B/Tmax)2 (1−ϵ)2 \u0011 , the following error bound must hold for all t ∈ [Tmin, Tmax] E[∥xt − ˜xt∥2] ≤ 2.1(1 − B/Tmax) (1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u00131/(k − 1) ! (4) The definition of βt,j means the attention scores computed on the tokens generated by the compressed approach. Our theorem assumes that dropping the tokens depends on the attention score of the current iteration. (4) provided a bound on the expected difference between the tokens generated in the budget and the original approach. The upper bound scales with 1 − B/Tmax. When B = Tmax, meaning that we are keeping all of the tokens, the error becomes zero. The term k − (k − 1) \u0010 1−ϵ B−ϵ \u00111/(k − 1) depends on the distribution that the attention scores are fitted to and is always less than one. With a strong power-law distribution, this term provides a further decrease to the error bound in (4). 85 Empirical Evaluation In this section, we present the results that demonstrate SCISSORHANDS achieves up to 5× reduction in the KV cache memory compared to the standard model with no accuracy loss. We also show that SCISSORHANDS is compatible with 4-bit quantization. Experiment Setting. We compare the accuracy of SCISSORHANDS -OPT against the original OPT on one language model datasets C4 [22] and a number of few-shot downstream tasks: Hellaswag [26], MathQA [27], PIQA [28], Winogrande [29]. We use lm-eval-harness [30] to evaluate few-shot tasks. Our experiments are conducted on NVIDIA 4 A100 40GB GPU servers. No Accuracy Drop untill 5×. In Figure 3, we present SCISSORHANDS ’s accuracy trend where 1× denotes the original OPT. In the language modeling setting, perplexity is the lower the better. For OPT-6B, perplexity is maintained until 50% of the original KV cache size for OPT-13B. For OPT-66B, perplexity is maintained until 75% of the original KV cache. We observe a flatter accuracy trend as the model size grows, which is exceptionally encouraging. This suggests that SCISSORHANDS can scale with the model size. Downstream tasks are usually less sensitive to perturbation and bear more variance in terms of accuracy. We evaluate the 5-shot setting and 1 × denotes the original OPT model. For Winogrande and MathQA, accuracy is maintained even after 5× compression for OPT-66B. Similar to the language modeling setting,SCISSORHANDS performs better at larger models. Generally, accuracy is maintained with 15% - 30% of the original KV cache size. Table 3: Applying 4-bit quantization on top of SCISSORHANDS on Hellaswag. OPT-6B Original S CISSORHANDS SCISSORHANDS + 4-bit 0.702 0.706 0.704 OPT-13B Original S CISSORHANDS SCISSORHANDS + 4-bit 0.720 0.720 0.720 Compatible with 4-bit Quantization We test the compatibility of quantization and SCIS - SORHANDS at 2× compression. We adopt 4-bit quantization following [7]. Even Hellaswag is most sensitive based on Figure 3, adding quan- tization doesn’t introduce compounded errors. Ablation on Attention Score Error. We present the change ratio in attention score be- tween original OPT-13B and SCISSORHANDS OPT-13B at 3× compression on C4 in Figure 4. We observe the attention score generated from SCISSORHANDS is almost the same as the original KV cache, which also echoes Theorem 4.1. The change ratio is calculated as αs−αo αo where αs is the SCISSORHANDS attention score and αo is the original score. From Figure 4, we observe that the change ratio is centered around 0. -1 indicating that αs is significantly smaller compared to the original, suggesting that a small portion of the important tokens are dropped in the cache. To explain the above observation of SCISSORHANDS , we denote the n number of tokens with the highest score as {xtop_n t }T t=0. Then, for any other sets of tokens {x′ t}T t=0 that has no greater than n tokens, we can easily prove that similarity \u0010 xtopB t , xt \u0011 ≤ (x′ t, xt). Thus, SCISSORHANDS gives the most similar output as the original model at all layers. 6 Discussion, Limitation, and Future Work 1.0  0.8  0.6  0.4  0.2  0.0 Change Ratio Figure 4: Score between OPT and SCISSORHANDS . We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an un- expected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section 3.1. As shown in Figure 5, the repetitive attention pattern does not exist in randomly initial- ized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problem in language generation such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations. Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such 9behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we foresee no negative impacts. 7 Conclusion 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (a) Attention map of the token at position 178 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (b) Attention map of the token at position 228 Figure 5: We plot the attention map corresponding to Section 3.1 but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model. Inspired by our intriguing find- ings that pivotal tokens exert a lasting influence on future steps, we developed SCISSORHANDS to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory re- ductions of 5× in the KV cache without compromising the perfor- mance of LLMs. Furthermore, we demonstrate the compatibility of SCISSORHANDS with quanti- zation techniques, opening up the possibility of reducing memory us- age in both the representation and sequence length dimensions. 10References [1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [2] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [4] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. [5] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh, Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In Advances in Neural Information Processing Systems, 2022. [6] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022. [7] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y . Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. High-throughput generative inference of large language models with a single gpu, 2023. [8] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022. [9] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022. [10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [12] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023. [13] Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, and Dan Roth. Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale. arXiv preprint arXiv:2212.09095, 2022. [14] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. [15] OpenAI. Gpt-4 technical report, 2023. [16] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [17] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 11[18] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable lsh framework for efficient neural network training. In International Conference on Learning Representations, 2021. [19] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:17413–17426, 2021. [20] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [21] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. [22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [23] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. [24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018. [25] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [28] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. [29] Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin. Winogrande: An adversarial winograd schema challenge at scale. In Communications of the ACM, 2019. [30] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation. In Version v0. 0.1. Sept. Zenodo, September 2021. 12Appendix A More Observation Plots A.1 Repetitive Attention Pattern We provide the attention map similar to Figure 1 but from a different transformer layer on the same text in Figure 6, Figure 7, Figure 8 and Figure 9. A repetitive pattern and attention sparsity can be observed across layers. 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (a) Attention map at position 178 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (b) Attention map at position 228 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (c) Attention map at position 278 Figure 6: Attention Map at Layer 5 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (a) Attention map at position 178 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (b) Attention map at position 228 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (c) Attention map at position 278 Figure 7: Attention Map at Layer 10 130 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (a) Attention map at position 178 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (b) Attention map at position 228 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (c) Attention map at position 278 Figure 8: Attention Map at Layer 15 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (a) Attention map at position 178 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (b) Attention map at position 228 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 Sequence Position 01234 Attention Head (c) Attention map at position 278 Figure 9: Attention Map at Layer 20 A.2 Cross Layer Cosine Similarity In Section 3.3, our analysis assumes a large cosine similarity between the input and output ofF. Here, we provide empirical evidence to support such an assumption in Figure 10. Because of the residual connection in F and the domination of x, the cosine similarity between x and F(x) is extremely high. 140 5 10 15 20 Transformer Layer 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00Cosine Similarity Residual At MLP Cos(x, (x)) (a) Cosine Similarity 0 5 10 15 20 Transformer Layer 0 20 40 60 80 100 120Norm Residual At MLP ||x|| ||W2Relu(W1x)|| (b) Norm Comparision Figure 10: x and F(x) is high in cosine similarity B Proofs B.1 Proof of Theorem 3.1 We consider the token generation process of a simplified model: a single-layer transformer model with single-head attention. xt+1 = F (at) , where at = softmax \u0000 1/t · xtWQW⊤ K X⊤ t−1 \u0001 Xt−1WV WO (5) xt ∈ R1×d is a row vector. Xt−1 ∈ R(t−1)×d denotes the aggregation of x1, . . . , xt−1, where the jth row is xj. WQ, WK, WV ∈ Rd×p and WO ∈ Rp×d are the attention weights. Lastly, F : R1×d → R1×d denotes the MLP block following attention block, a two-layer MLP with skip connections, given by F(x) = x + W2relu(W1x) (6) We are interested in the attention scoresαt = softmax(1/t ·xtWQW⊤ K X⊤ t−1). Notice that αt,j scales with xtWQW⊤ K x⊤ j . We first re-state the Theorem 3.1 below. Theorem B.1. Let A = WV WOWQW⊤ K and let λK, λQ, λV , λO denote the largest singular values of WK, WQ, WV , WO, respectively. Consider the transformer in (5) with normalized inputs ∥xt∥2 = 1 for all t. Let c, ϵ >0 be constants. Assume that atx⊤ t+1 ≥ (1 − δ) ∥at∥2 with δ ≤ \u0010 cϵ λQλKλV λO \u00112 . Then for all xℓ satisfying xℓAx⊤ ℓ ≥ c and xℓAxℓ ≥ ϵ−1 maxj∈[t],j̸=ℓ xjAx⊤ ℓ , it holds that xℓAx⊤ ℓ ∥at∥2 (αt,ℓ − 3ϵ) ≤ xt+1WQW⊤ K x⊤ j ≤ xℓAx⊤ ℓ ∥at∥2 (αt,ℓ + 3ϵ) (7) As a preparation of the proof, we first show two lemmas. Lemma B.1. Let x1, x2 ∈ R1×m satisfies ∥x1∥2 = ∥x2∥2 = 1 and x1x⊤ 2 ≥ 1 − δ for some δ ∈ (0, 1). Then for all y ∈ R1×m we have \f\fx1y⊤ − x2y⊤\f\f ≤ √ 2δ ∥y∥2 15Proof. Let x2 = x∥ 2 + x⊥ 2 where x∥ 2 = x1x⊤ 2 · x1; x⊥ 2 = x2 − x∥ 2 Then it is easy to see that x⊥ 2 x⊤ 1 = 0. By the Pythagorean Theorem, we have \r\rx⊥ 2 \r\r2 2 = ∥x2∥2 2 − \r\r\rx∥ 2 \r\r\r 2 2 = δ(2 − δ) Therefore, we have ∥x1 − x2∥2 2 = \r\r\r(x1 − x∥ 2) − x⊥ 2 \r\r\r 2 2 = \r\r\u0000 1 − x1x⊤ 2 \u0001 x1 − x⊥ 2 \r\r2 2 = \u0000 1 − x1x⊤ 2 \u00012 + \r\rx⊥ 2 \r\r2 2 = 2δ Thus, the Cauchy-Schwarz inequality implies \f\fx1y⊤ − x2y⊤\f\f ≤ ∥x1 − x2∥2 · ∥y∥2 = √ 2δ ∥y∥2 Lemma B.2. Let ℓ ∈ [t] be given. Suppose that xℓAx⊤ ℓ > ϵ−1 \f\fxjAx⊤ ℓ \f\f for all j ̸= ℓ. Then we have (S(t)ℓ − ϵ) x⊤ ℓ axℓ ≤ x⊤ ℓ W⊤ K WQat ≤ (S(t)ℓ + ϵ) x⊤ ℓ axℓ Proof. Notice that at = αtXt−1WV WO =   t−1X j=1 αt,jxj  WV WO Thus, we have atWQW⊤ K x⊤ ℓ =   t−1X j=1 αt,jxj  WV WOWQW⊤ K x⊤ ℓ = t−1X j=1 αt,jxjAx⊤ ℓ Therefore \f\fatWQW⊤ K x⊤ ℓ − αt,ℓxℓAx⊤ ℓ \f\f = \f\f\f\f\f\f t−1X j=1,j̸=ℓ αt,jxjAx⊤ ℓ \f\f\f\f\f\f ≤ t−1X j=1,j̸=ℓ αt,j \f\fxjAx⊤ ℓ \f\f ≤ ϵxℓAx⊤ ℓ t−1X j=1,j̸=ℓ αt,j ≤ ϵxℓAx⊤ ℓ where in the second inequality we use ϵ−1 \f\fxjAx⊤ ℓ \f\f ≤ xℓAx⊤ ℓ and in the third inequality we usePt−1 j=1,j̸=ℓ αt,j ≤ Pt−1 j=1 αt,j = 1. This implies that (αt,ℓ − ϵ) xℓAx⊤ ℓ ≤ atWQW⊤ K x⊤ ℓ ≤ (αt,ℓ + ϵ) xℓAx⊤ ℓ 16Now we proceed to the main body of the proof. Assume that ∥xℓ∥2 = 1 for all ℓ. Using Lemma (B.1), if atx⊤ t+1 ≥ (1 − δ) ∥at∥2, then we have \f\f\f∥at∥−1 2 atWQW⊤ K x⊤ ℓ − xt+1WQW⊤ K x⊤ ℓ \f\f\f ≤ √ 2δ \r\rWQW⊤ K x⊤ ℓ \r\r 2 Recall that λQ, λK are the maximum singular values of WQ and WK, respectively. Then it holds that \r\rWQW⊤ K x⊤ ℓ \r\r 2 ≤ λQλK ∥xℓ∥2. Using ∥xℓ∥2 = 1, we have \f\f\f∥at∥−1 2 atWQW⊤ K x⊤ ℓ − xt+1WQW⊤ K x⊤ ℓ \f\f\f ≤ √ 2δλQλK Notice that ∥at∥2 = \r\r\r\r\r\r   t−1X j=1 αt,jxj  WV WO \r\r\r\r\r\r ≤ λOλV \r\r\r\r\r\r t−1X j=1 αt,jxj \r\r\r\r\r\r 2 ≤ λOλV t−1X j=1 αt,j ∥xj∥2 = λOλV Then since δ ≤ \u0010 cϵ λQλKλV λO \u00112 , we have \f\f\f∥at∥−1 2 atWQW⊤ K x⊤ ℓ − xt+1WQW⊤ K x⊤ ℓ \f\f\f ≤ 2cϵ λV λO ≤ 2cϵ ∥at∥2 Since by Lemma (B.2), we have \f\fatWQW⊤ K x⊤ ℓ − αt,ℓxℓAx⊤ ℓ \f\f ≤ ϵx⊤ ℓ axℓ It must hold that \f\f\fxt+1WQW⊤ K x⊤ ℓ − ∥at+1∥−1 2 αt,ℓxℓAx⊤ ℓ \f\f\f ≤ ϵ ∥at∥2 x⊤ ℓ axℓ + 2cϵ ∥at∥2 Since x⊤ ℓ axℓ ≥ c, it holds that 2cϵ ∥at∥2 ≤ 2ϵ ∥at∥2 x⊤ ℓ axℓ which implies that \f\f\fxt+1WQW⊤ K x⊤ ℓ − ∥at∥−1 2 αt,ℓxℓAx⊤ ℓ \f\f\f ≤ 3ϵ ∥at∥2 x⊤ ℓ axℓ Therefore xℓAx⊤ ℓ ∥at∥2 (αt,ℓ − 3ϵ) ≤ xt+1WQW⊤ K x⊤ ℓ ≤ xℓAx⊤ ℓ ∥at∥2 (αt,ℓ + 3ϵ) B.2 Proof of Theorem 4.1 Let {˜xt}T t=0 denote the tokens generated by the transformer with budget KV cache as in Algorithm 2 with m = 1: ˜xt+1 = F (˜at) , where ˜at = softmax \u0010 1/t · ˜xtWQ ˜K⊤ t \u0011 ˜V⊤ t WO Notice that when m = 1, i.e., in each iteration, we drop one token with the lowest score, the cache will always maintain B tokens. If the ranking of the attention scores does not change in each iteration, Algorithm 2 will always drop tokens with the smallest attention scores. For reference purposes, let {xt}T t=0 denote the tokens generated by a vanilla transformer defined in (5). We re-state Theorem 4.1 below, which bounds the difference∥xt − ˜xt∥2. 17Theorem B.2. Let λ1, λ2 denote the largest singular values of W1 and W2 in (6). Let βt,j = exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ j \u0001 Pt−1 i=1 exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ i \u0001 and assume that each βt,j = cvt,j, where vt,j are sampled from a power-law distribution with pdf f(x) = c(x + b)−k. Suppose that λV λO(1 + λ1λ2)(1 + λQλK) ≤ 1 2 . Let Tmin and Tmax denote the starting and maximum sequence lengths, respectively, and let B ≤ Tmax denote the budget as in Algorithm 2. If for all t ∈ [Tmin, Tmax], St contains only tokes with at most the largest B values of βt,j, that is, |St| = B and minj∈St βt,j ≥ maxj /∈ˆSt βt,j, then for all ϵ ∈ (0, 1), with probability at least 1 − Tmax exp \u0010 − ϵ2b2(Tmin−1) (k−2)2(u−b)2 \u0011 − Tmax exp \u0010 −2(Tmin−1)(1−B/Tmax)2 (1−ϵ)2 \u0011 , the following error bound must hold for all t ∈ [Tmin, Tmax] E[∥xt − ˜xt∥2] ≤ 2.1(1 − B/Tmax) (1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u00131/(k − 1) ! Define mk,j = I{j ∈ St}. With the definition of mk,j, ˜at can be written as ˜at =   t−1X j=1 ˜αt,j ˜xj  WV WO; ˜ αt,j = mk,j exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ j \u0001 Pt−1 i=1 mk,j exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ i \u0001 (8) Our first lemma shows the Lipschitzness of the attention module. Lemma B.3. Consider two sequences of tokens {xi}t i=1 and {yi}t i=1 where ∥xi∥2 = ∥yi∥2 = 1 for all i ∈ [t]. Define Xt−1, Yt−1 ∈ R(t−1)×d as the matrices whose ith row are xi and yi, respectively. Let ∆t = ∥xt − yt∥2. Then we have \r\r\r\rsoftmax \u00121 t xtWQW⊤ K X⊤ t−1 \u0013 2 − softmax \u00121 t ytWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 ≤ 2 √t − 1 t λQλK∆t Proof. We can decompose the difference as\r\r\r\rsoftmax \u00121 t xtWQW⊤ K X⊤ t−1 \u0013 − softmax \u00121 t ytWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 ≤ \r\r\r\rsoftmax \u00121 t xtWQW⊤ K X⊤ t−1 \u0013 − softmax \u00121 t xtWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 + \r\r\r\rsoftmax \u00121 t xtWQW⊤ K Y ⊤ t−1 \u0013 − softmax \u00121 t ytWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 By the Lipschitzness of softmax, we have\r\r\r\rsoftmax \u00121 t xtWQW⊤ K X⊤ t−1 \u0013 − softmax \u00121 t xtWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 ≤ 1 t \r\r\rxtWQW⊤ K (Xt−1 − Yt−1)⊤ \r\r\r 2 ≤ 1 t λQλK ∥xt∥2 ∥Xt−1 − Yt−1∥2 Since ∥xt∥2 = 1 and ∥Xt−1 − Yt−1∥2 = \u0010Pt−1 j=1 ∥xj − yj∥2 \u00111 2 ≤ √t − 1∆t, we have \r\rsoftmax \u0000 xtWQW⊤ K X⊤ t−1 \u0001 − softmax \u0000 xtWQW⊤ K Y ⊤ t−1 \u0001\r\r 2 ≤ √t − 1 t λQλK∆t Similarly, \r\r\r\rsoftmax \u00121 t xtWQW⊤ K Y ⊤ t−1 \u0013 − softmax \u00121 t ytWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 ≤ 1 t \r\r(xt − yt)WQW⊤ K Y ⊤ t−1 \r\r 2 ≤ 1 t λQλK ∥Yt−1∥F ∥xt − yt∥2 18Since ∥xt − yt∥2 = ∆t and ∥Yt−1∥2 = √t − 1, we have \r\r\r\rsoftmax \u00121 t xtWQW⊤ K Y ⊤ t−1 \u0013 − softmax \u00121 t ytWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 ≤ √t − 1 t λQλK∆t Combining the two bounds gives \r\r\r\rsoftmax \u0012 1√ txtWQW⊤ K X⊤ t−1 \u0013 − softmax \u0012 1√ tytWQW⊤ K Y ⊤ t−1 \u0013\r\r\r\r 2 ≤ 2 √t − 1 t λQλK∆t Our second lemma shows the difference between the output of the sampled and vanilla transformer when the input is the same. Lemma B.4. Let ˜at be defined as in (8). Define bt as bt =   t−1X j=1 βt,j ˜xj  WV WO; βt,j = exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ j \u0001 Pt−1 i=1 exp \u0000 1/t · ˜xtWQW⊤ K ˜x⊤ i \u0001 (9) Assume that ∥xj∥2 = 1 for all j ∈ [t]. Then we have ∥˜at − bt∥2 ≤ λV λO X j /∈ˆSt βt,j Proof. A direction computation yields ˜at − bt =   t−1X j=1 (˜αt,j − βt,j) ˜xj  WV WO Thus, ∥˜at − bt∥2 can be bounded as ∥˜at − bt∥2 ≤ λV λO t−1X j=1 (˜αt,j − βt,j) ∥˜xj∥2 = λV λO t−1X j=1 (˜αt,j − βt,j) since ∥˜xj∥2 = 1 for all j ∈ [t]. Now we analyze ˜αt,j − βt,j. Let ˆSt = St \\ {t}. Then mk,j = 1 if and only if j ∈ ˆSt. For convenience, let rt,j = 1/t · ˜xtWQW⊤ K ˜x⊤ j . Thus, β can be written as βt,j = exp (rt,j)P i∈ˆSt exp (rt,i) + P i/∈ˆSt exp (rt,i) Furthermore, for all j /∈ ˆSt, we have ˜αt,j = 0. For all j ∈ ˆSt, we have ˜αt,j = exp (rt,j)P i∈ˆSt exp (rt,i) Therefore, for all j ∈ ˆSt, we have βt,j − ˜αt,j = exp (rt,j) · P i/∈ˆSt exp (rt,i)\u0010P i∈ˆSt exp (rt,i) \u0011\u0010P i∈ˆSt exp (rt,i) + P i/∈ˆSt exp (rt,i) \u0011 = exp (rt,j)P i∈ˆSt exp (rt,i) · P i/∈ˆSt exp (rt,i)P i∈ˆSt exp (rt,i) + P i/∈ˆSt exp (rt,i) = ˜αt,j X i/∈ˆSt βt,j Therefore, the bound of ∥˜at − bt∥2 can be written as ∥˜at − bt∥2 ≤ λV λO   t−1X j∈ˆSt ˜αt,j X i/∈ˆSt βt,j − X j /∈ˆSt βt,j   = 2λV λO X j /∈ˆSt βt,j where the last equality follows from P j∈ˆSt ˜αt,j = 1. 19Our last lemma shows the Lipschitzness of the MLP in (6). Lemma B.5. Let λ1, λ2 denote the largest singular values of W1, W2 in (6). For all x1, x2 ∈ Rd, we have ∥F(x1) − F(x2)∥ ≤(1 + λ1λ2) ∥x1 − x2∥2 Proof. Direct computation yields ∥F(x1) − F(x2)∥ = ∥(x1 + W2relu (W1x1)) − (x2 + W2relu (W1x2))∥ ≤ ∥x1 − x2∥2 + ∥W2relu (W1x1) − W2relu (W1x2)∥ ≤ ∥x1 − x2∥2 + λ2 ∥relu (W1x1) − relu (W1x2)∥ ≤ ∥x1 − x2∥2 + λ2 ∥W1 (x1 − x2)∥2 ≤ ∥x1 − x2∥2 + λ1λ1 ∥x1 − x2∥2 = (1 + λ1λ2) ∥x1 − x2∥2 where in the third inequality we use the fact that relu(·) is 1-Lipschitz. Now we turn to the proof of our main theorem. Combining all of the results, we have at − ˜at =   t−1X j=1 αt,jxj  WV WO −   t−1X j=1 ˜αt,j ˜xj  WV WO =   t−1X j=1 αt,jxj  WV WO −   t−1X j=1 αt,j ˜xj  WV WO | {z } T1 +   t−1X j=1 αt,j ˜xj  WV WO −   t−1X j=1 βt,j ˜xj  WV WO | {z } T2 +   t−1X j=1 βt,j ˜xj  WV WO −   t−1X j=1 ˜αt,j ˜xj  WV WO | {z } T3 Therefore, by triangle inequality, we have ∥at − ˜at∥2 ≤ ∥T1∥2 + ∥T2∥2 + ∥T3∥2 (10) To start, the magnitude of T1 can be bounded as ∥T1∥2 = \r\r\r\r\r\r   t−1X j=1 αt,j(xt,j − ˜xt,j)  WV WO \r\r\r\r\r\r 2 ≤ λV λO \r\r\r\r\r\r t−1X j=1 αt,j(xt,j − ˜xt,j) \r\r\r\r\r\r ≤ λV λO t−1X j=1 αt,j ∥xt,j − ˜xt,j∥2 ≤ λV λO∆t t−1X j=1 αt,j = λV λO∆t 20where in the third inequality we use ∥xt,j − ˜xt,j∥2 = ∆ t and in the last equality we usePt−1 j=1 αt,j = 1 . To bound the magnitude of T2, we apply Lemma B.3, which shows that ∥αt − βt∥ ≤2 √t−1 t λQλK∆t to get that ∥T2∥2 = \r\r\r\r\r\r   t−1X j=0 (αt,j − βt,j)˜xj  WV WO \r\r\r\r\r\r 2 ≤ λV λO \r\r\r\r\r\r   t−1X j=0 (αt,j − βt,j)˜xj   \r\r\r\r\r\r 2 ≤ λV λO t−1X j=0 |αt,j − βt,j|∥ ˜xj∥2 ≤ λV λO ∥αt − βt∥1 ≤ √ t − 1λV λO ∥αt − βt∥2 ≤ 2 \u0012 1 − 1 t \u0013 λQλKλV λO∆t Lastly, to bound the magnitude of T3, we use Lemma B.4 to get that ∥T3∥2 ≤ 2λV λO X j /∈ˆSt βt,j Putting things together for (10), we have ∥at − ˜at∥2 ≤ λV λO  2 X j /∈ˆSt βt,j + (2λQλK + 1) ∆t   By Lemma B.5 we can further show that ∥xt+1 − ˜xt+1∥2 ≤ (1 + λ1λ2)λV λO  2 X j /∈ˆSt βt,j + (2λQλK + 1) ∆t   By Theorem B.3, we have that with probability at least 1 − Tmax exp \u0010 − ϵ2b2(Tmin−1) (k−2)2(u−b)2 \u0011 − Tmax exp \u0010 −2(Tmin−1)(1−B/Tmax)2 (1−ϵ)2 \u0011 , it holds for all t ∈ [Tmin, Tmax] that E  X j /∈ˆSt βt,j   ≤ (1 − B/Tmax) 0.98(1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u0013 1 k−1 ! := ∆max Given that E[∥xt − ˜xt∥] ≤ 2∆max, we have E[∥xt+1 − ˜xt+1∥2] ≤ (1 + λ1λ2)λV λO (2∆max + 2 (2λQλK + 1) ∆max) ≤ 4λV λO(1 + λ1λ2)(1 + λQλK)∆max Thus, as long as λV λO(1 + λ1λ2)(1 + λQλK) ≤ 1 2 , we can guarantee that E[∥xt+1 − ˜xt+1∥2] ≤ 2∆max Thus, for all t ∈ [Tmin, Tmax], we have that E[∥xt − ˜xt∥2] ≤ 2.1(1 − B/Tmax) (1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u0013 1 k−1 ! 21B.3 Budgeted Cache Theorem B.3. Let βt,j be sampled from some power-law distributionf(x) = c(x+b)−γ with support on [0, u−b) for some k >2 and u ≥ 5b. Let St be defined in Theorem B.2, and define ˆSt = St \\ {t}. Then with probability at least 1 − Tmax exp \u0010 − ϵ2b2(Tmin−1) (k−2)2(u−b)2 \u0011 − Tmax exp \u0010 −2(Tmin−1)(1−B)2 (1−ϵ)2 \u0011 it holds for all t ∈ T that E  X j /∈ˆSt βt,j   ≤ (1 − B/Tmax) 0.98(1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u0013 1 k−1 ! (11) We consider the case of maintaining a budget of B by dropping the smallest βt,j’s. Assume that vj has pdf f(x) = c(x + b)−k with support on [0, u− b). To make things precise, we first compute c c =  Z u−b 0 (x + b)−kdx !−1 = k − 1 b1−k − u1−k To start, we notice that Z x(x + b)−k = −(x + b)1−k((k − 1)x + b) (k − 1)(k − 2) := g(x) Let C = Pt−1 j=1 vj, then the expectation of C is E[C] = (t − 1)E[v1] = (t − 1) k − 1 b1−k − u1−k Z ∞ 0 x(x + b)−kdx = (t − 1) k − 1 b1−k − u1−k (g(u) − g(0)) = (t − 1) k − 1 b1−k − u1−k \u0012 b2−k (k − 1)(k − 2) − u1−k((k − 1)u − (k − 2)b) (k − 1)(k − 2) \u0013 = t − 1 k − 2 · b2−k − (k − 1)u2−k + (k − 2)bu1−k b1−k − u1−k Let ∆ = b2−k−(k−1)u2−k+(k−2)bu1−k b1−k−u1−k . By Hoeffding’s inequality, we have that P(C ≤ (1 − ϵ)E[C]) ≤ exp   − 2ϵ2E[C]2 (t − 1)(u − b)2 ! This implies that with probability at least 1 − exp \u0010 − 2ϵ2∆2(t−1) (k−2)2(u−b)2 \u0011 we have C ≥ (1 − ϵ)∆ t − 1 k − 2 Now, we proceed to bound P j /∈ˆSt βt,j where ˆSt = {j ∈ [t − 1] : βt,j ≥ γ C }. Equivalently, we can bound C−1 Pt−1 j=1 I{vj ≤ γ}vj. Its expectation is given by E  C−1 t−1X j=1 I{vj ≤ γ}vj   ≤ k − 2 (t − 1)∆(1 − ϵ)E   t−1X j=1 I{vj ≤ γ}vj   = k − 2 ∆(1 − ϵ) · k − 1 b1−k − u1−k Z γ 0 x(x + b)−kdx = (k − 1)(k − 2) ∆(1 − ϵ) (b1−k − u1−k) (g(γ) − g(0)) We pause here and study how small can we choose γ. Notice that E   t−1X j=1 I{vj ≤ γ}   = (t − 1)P(vj ≤ γ) = (t − 1) · b1−k − (γ + b)1−k b1−k − u1−k 22By Hoeffding’s inequality again, we have that P   t−1X j=1 I{vj ≤ γ} ≥(1 − ϵ)(t − 1) · b1−k − (γ + b)1−k b1−k − u1−k   ≤ exp   −2(t − 1)ϵ2 \u0000 b1−k − (γ + b)1−k\u00012 (b1−k − u1−k)2 ! Enforcing Pt−1 j=1 I{vj ≤ γ} ≥Tmax − B gives (γ + b)1−k ≤ b1−k − 1−B/Tmax 1−ϵ (b1−k − u1−k), which can be satisfied as long as γ ≥ \u0012\u0010 B/Tmax−ϵ 1−ϵ \u0011 1 1−k − 1 \u0013 b. Therefore g(γ) = − \u0012 b1−k − 1 − B/Tmax 1 − ϵ (b1−k − u1−k) \u0013 b + (k − 1)γ (k − 1)(k − 2) We further notice that b1−k − 1 − B/Tmax 1 − ϵ (b1−k − u1−k) ≥ B/Tmax − ϵ 1 − ϵ (b1−k − u1−k) This gives E  C−1 t−1X j=1 I{vj ≤ γ}vj   ≤ b(1 − B/Tmax) ∆(1 − ϵ)2 − (k − 1)(B/Tmax − ϵ)γ ∆(1 − ϵ)2 ≤ b(1 − B/Tmax) ∆(1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u0013 1 k−1 ! Notice that if u ≥ 5b, we have ∆ = b − (k − 1) \u0010u b \u00111−k · b − u b1−k − u1−k ≤ 0.98b Therefore E  C−1 t−1X j=1 I{vj ≤ γ}vj   ≤ (1 − B/Tmax) 0.98(1 − ϵ)2   k − (k − 1) \u0012 1 − ϵ B/Tmax − ϵ \u0013 1 k−1 ! holds with probability at least 1 − exp \u0010 − ϵ2b2(t−1) (k−2)2(u−b)2 \u0011 − exp \u0010 −2(t−1)(1−B/Tmax)2 (1−ϵ)2 \u0011 . Taking a union bound gives the desired result. 23",
      "references": [
        "On the opportunities and risks of foundation models.",
        "Holistic evaluation of language models.",
        "Language models are few-shot learners.",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Data distributional properties drive emergent in-context learning in transformers.",
        "Efficiently scaling transformer inference",
        "High-throughput generative inference of large language models with a single gpu",
        "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.",
        "nuqmm: Quantized matmul for efficient inference of large-scale generative language models.",
        "Llm. int8 (): 8-bit matrix multiplication for transformers at scale.",
        "Gptq: Accurate post-training quantization for generative pre-trained transformers.",
        "Massive language models can be accurately pruned in one-shot.",
        "Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale.",
        "Smoothquant: Accurate and efficient post-training quantization for large language models.",
        "Gpt-4 technical report",
        "Reformer: The efficient transformer.",
        "Linformer: Self-attention with linear complexity.",
        "Mongoose: A learnable lsh framework for efficient neural network training.",
        "Scatterbrain: Unifying sparse and low-rank attention.",
        "Rethinking attention with performers.",
        "Flashattention: Fast and memory-efficient exact attention with io-awareness",
        "Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "Opt: Open pre-trained transformer language models",
        "Can a suit of armor conduct electricity? a new dataset for open book question answering.",
        "Pointer sentinel mixture models",
        "Hellaswag: Can a machine really finish your sentence?",
        "Language models are unsupervised multitask learners.",
        "Piqa: Reasoning about physical commonsense in natural language.",
        "Winogrande: An adversarial winograd schema challenge at scale.",
        "A framework for few-shot language model evaluation."
      ],
      "meta_data": {
        "arxiv_id": "2305.17118v2",
        "authors": [
          "Zichang Liu",
          "Aditya Desai",
          "Fangshuo Liao",
          "Weitao Wang",
          "Victor Xie",
          "Zhaozhuo Xu",
          "Anastasios Kyrillidis",
          "Anshumali Shrivastava"
        ],
        "published_date": "2023-05-26T17:39:58Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses test-time KV-cache memory bottleneck for autoregressive LLM inference, where KV cache can exceed model weights and limits batch size/throughput. Introduces the “Persistence of Importance” hypothesis: tokens that receive high attention at some step tend to remain the main targets of attention in future steps. Empirically verifies high overlap (>90–95% across many layers) between early and later pivotal-token sets, and notes repetitive sparse attention patterns in trained (not random) models. Proposes SCISSORHANDS, a training-free, budgeted KV-cache management system that drops low-importance historical tokens to keep cache under a fixed token budget. Provides theoretical guarantees bounding approximation error under assumptions (power-law attention weights, Lipschitz/contractive components, correct top-B retention). Demonstrates up to ~5× KV-cache memory reduction without degrading perplexity or few-shot task accuracy, and shows compatibility with 4-bit KV quantization for further compression.",
        "methodology": "Defines pivotal tokens using attention-score threshold (e.g., >1/t) and aggregates across positions to test persistence ratio. SCISSORHANDS maintains per-head KV cache within a fixed token budget B during generation: when cache exceeds B, it computes an importance record over a sliding history window w by counting low-attention events per token, always preserves a recent window r, and evicts m tokens with highest low-score counts (least influential) via argsort-based selection. Compression can be periodic (m controls frequency) to amortize overhead. Budgets are distributed across layers/heads, allocating more to later layers due to lower persistence. Theoretical analysis uses a simplified single-layer single-head transformer; shows conditions under which large attention at time t implies large pre-softmax similarity at t+1 (via residual-dominated MLP), and derives an expected token-deviation bound when keeping the top-B attention-weight tokens assuming attention weights follow a power-law distribution.",
        "experimental_setup": "Models: OPT family (OPT-6B, 13B, 30B, 66B; persistence tests also mention OPT-30B/66B). Hardware: NVIDIA 4× A100 40GB servers (evaluation), memory motivation examples for 8× A100 80GB. Datasets/benchmarks: C4 for language modeling (perplexity); few-shot downstream tasks evaluated with lm-eval-harness in 5-shot setting: HellaSwag, MathQA, PIQA, WinoGrande. Persistence-hypothesis verification uses datasets including OpenBookQA and WikiText (and attention maps on C4 samples). Metrics: perplexity (LM) and accuracy (downstream). Compression levels reported as 1×–5× KV-cache reduction. Hyperparameters: recent window r=10, history window w=400; drop amount m≈0.5B in experiments. Additional experiment: 4-bit quantization of KV cache (per FlexGen-style) combined with SCISSORHANDS at 2× compression; reports HellaSwag accuracy unchanged. Ablation: attention-score change ratio distribution between original OPT-13B and SCISSORHANDS at 3× on C4.",
        "limitations": "Relies on empirical “persistence of importance” behavior; may vary by architecture, training regime, or task/domain. Eviction heuristic uses past attention as a proxy for future importance; important-but-rare tokens may be dropped, and later layers show reduced persistence requiring more budget. Theoretical guarantees depend on strong assumptions: residual-dominated MLP (high cosine similarity), accurate retention of top-B attention weights, and power-law modeling of attention scores, plus contractive bounds on weight singular values—conditions not fully verified for full multi-layer, multi-head LLMs. Additional compute overhead during compression due to recomputing/aggregating attention over a history window (though amortized). Evaluation limited to OPT up to 66B and a small set of benchmarks; no validation on very long contexts (e.g., 32K) or other model families (LLaMA, GPT-like) in the provided results. Does not directly report end-to-end throughput/latency improvements or system-level integration effects beyond memory reduction. Budget allocation across layers is heuristic. Potential carbon/emissions cost noted for large-model experiments.",
        "future_research_directions": "Test generality across architectures (decoder-only vs others), training objectives, and model families; analyze whether repetitive attention is an architectural bias or emergent from training. Develop principled/learned budget allocation across layers/heads and adaptive per-step budgets based on uncertainty or task signals. Improve importance estimation to reduce recomputation overhead (e.g., online maintenance of importance records, approximate attention probes, head-wise/shared statistics). Extend evaluation to very long contexts (16K–32K+) and dialogue/RAG workloads, and measure system metrics (throughput, latency, energy) in real inference stacks. Combine with additional compression methods (KV quantization beyond 4-bit, structured sparsity, CPU/GPU offload policies) and study compounded error. Investigate relationship between repetitive attention patterns and undesirable generation behaviors (e.g., repetition), potentially using cache management as a mitigation. Provide tighter multi-layer theoretical analysis and robustness guarantees when importance rankings shift over time.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Compositional Semantic Parsing with Large Language Models",
      "full_text": "Decorated discrete conformal maps and convex polyhedral cusps Alexander I. Bobenko Carl O. R. Lutz Abstract We discuss a notion of discrete conformal equivalence for decorated piecewise euclidean surfaces (PE-surface), that is, PE-surfaces with a choice of circle about each vertex. It is closely related to inversive distance and hyperideal circle patterns. Un- der the assumption that the circles are non-intersecting, we proof the corresponding discrete uniformization theorem. The uniformization theorem for discrete conformal maps corresponds to the special case that all circles degenerate to points. Our proof relies on an intimate relationship between decorated PE-surfaces, canonical tessella- tionsofhyperbolicsurfacesandconvexhyperbolicpolyhedra. Itisbasedonaconcave variational principle, which also provides a method for the computation of decorated discrete conformal maps. MSC (2020).Primary 52C26; Secondary 57M50, 52B70, 52B10. Key words and phrases. Discrete conformal maps, discrete uniformization, circle patterns, polyhedral realization, hyperbolic polyhedra, variational principle, variable combinatorics. 1 Introduction Circle patterns, discrete conformal maps, hyperbolic polyhedra, and their relationship are now a well established and flourishing theory. Modern interest in this subject started with W. Thurston’s idea to use circle patterns at the ideal boundary of hyperbolic3-space as an elementary tool to visualize hyperbolic polyhedra [52, Chpt. 13]. He rediscovered P. Koebe’s circle packing theorem [30] and connected it to the work ofE. Andreev on compact convex hyperbolic polyhedra [3,4]. Subsequently,B. Rodin’s andD. Sullivan’s proof of Thurston’s conjecture that circle packings could be used to approximate the classical Riemann map [46], set off a flurry of research on discrete analytic functions and conformal maps based on packings and patterns of circles [51]. Inparallel,anotherapproachtodiscreteconformalequivalenceoftriangulatedpiecewise euclidean surfaces (PE-surfaces) appeared: the discrete conformal change of metric is to multiply all edge lengths with scale factors that are associated with the vertices. Appar- ently, this idea was first considered in the four-dimensional Lorentz-geometric context of T. Regge’sdiscreteapproachtogeneralrelativity[43,45]. TheRiemann-geometricversion of this notion was introduced byF. Luo[32]. Informally speaking, a triangulated PE-surface is a surfaceS𝑔 glued edge-to-edge from euclidean triangles (see Sec. 2.1). It is determined by a triangulationTwith vertex set Date: June 13, 2023 arXiv:2305.10988v2  [math.GT]  12 Jun 20232 A. I. Bobenko and C. O. R. Lutz 𝑉 ⊂S𝑔 and edge-lengths ℓ𝑖𝑗 on the edges𝑖𝑗 of T. Now, two triangulated PE-surfaces (T,ℓ)and (T, ˜ℓ)are said to bediscrete conformally equivalentif there arelogarithmic scale factors𝑢∈R𝑉 such that (1–1) ˜ℓ𝑖𝑗 = e (𝑢𝑖+𝑢𝑗 )/2 ℓ𝑖𝑗 for all edges𝑖𝑗. This gives a rich theory, including convex variational principles and uniqueness results for the corresponding discrete mapping problems. The reason for this is the connection between discrete conformal equivalence, hyperbolic cusp surfaces, and R. Penner’s decorated Teichmüller spaces (see,e.g., [39]), discovered byU. Pinkall, B. Springborn, and the first author [8]. Ingeneral,theexistenceofsolutionscannotbeguaranteeifthecombinatoricsarefixed. Thus, using the mentioned interpretation in terms of hyperbolic geometry, one extends the notion of discrete conformal equivalence to variable combinatorics [25,49]. This extension canbeformulatedusingasequenceofDelaunaytriangulations. Furthermore,itonlydepend on the PE-metricdistS𝑔 on S𝑔, i.e., the complete euclidean path-metric onS𝑔 determined by (T,ℓ). X. Guet al.proved the corresponding uniformization theorem [25] (see also [49]). It states: given a PE-metricdistS𝑔 on the marked surface(S𝑔,𝑉)and Θ∈R𝑉 >0 satisfying the Gauß–Bonnet condition (1–2) 1 2𝜋 ∑︁ Θ𝑖 = 2𝑔−2 +|𝑉|, thenthereexistsauniquesecondPE-metric gdistS𝑔 on (S𝑔,𝑉),uptoscale,whichisdiscrete conformally equivalent todistS𝑔 and has the desired angle sumΘ𝑖 about each vertex𝑖. The aim of this article is to generalize this result to decorated PE-surfaces,i.e., PE- surfaces with a choice of circle about each vertex. They are determined by the PE-metric distS𝑔 and radii𝑟𝑖 ≥0 at the vertices (Sec. 2.2). Note that “undecorated” PE-surfaces correspond to the special choice𝑟𝑖 =0 for all vertices. We introduce two new viewpoints on the discrete conformal equivalence of such surfaces. The first is based on the Möbius- geometric properties of decorated euclidean triangles (Def. 2.3). It provides a framework unifying the approaches to discrete conformal geometry via circle patterns and vertex- scaling. Thisframeworkiscloselyconnectedto P. Bowers’and K. Stephenson’sinversive distance circle patterns[10,11], D. Glickenstein’sduality structures[21], andM. Zhang et al.’sunified discrete Ricci flow[55] (see Sec. 2.2). The second viewpoint uses lifts of decorated PE-surfaces into Minkowski3-space (Sec. 2.3). They provide decorated discrete conformal maps(Def. 2.5), that is, certain piecewise projective maps, which posses desirable properties for applications,e.g., com- puter graphics [20,50]. Furthermore, the lifts give an interesting interpretation of the (generalized) Epstein–Penner convex hull construction[16,35] (see Remark 4.3). WeightedDelaunaytessellationsaretheanalogueofDelaunaytessellationsfordecorated PE-surfaces (Sec. 3.1). Restricting to hyperideal decorations,i.e., choices of circles at the vertices such that no pair intersects, they always exist and are uniquely determined by the decorated PE-metric. Moreover, we can associate to each triangulated hyperideally decorated PE-surface a complete hyperbolic surfaceΣ𝑔 with cusps and complete ends of infinite area (Sec. 4.1). In the undecorated case two PE-surfaces are discrete conformally equivalent if and only if their Delaunay triangulations induce the same hyperbolic surface Σ𝑔 [8, Thm. 5.1.2]. In this spirit, we callΣ𝑔 the fundamental discrete conformal invariantDecorated discrete conformal maps and convex polyhedral cusps 3 ofthedecoratedPE-surfaceif Σ𝑔 isinducedbyitsweightedDelaunaytessellation. Relating weightedDelaunaytessellationstocanonicaltessellationsofhyperbolicsurfaces(Sec.4.3), weseethattwodecoratedPE-surfacesarediscreteconformallyequivalentifandonlyiftheir fundamental discrete conformal invariant coincide (Sec. 4.4). This leads to the main result of the paper. Theorem A. Let (distS𝑔 ,𝑟)be a hyperideally decorated PE-metric on the marked genus 𝑔 ≥0 surface (S𝑔,𝑉). Denote byΣ𝑔 its fundamental discrete conformal invariant,i.e., the complete hyperbolic surface induced by any triangular refinement of its unique weighted Delaunay tessellation. Then 1. (existence of realizations)there exists a decorated PE-metric discrete conformally equivalent to (distS𝑔 ,𝑟)realizing Θ ∈ R𝑉 >0 if and only ifΘ satisfies the Gauß–Bonnet condition (1–2). 2. (uniqueness of realizations)for eachΘ ∈R𝑉 >0 there exists at most one decorated PE-metric discrete conformally equivalent to(distS𝑔 ,𝑟)realizingΘ, up to scale. 3. (variational principle) The logarithmic scale factors, which give to the change of metric, correspond to a maximum point of the concave discrete Hilbert–Einstein functional HΣ𝑔,Θ (Sec. 4.6). Note that because of the variational principle this theorem provides an effective method to compute the corresponding decorated discrete conformal maps. An important special caseofthistheoremistheanalogueoftheclassicalPoincaré–Koebeuniformizationtheorem for decorated discrete metrics. Theorem B(discrete uniformization of decorated PE-surfaces). Given a hyperideally dec- orated PE-metric(distS𝑔 ,𝑟)on the marked genus𝑔surface (S𝑔,𝑉). Then there is a unique discrete conformally equivalent decorated metric realizing the uniform angle distribution Θ𝑖 ≡ 2𝜋(2𝑔−2 +|𝑉|) |𝑉| . The discrete Hilbert–Einstein functionalHΣ𝑔,Θ is a concave, twice continuously dif- ferentiable function overR𝑉 (Prop. 4.13), which can be explicitly expressed usingMil- nor’s Lobachevsky function(Sec. 4.5). Its gradient flow is related to thecombinatorial Yamabe/Ricci-flow (see Sec. 3.3). Up to a linear function, the discrete Hilbert–Einstein functional is the Legendre-transformation of the volume ofconvex (hyperideal) polyhedral cusps. Thesearecertaincollectionsofhyperidealtetrahedra, i.e.,hyperbolictetrahedrawith all vertices lying “outside” of hyperbolic3-space (Sec. 4.2). Indeed, Thm. A is connected to generalizations ofAndreev’s theorem: a discrete subgroup𝐺 of isometries of hyper- bolic 3-space isparabolic if it acts freely cocompactly on a horosphere and a hyperideal polyhedron 𝑃 is invariant under 𝐺 if 𝐺(𝑃)= 𝑃. Furthermore, the polyhedronrealizes a hyperbolic surfaceΣ𝑔 if 𝜕𝑃/𝐺 is isometric toΣ𝑔. Hence, our approach provides a new variational proof of the following theorem, which is equivalent to Thm. B in the case of genus1 surfaces. Theorem C. Let (S1,𝑉)be a marked genus1 surface. Each complete hyperbolic metric on (S1,𝑉)with cusps and complete ends of infinite area at𝑉 can be realized as a unique convex hyperideal polyhedron, up to hyperbolic congruence, invariant under the action of a parabolic group.4 A. I. Bobenko and C. O. R. Lutz This theorem first appeared inF. Fillastre’s work [17], where he uses themethod of continuity, by A. Alexandrov [2]. Note that this method is not constructive. The viewpoints taken byE. Andreevand F. Fillastreare in some sense “dual” to each other: the former deals with hyperbolic polyhedra withprescribed dihedral angleswhile the latter prescribesthemetric ofthehyperbolicpolyhedra(see[17]andreferencestherein). Thus,the dualtheorytoourhyperideallydecoratedPE-surfacesisgivenby hyperidealcirclepatterns . They were characterized independently byJ.-M. Schlenker [47] using the method of continuity andB. Springbornusing a variational approach [48] (see also [6]). Although, the functionals in [48] and [6] are closely related to our functional, the problems for circle patterns require fixed combinatorics. In contrast, discrete conformal equivalence problems are naturally considered with variable combinatorics. Some important questions are not addressed in this article. The first concerns conver- gence. Recently,F. Luoetal. provedtheanalogueof Rodin–Sullivan’sconvergencetheo- rem for undecorated PE-surfaces [33]. Furthermore, several results for the the convergence ofthe(discrete)logarithmicscalefactorstotheirsmoothcounterpartsareknown[23,34,56]. We believe that these results can be extended to decorated PE-surfaces. The second question is on discrete conformal equivalence in different “background geometries”. In [8] the analogue of Eq. (1–1) forpiecewise hyperbolic surfaceswas in- troduced. The corresponding existence and uniqueness results were obtained in [24] and an interpretation in terms of hyperbolic polyhedra was discussed in [41]. It is possible to definedecorationsforpiecewisehyperbolicsurfacesinthesamewayasforPE-surfaces(see also [22,26]). We plan to address the analogue of Thm. A in this setup in an upcoming article. Finally, there is a natural way to extend decorations beyond the case of strictly non- negative radii: decorations allowing imaginary radii (see Remark 2.4). The corresponding hyperbolic tetrahedra posses also vertices “inside” of hyperbolic3-space (see [55]). A variational approach via the discrete Hilbert–Einstein functional is still feasible (see,e.g., [18]). So some version of Thm. A should exist for this generalization. Clearly, the Gauß– Bonnetcondition(1–2)isstillnecessarybutwebelievethatitisnotsufficient. Thus,aspecial emphasis should be put on finding a characterization of all realizable discrete curvatures. 2 Decorated discrete conformal equivalence and maps 2.1 Piecewise euclidean surfaces and notation In this article, amarked surface(S𝑔,𝑉)is a closed orientable2-dimensional manifold of genus𝑔withafiniteset 𝑉 ⊂S𝑔,the marking. Acompletepathmetric distS𝑔 onthemarked surface ispiecewise euclidean(PE) if there is a cell-complexThomeomorphic toS𝑔 with 0-cells given by𝑉 such that each open2-cell with the restriction ofdistS𝑔 is isometric to a euclidean polygon (seee.g. [14]). We call a marked surface endowed with a PE-metric a PE-surface. It is flat except maybe for conical singularities at points in𝑉. The 1-cells of the cell-complex above are geodesics in the PE-surface. Thus, the cell- complexiscalleda (geodesic)tessellation. Ingeneralthereareinfinitelymanytessellations of a given PE-surface. The tessellation is atriangulation if all 2-cells are isometric to euclideantriangles. Werefertothe 0-cellsas vertices,the 1-cellsas edges,andthe 2-cellsas faces. The set of edges and faces is denoted by𝐸Tand 𝐹T, respectively. If the tessellation is clear from the context, we often simplify this to𝐸 and 𝐹.Decorated discrete conformal maps and convex polyhedral cusps 5 ℓ𝑖𝑗 ℓ𝑘𝑖 ℓ𝑗𝑘 𝜃𝑖 𝑗𝑘 𝜃𝑗 𝑘𝑖 𝜃𝑘 𝑖𝑗 𝑟𝑖 𝑟𝑗 𝑟𝑘 Figure 1: Notation in a decorated euclidean triangle. Remark2.1. The definitions and results in this article can be naturally extended to compact PE-surfaces with boundary. Doubling the surface and gluing it isometrically along the boundary, we obtain a closed PE-surfaces with reflection-symmetry corresponding to the boundary. This symmetry grants that we can work with the doubled surface and restrict to one half afterwards. There is no need that a triangulationT, in the sense of this article, is a simplicial complex,i.e.,self-glueingsofasingletriangleorglueingtwotrianglesalongmultipleedges isallowed. Indeed, it isessential forour constructionsin Sec.3.1 andSec. 4.3thatwe work withthegeneralclassoftriangulations. Still,weadoptthefollowingnotation: wedenoteby 𝑖𝑗 theedgewithvertices 𝑖and 𝑗,by 𝑖𝑗𝑘 thefaceincidenttothevertices 𝑖, 𝑗,and 𝑘,andby 𝑖 𝑗𝑘 the corner at vertex𝑖in the triangle𝑖𝑗𝑘 . This notation is simple but only unambiguous ifT is a simplicial complex. Nonetheless, we think that the risk of confusion is small compared to the gain of conciseness of our expositions. Remark 2.2. Most of our considerations will start with a single triangle or a quadrilateral givenbytwoadjacenttrianglesofthePE-surface. Thesecanalwaysbeliftedtheplanewhere our notation is unambiguous. The quantities we obtain in this way can then be projected back to the surface. Thus, the problem with the adopted notation amounts “merely” to an exercise in bookkeeping. A PE-metric on the marked surface(S𝑔,𝑉)is determined by a triangulationTand a function ℓ: 𝐸T →R>0, theedge-lengths, such that the triangle inequalities are satisfied on each triangle ofT. In particular, we can compute the angle𝜃𝑖 𝑗𝑘 at each corner abutting the vertex𝑖 ∈𝑉, e.g., using the law of cosines. Then thecone-angle𝜃𝑖 at 𝑖is given by sum of these angles𝜃𝑖 𝑗𝑘 (see Fig. 1). 2.2 Decorations and discrete conformal equivalence A decoration of a euclidean polygon is a choice of circle about each of its vertices (see Fig.1). ThisextendstoPE-surfaces, i.e.,a decorationofthePE-surface (S𝑔,𝑉, distS𝑔 )with geodesic triangulationTis a choice of decoration of each triangle such that it is consistent6 A. I. Bobenko and C. O. R. Lutz along edges of pairs of neighbouring faces. We refer to the circles of the decoration as vertex-circles. A decoration is determined by the radii𝑟: 𝑉 →R≥0, showing that it only depends on the PE-metricdistS𝑔 and not on the triangulationT. We call a decoration hyperideal if no pair of vertex-circles intersects. This is equivalent to (2–1) 𝑟2 𝑖 +𝑟2 𝑗 < distS𝑔 (𝑖,𝑗 ) for all(𝑖,𝑗 )∈ 𝑉2. Here, distS𝑔 (𝑖,𝑖)is the length of a smallest geodesic loop starting and ending in𝑖. For this article we will only consider hyperideal decorations. So from now on wemightdropthe“hyperideal”. Wecallthepair (distS𝑔 ,𝑟)adecoratedPE-metric andalso denote it by(ℓ,𝑟)if a triangulation is prescribed. Let us first consider a single decorated triangle. We can think of it to be embedded in C. This embedding is uniquely determined by(ℓ,𝑟)up to euclidean transformations. The conformalautomorphismsoftheextendedcomplexplane C∪{∞}aregivenby (orientation preserving) Möbius transformations,i.e., complex linear fractional transformations 𝑀: 𝑧 ↦→ 𝛼𝑧+𝛽 𝛾𝑧+𝛿 with 𝛼𝛿−𝛽𝛾 ≠0. Möbius transformations act bĳectively on the set of pointsC ∪{∞} and on the set of (euclidean) circles and lines, respectively. More precisely, they preserve the quadratic equations of the form (2–2) 𝑎𝑧¯𝑧 − ¯𝑏𝑧 −𝑏¯𝑧 +𝑐 = 0, where 𝑎,𝑐 ∈R and 𝑏 ∈C. For example, the circle with center(𝑝𝑥,𝑝𝑦)∈ R2 and radius 𝑟 >0 corresponds to the parameters (2–3) 𝑎=1, 𝑏 = 𝑝𝑥 +i𝑝𝑦 and 𝑐= 𝑝2 𝑥 +𝑝2 𝑦 −𝑟2. Identifying a Möbius transformation𝑀with the matrix \u0010 𝛼 𝛽 𝛾 𝛿 \u0011 ∈SL(2; C)and interpreting (2–2) in terms of Hermitian matrices, the action of𝑀on a circle is given by (2–4) ¯𝑀T \u0012𝑎 𝑏 ¯𝑏 𝑐 \u0013 𝑀. Thus, 𝑏¯𝑏−𝑎𝑐 is preserved under this action because it is minus the determinant of the Hermitian matrix. This expression is>0 for circles and=0 for points. Furthermore, it is a quadratic form with signature(3,1). The corresponding bilinear form is given by ⟨𝑋,𝑌⟩3,1 ≔ −1 2 tr \u0010 𝑋\u00000 −i i 0 \u0001 𝑌T \u00000 −i i 0 \u0001\u0011 . Parametrizing 𝑋 ∈Herm(2)by (2–5) 𝑋 = \u0012 𝑥4 +𝑥3 𝑥1 +i𝑥2 𝑥1 −i𝑥2 𝑥4 −𝑥3 \u0013 weseethat ⟨𝑋,𝑌⟩3,1 =𝑥1𝑦1 +𝑥2𝑦2 +𝑥3𝑦3 −𝑥4𝑦4. Furthermore,(2–4)showsthattheidentity component SO+(3,1)of its isometry group is isomorphic toSL(2; C). Thus, we obtain an identification up to scaling of circles and points with elements in{∥𝑋∥2 3,1 > 0}andDecorated discrete conformal maps and convex polyhedral cusps 7 {∥𝑋∥2 3,1 = 0}, respectively. Here ∥𝑋∥2 3,1 ≔ ⟨𝑋,𝑋 ⟩3,1. For more information about this identification we refer the reader to [5, Chpt. 3] and [27]. We say that a Möbius transformation maps a decorated triangle to another decorated triangle if it maps the vertex-circles of the first triangle to the corresponding vertex-circles of the second triangle, respectively. The euclidean triangles themself can be recaptured by considering the convex hulls of the centers of the vertex-circle. Such a transformation does not always exist. Indeed, a pair of circles cannot be mapped arbitrarily to two other circles by a Möbius transformation since theirinversive distance, i.e., (2–6) 𝐼𝑖𝑗 ≔ ℓ2 𝑖𝑗 −𝑟2 𝑖 −𝑟2 𝑗 2𝑟𝑖𝑟𝑗 , is a Möbius-geometric invariant (see,e.g., [15]). Lemma 2.1.Consider two decorated triangles given by(ℓ,𝑟)and (˜ℓ,˜𝑟), respectively. Sup- posethat 𝑟𝑖 >0 and ˜𝑟𝑖 >0 forall 𝑖 ∈{1,2,3}. ThenthereisauniqueMöbiustransformation mappingthefirstdecoratedtriangletotheseconddecoratedtriangleifandonlyiftheinver- sive distances for corresponding edges coincide. Proof. From parametrization (2–3) we obtain representatives𝐶𝑖, ˜𝐶𝑖 ∈ Herm(2)for the vertex-circles. Now, finding a Möbius-transformation mapping the first decorated triangle tothesecondisequivalenttofindinganelementof SO+(3,1)thatmaps 𝐶𝑖 to ˜𝐶𝑖 uptoscale. That is, we are looking for𝑢𝑖 ∈R such that \r\r ˜𝐶𝑖 \r\r2 3,1 = \r\re𝑢𝑖 𝐶𝑖 \r\r2 3,1 and ⟨˜𝐶𝑖, ˜𝐶𝑗⟩3,1 = ⟨e𝑢𝑖 𝐶𝑖,e𝑢𝑗 𝐶𝑗⟩3,1. Using the alternative parametrization (2–5), a small computation shows that ∥𝐶𝑖∥2 3,1 = 𝑟2 𝑖 and ⟨𝐶𝑖,𝐶𝑗⟩3,1 = −1 2 \u0000ℓ2 𝑖𝑗 −𝑟2 𝑖 −𝑟2 𝑗). Hence, the first equality givese𝑢𝑖 = ∥˜𝐶𝑖 ∥3,1/∥𝐶𝑖 ∥3,1 and the second equality shows that the compatibility conditions are □(2–7) −𝐼𝑖𝑗 = ⟨𝐶𝑖,𝐶𝑗⟩3,1 ∥𝐶𝑖∥3,1 ∥𝐶𝑗∥3,1 = ⟨˜𝐶𝑖, ˜𝐶𝑗⟩3,1 ∥˜𝐶𝑖∥3,1 ∥˜𝐶𝑗∥3,1 = −˜𝐼𝑖𝑗. The𝑢𝑖introducedinthepreviousproofarecalled (discrete)logarithmicscalefactors . To extend these considerations to decorated triangles with vanishing vertex-radii we consider 𝜖-families (ℓ𝜖,𝑟𝜖)of decorated triangles: the length and radii depend smoothly on the real parameter 𝜖. Now, a decorated triangle with aninfinitesimal circle at the vertex𝑖 is an 𝜖-family(ℓ𝜖,𝑟𝜖)of decorated triangles such that 𝑟𝜖 𝑖 = 𝜖𝑅𝑖 +o(𝜖). Here, 𝑅𝑖 >0 ando is a function satisfyinglim𝜖→0 o(𝜖)/𝜖 =0. Lemma 2.2. Given two decorated triangles(ℓ,𝑟)and (˜ℓ,˜𝑟). There is a unique Möbius transformation 𝑀 mapping the first decorated triangle to the second decorated triangle if and only if there are𝑢𝑖 ∈R for𝑖 ∈{1,2,3}such that ˜𝑟𝑖 = e𝑢𝑖 𝑟𝑖, ˜ℓ2 𝑖𝑗 = (e2𝑢𝑖 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑖 +e𝑢𝑖+𝑢𝑗 ℓ2 𝑖𝑗 + (e2𝑢𝑗 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑗8 A. I. Bobenko and C. O. R. Lutz for all edges𝑖𝑗. In particular, the𝑢𝑖 can be computed via (2–8) e𝑢𝑖 = ( ˜𝑟𝑖/𝑟𝑖 ,if𝑟𝑖 ≠0, |𝑀′(𝑝𝑖)| ,if𝑟𝑖 =0. Here, 𝑀′is the derivative of𝑀, and𝑝𝑖 ∈C represents the corresponding vertex of the first triangle. Proof. This lemma can essentially be proved in the same way as Lem. 2.1. The only difference is that in general we cannot obtain the logarithmic scale factors as quotients of ∥·∥ 3,1-norms. Instead, we can use the conditions on the mixed inner products to derive the alternative formula e2𝑢𝑖 = (˜ℓ2 𝑖𝑗 −˜𝑟2 𝑖 −˜𝑟2 𝑗) (ℓ2 𝑖𝑗 −𝑟2 𝑖 −𝑟2 𝑗) (ℓ2 𝑗𝑘 −𝑟2 𝑗 −𝑟2 𝑘) (˜ℓ2 𝑗𝑘 −˜𝑟2 𝑗 −˜𝑟2 𝑘) (˜ℓ2 𝑘𝑖 −˜𝑟2 𝑘 −˜𝑟2 𝑖) (ℓ2 𝑘𝑖 −𝑟2 𝑘 −𝑟2 𝑖), which is also valid if some𝑟𝑖 =0. It is immediate that we can compute𝑢𝑖 via ˜𝑟𝑖/𝑟𝑖 if 𝑟𝑖 ≠0. To see that𝑢𝑖 =log |𝑀′(𝑝𝑖)| if 𝑟𝑖 =0, endow(ℓ,𝑟)with an infinitesimal circle(ℓ𝜖,𝑟𝜖)at the vertex𝑖. Applying 𝑀 to this 𝜖-family, we obtain an infinitesimal circle(˜ℓ𝜖,˜𝑟𝜖)at𝑖with (˜ℓ0,˜𝑟0)=(˜ℓ,˜𝑟). Then lim 𝜖→0 ˜𝑟𝜖 𝑖 𝑟𝜖 𝑖 = |𝑀′(𝑝𝑖)|. □ Definition2.3 (DiscreteconformalequivalenceviaMöbiustransformations) . Twocombina- torially equivalent triangulated decorated PE-surfaces(T,ℓ,𝑟 )and (T, ˜ℓ,˜𝑟), arediscretely conformally equivalentif (i) for each face𝑖𝑗𝑘 ∈𝐹there is a Möbius transformation𝑀𝑖𝑗𝑘 , that maps the decorated triangle (𝑖𝑗𝑘,ℓ |𝑖𝑗𝑘 ,𝑟|𝑖𝑗𝑘 )to the decorated triangle(𝑖𝑗𝑘, ˜ℓ|𝑖𝑗𝑘 ,˜𝑟|𝑖𝑗𝑘 ), and (ii) for each𝑖 ∈𝑉with𝑟𝑖 =0 and adjacent faces𝑖𝑗𝑘,𝑖𝑗𝑙 ∈𝐹the Möbius transformations satisfy |𝑀′ 𝑖𝑗𝑘 (𝑝𝑖)| = |𝑀′ 𝑖𝑗𝑙 (𝑝𝑖)|. Remark 2.3. Our previous considerations show, that the inversive distance𝐼𝑖𝑗 along the edges are invariant under discrete conformal equivalence. Thus, our notion of discrete conformal equivalence is equivalent to thediscrete conformal mapsof inversive distance circlepackings proposedby P. BowersandK. Stephenson[11]. Notethatinversivedistance packings require𝑟𝑖 >0 for all𝑖 ∈𝑉. The notions of logarithmic scale factors and infinitesimal circles naturally extend to generaldecoratedPE-surfaces. Hence,Lem.2.2leadstothefollowingalternativedefinition of discrete conformal equivalence. Proposition&Definition 2.4 (discrete conformal equivalence via scale factors). Let Tbe a triangulation of the marked surface(S𝑔,𝑉). Two decorated PE-metrics(ℓ,𝑟)and (˜ℓ,˜𝑟) are discrete conformally equivalent if and only if there is a𝑢∈R𝑉 such that (2–9) ˜𝑟𝑖 = e𝑢𝑖 𝑟𝑖 ˜ℓ2 𝑖𝑗 = (e2𝑢𝑖 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑖 +e𝑢𝑖+𝑢𝑗 ℓ2 𝑖𝑗 + (e2𝑢𝑗 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑗 for all𝑖𝑗 ∈𝐸 (see Fig. 2).Decorated discrete conformal maps and convex polyhedral cusps 9 𝑖 𝑗 𝑟𝑖,𝑟𝑗 >0 ˜ℓ2 𝑖𝑗 =(e2𝑢𝑖 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑖 +e𝑢𝑖+𝑢𝑗 ℓ2 𝑖𝑗 +(e2𝑢𝑗 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑗 𝑖 𝑗 𝑟𝑖 >0, 𝑟𝑗 =0 ˜ℓ2 𝑖𝑗 =(e2𝑢𝑖 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑖 +e𝑢𝑖+𝑢𝑗 ℓ2 𝑖𝑗 𝑖 𝑗 𝑟𝑖,𝑟𝑗 =0 ˜ℓ2 𝑖𝑗 =e𝑢𝑖+𝑢𝑗 ℓ2 𝑖𝑗 Figure 2: Sketch of the three different cases of decorations along an edge together with the transformation formulas for the lengths under discrete conformal equivalence. Remark2.4. Thedefinitionsandderivationsinthissectioncanbedirectlyextendedbeyond the case of hyperideal decorations. Actually we can even allow “circles with imaginary radii”, that is, we considergeneralized radiigiven via𝑟2 : 𝑉 →R. Using the identification of circles with Hermitian matrices and Eq. (2–3) this gives us a decoration of the PE- surface. Eq. (2–4) shows that our definition of discrete conformal equivalence via Möbius transformations (Def. 2.3) is still applicable. Furthermore, there is a notion of inversive distance for these general circles given via (2–7). So also Prop. 2.4 is still true. ItfollowsthatourdefinitionofdiscreteconformalequivalenceofdecoratedPE-surfaces is equivalent to theunified discrete Ricci flowof M. Zhang et al.[55, Eq. 3.3] and the discrete conformal structuresvia duality structuresof D. Glickenstein[21]. Indeed, their different choice of variables is related to our(ℓ,𝑟)via Zhang et al. Glickenstein 𝜖𝑖 sgn(𝑟𝑖) — 𝛼𝑖 — 𝑟2 𝑖 2𝜂𝑖𝑗 |𝐼𝑖𝑗| (if𝑟𝑖𝑟𝑗 ≠0, otherwise see Eq.(4–4)) ℓ2 𝑖𝑗 −𝑟2 𝑖 −𝑟2 𝑗 2.3 Decorated discrete conformal maps For a decoration of a euclidean triangle𝑖𝑗𝑘 there is a unique circle𝐶𝑖𝑗𝑘 which is simul- taneously orthogonal to all vertex-circles of the triangle. We call it theface-circle of the decoratedtriangle 𝑖𝑗𝑘 anddenoteitsradiusby 𝑟𝑖𝑗𝑘 . Aface-circlepreservingprojectivemap between two decorated euclidean triangles is a projective map that maps one triangle onto the other and the face-circle of one to the face-circle of the other. Definition 2.5(decorated discrete conformal map). Let Tbe a triangulation of the marked surface (S𝑔,𝑉). A decorated discrete conformal mapfrom a triangulated decorated PE- surface (T,ℓ,𝑟 )to another combinatorially equivalent decorated PE-surfaces(T, ˜ℓ,˜𝑟)is a markingpreservingself-homeomorphismof (S𝑔,𝑉)thatrestrictstoaface-circlepreserving projective map on each triangle. Remark 2.5. In the special case of “undecorated” PE-surfaces this definition recovers the discrete conformal maps defined via circumcircle preserving projective maps proposed in [8]. To get a better grasp on face-circle preserving projective maps we make an auxiliary construction: denote byR2,1 Minkowski3-space, i.e., R3 endowed with the inner-product10 A. I. Bobenko and C. O. R. Lutz F {𝑧=𝑟𝑖𝑗𝑘 } Figure 3: Sketch of the lift of a decorated euclidean triangle to Minkowski3-space. Note thatface-circle(dotted)istheintersectionoftheaffineplane {𝑧=𝑟𝑖𝑗𝑘 }withthefuturecone F. Furthermore, the centers of the vertex-circles lie outside ofF. ⟨𝑝,𝑞⟩2,1 = 𝑝𝑥𝑞𝑥 +𝑝𝑦𝑞𝑦 −𝑝𝑧𝑞𝑧. In particular, ∥𝑝∥2 2,1 B ⟨𝑝,𝑝⟩2,1. Given a decorated euclideantriangle 𝑖𝑗𝑘 . Wecanisometricallyembeditintotheaffineplane {𝑧=𝑟𝑖𝑗𝑘 }⊂ R2,1 such that the face-circle𝐶𝑖𝑗𝑘 is the intersection of this plane with thefuture cone F ≔ \b 𝑝 ∈R2,1 : ∥𝑝∥2 2,1 <0, 𝑝𝑧 >0 \t (seeFig.3). Denoteby 𝑝𝑖 =(𝑥𝑖,𝑦𝑖,𝑟𝑖𝑗𝑘 )∈ R2,1 theliftsofthevertices. Fromtheeuclidean laws of cosine follows that𝑥2 𝑖 +𝑦2 𝑖 =𝑟2 𝑖 +𝑟2 𝑖𝑗𝑘 and 2(𝑥𝑖𝑥𝑗 +𝑦𝑖𝑦𝑗)=𝑟2 𝑖 +𝑟2 𝑗 +2𝑟2 𝑖𝑗𝑘 −ℓ2 𝑖𝑗. Hence, ∥𝑝𝑖∥2 2,1 = 𝑟2 𝑖 and ∥𝑝𝑖 −𝑝𝑗∥2 2,1 = ℓ2 𝑖𝑗. This shows that we can identify decorations of the euclidean triangle𝑖𝑗𝑘 with triples 𝑝𝑖,𝑝 𝑗,𝑝𝑘 ∈R2,1. Theidentificationisuniqueuptoconjugationwithelementsof SO+(2,1), i.e.,futureconepreservingisometriesoftheMinkowskiinnerproduct. Itfollows,thatface- circle preserving projective maps are in one-to-one correspondence (up to conjugation) to scaling the lifts,i.e., considering 𝑝 ↦→e𝑢𝑝 for scale factors(𝑢𝑖,𝑢𝑗,𝑢𝑘)∈ R3. Using the same argument as before, we can compute \r\re𝑢𝑖 𝑝𝑖 \r\r2 2,1 = e2𝑢𝑖 𝑟2 𝑖 and \r\re𝑢𝑖 𝑝𝑖 −e𝑢𝑗 𝑝𝑗 \r\r2 2,1 = (e2𝑢𝑖 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑖 +e𝑢𝑖+𝑢𝑗 ℓ2 𝑖𝑗 + (e2𝑢𝑗 −e𝑢𝑖+𝑢𝑗 )𝑟2 𝑗. These are exactly the equations derived in Prop. 2.4. So we obtain Proposition2.6 (decorateddiscreteconformalmaps) . LetTbeatriangulationofthemarked surface (S𝑔,𝑉). Given two decorated PE-metrics(ℓ,𝑟)and (˜ℓ,˜𝑟)w.r.t.T. The following statements are equivalent: (i) (ℓ,𝑟)and (˜ℓ,˜𝑟)are discrete conformally equivalent. (ii) There is a decorated discrete conformal map from(T,ℓ,𝑟 )to (T, ˜ℓ,˜𝑟).Decorated discrete conformal maps and convex polyhedral cusps 11 Remark2.6. Forpracticalapplications, e.g.,texturemapping,thedecorateddiscreteconfor- malmapsprovideuswitha projectiveinterpolationscheme betweentriangulatedPE-surfaces (see[50]). Comparedtothestandardlinearinterpolationitlooks“smoother”. Furthermore, M. Gillespie et al.showed how the identification with lifts toR2,1 allows us to use this interpolation method even for variable combinatorics [20]. 3 WeightedDelaunaytriangulationsanddiscreteconformalmap- ping problems 3.1 Weighted Delaunay tessellations of decorated PE-surfaces The notions of discrete conformality discussed in the previous section strongly depends on the choice of triangulation of the PE-surface under consideration. However, each pair of PE-metric and decoration(distS𝑔 ,𝑟)distinguishes a unique tessellation of the marked surface (S𝑔,𝑉): a(intrinsic)weightedDelaunaytessellation . Theyaredefinedvia properly immersed disks(𝜑,𝐷), or shortproper disks. Here,𝜑: ¯𝐷 →S𝑔 is a continuous map,𝐷is a disk in the euclidean plane, and¯𝐷its closure. Furthermore, the map𝜑|𝐷 is anisometric immersion, i.e., each point 𝐷 possesses a neighborhood which is mapped isometrically w.r.t. distS𝑔 . Finally, the properness says that the immersed circle𝜑(𝜕𝐷)intersects no vertex-circle, defined by𝑟, more then orthogonally. The intersection angle is understood to be the interior intersection angle of the disks bounded by𝜑(𝜕𝐷)and the vertex-circles, respectively. Proposition 3.1 (weighted Delaunay tessellations). Let (S𝑔,𝑉, distS𝑔 )be a PE-surface. Given a hyperideal decoration𝑟 ∈R𝑉 >0 of the vertices (see Eq. (2–1)). Then there exists a unique geodesic tessellationTof S𝑔 with vertex set𝑉, such that for each cell ofTthere is a proper disk which is orthogonal to the vertex-circles of the cell and either does not, or at most with an angle of𝜋/2, intersect any other vertex-circle. ThisTis called theweighted Delaunay tessellationof (S𝑔,𝑉, distS𝑔 )w.r.t.𝑟. Proof. In the case𝑟𝑖 = 0 for all 𝑖 ∈𝑉 this was proved in [9]. One can extend their arguments to the general case (see [35, Sec. 3.2] for the analogous considerations for piecewise hyperbolic metrics). □ Remark 3.1. Requiring that the decoration is hyperideal is sufficient to guarantee the exis- tence in Prop. 3.1. But it is not necessary. See [7, Sec. 2.5] for the general case. Given a marked surface(S𝑔,𝑉)endowed with the decorated PE-metric(distS𝑔 ,𝑟). We callageodesictriangulationofthisPE-surfacewhichrefinestheweightedDelaunaytessella- tiona weightedDelaunaytriangulationw.r.t. 𝑟. NotethatweightedDelaunaytriangulations are in general not unique. For a given weighted Delaunay triangulationTof the decorated PE-surfacewedenoteby CT(distS𝑔 ,𝑟)⊂ R𝑉 thesetoflogarithmicscalefactorssuchthat T is still a weighted Delaunay triangulation after discrete conformally changing the decorated metric. Proposition 3.2.The spaceCT(distS𝑔 ,𝑟)is homeomorphic to a polyhedral cone (with its apex removed). In particular, its interior is homeomorphic toR𝑉. Proof. This is a combination of Prop. 4.3 item (ii) and Prop. 4.9. □12 A. I. Bobenko and C. O. R. Lutz 𝑖 𝑗 𝑘 𝛼𝑘 𝑖𝑗 𝛼𝑖 𝑗𝑘 𝛼𝑗 𝑘𝑖 𝑟𝑖𝑗 𝑟𝑗𝑘𝑟𝑘𝑖 𝑖 𝑗 𝑘 𝑙 𝑑𝑘 𝑖𝑗 𝑑𝑙 𝑖𝑗 Figure4: Sketchofgeometricquantitiesinadecoratedeuclideantrianglewhichcanbeused to locally characterize weighted Delaunay triangulations. Remark 3.2. In the literature a hyperideal decorated PE-surface together with a weighted Delaunay triangulation is also called ahyperideal circle pattern[47,48]. It will be useful to have a local characterization of weighted Delaunay triangulations. To this end, let us consider a decorated euclidean triangle𝑖𝑗𝑘 . Denote by𝛼𝑘 𝑖𝑗 the interior intersection angle of the face-circle𝐶𝑖𝑗𝑘 and the edge𝑖𝑗. Furthermore, let 𝑟𝑖𝑗 be half of the distance between the two intersection-points of𝐶𝑖𝑗𝑘 with𝑖𝑗 (see Fig. 4, left). Note that 𝑟𝑖𝑗 is the radius of the unique circle which is orthogonal to the edge𝑖𝑗 and intersects it in the same points as𝐶𝑖𝑗𝑘 . It can be computed using𝑟𝑖, 𝑟𝑗 and ℓ𝑖𝑗 only. Now, the (oriented) distance 𝑑𝑘 𝑖𝑗 between the center of𝐶𝑖𝑗𝑘 and the edge𝑖𝑗 can express as (3–1) 𝑑𝑘 𝑖𝑗 = 𝑟𝑖𝑗 cot 𝛼𝑘 𝑖𝑗, wheretheorientationischosensuchthat 𝑑𝑘 𝑖𝑗 ispositiveifthecenterliesonthesamesideof 𝑖𝑗 as the triangle. Lemma 3.3.Given a decorated euclidean quadrilateral𝑖𝑙𝑗𝑘 . The following statements are equivalent (see Fig. 4, right): (i) Theproperdiskofthedecoratedtriangle 𝑖𝑗𝑘 intersectsthevertex-circleat 𝑙eithernot at all or at an angle not larger than𝜋/2. (ii) The center of the proper disk of the hyperideal triangle𝑖𝑗𝑘 “lies to the left” of the center corresponding to𝑖𝑙𝑗,i.e., (3–2) 𝑑𝑘 𝑖𝑗 +𝑑𝑙 𝑖𝑗 ≥ 0. (iii) The proper disks intersect with an angle less or equal to𝜋, i.e., 𝛼𝑘 𝑖𝑗 +𝛼𝑙 𝑖𝑗 ≤ 𝜋. Proof. The equivalence of items (ii) and (iii) follows from Eq. (3–1) and (3–3) cot 𝛼𝑘 𝑖𝑗 +cot 𝛼𝑙 𝑖𝑗 = sin \u0000𝛼𝑘 𝑖𝑗 +𝛼𝑙 𝑖𝑗 \u0001 sin 𝛼𝑘 𝑖𝑗 sin 𝛼𝑙 𝑖𝑗 .Decorated discrete conformal maps and convex polyhedral cusps 13 To see the other equivalence, we remember that the centers of all circles which are simul- taneously orthogonal to the vertex-circles at𝑖 and 𝑗 lie on a line, theirradical line. It is given by the perpendicular bisector of the line segment between the two intersection points of the face-circle𝐶𝑖𝑗𝑘 with the edge𝑖𝑗. The distance𝑑𝑘 𝑖𝑗 parametrizes this line overR and thus the1-parameter family of circles orthogonal to the vertex-circles at𝑖and 𝑗. It follows, that the inversive distance (Eq. (2–6)) between such a circle and the vertex-circle at𝑙 is a monotonically increasing function (in this parametrization). We observe that this inversive distance is 1 if 𝑑𝑘 𝑖𝑗 = −𝑑𝑙 𝑖𝑗. Now, the standard relationship of the inversive distance to intersection angles of circles (see,e.g., [10, Sec. 1]) gives the equivalence of items (i) and (ii). □ Proposition 3.4(weighted Delaunay triangulations, local characterization). Given a deco- rated PE-metric(distS𝑔 ,𝑟)on the marked surface(S𝑔,𝑉). (i) A geodesic triangulationTof the PE-surface(S𝑔,𝑉, distS𝑔 )is a weighted Delaunay triangulation w.r.t. 𝑟 if and only if all edges ofT satisfy condition (3–2) (or an equivalent). (ii) Two weighted Delaunay triangulations of(S𝑔,𝑉, distS𝑔 )w.r.t.𝑟 only differ on edges satisfying condition (3–2) with equality. (iii) A weighted Delaunay triangulation can be computed, starting from any geodesic triangulation, by the flip algorithm. That is, consecutively flipping edges violat- ing condition (3–2),i.e., locally modifying the geodesic triangulation by switching between the two possible triangulations of a quadrilateral. Proof. WeightedDelaunaytriangulationsandtheircomputabilitybytheflipalgorithmwere treated in [7, § 2.5]. For an alternative proof in the case of “undecorated” PE-surfaces see [9]. □ 3.2 The local discrete conformal mapping problems Tothenotionofdiscreteconformalequivalencecorrespondsthefollowingdiscreteconformal mapping problem: Problem 3.5(prescribed angle sums). Given • a decorated PE-metric(distS𝑔 ,𝑟)on the marked surface(S𝑔,𝑉), • a weighted Delaunay triangulationTw.r.t.𝑟, • and a desired angel sumΘ𝑖 for each vertex𝑖 ∈𝑉. Find logarithmicscalefactors 𝑢∈CT(distS𝑔 ,𝑟)suchthatthediscreteconformallychanged PE-metric w.r.t.𝑢has angle sumΘ𝑖 about each vertex𝑖 ∈𝑉. Several special cases of Problem 3.5 are interesting on their one. One of them is the analogue of the Poincaré–Koebe uniformization theorem for decorated PE-metrics. Problem 3.6(discrete uniformization). Given • a decorated PE-metric(distS𝑔 ,𝑟)on the marked surface(S𝑔,𝑉),14 A. I. Bobenko and C. O. R. Lutz • and a weighted Delaunay triangulationTw.r.t.𝑟. Find logarithmicscalefactors 𝑢∈CT(distS𝑔 ,𝑟)suchthatthediscreteconformallychanged PE-metric w.r.t.𝑢has angle sum2𝜋(2𝑔−2+|𝑉|)/|𝑉|about each vertex𝑖 ∈𝑉. In particular, Problem 3.6 asks to find a flat metric,i.e., Θ𝑖 =2𝜋for all𝑖 ∈𝑉, for PE- surfaces of genus1. Another sub-problem is to find such flat metrics on simply connected decorated PE-surfaces with boundary: Problem 3.7(planar triangulation with prescribed boundary angles). Given • a decorated PE-metric(distD,𝑟)on the (topological) diskD with marking𝑉, • a geodesic triangulationTof the PE-surface(D,𝑉, distD), which is the restriction of a weighted Delaunay tessellation of its geometric double (see Remark 2.1), • and a desired angle sumΘ𝑖 for each boundary vertex𝑖. Find logarithmic scale factors𝑢 ∈CT(distD,𝑟)such that the discrete conformally changed PE-metric w.r.t.𝑢is planar with the prescribed angle sums at the boundary. Proposition 3.8.If Problem 3.5 has a solution, then it is unique up to scale. The solution can be found by maximizing a concave functional. Proof. This follows from Thm. A. Moreover, we also have existence if we allow the combinatorics to change andΘsatisfies the Gauß-Bonnet condition (Eq. (1–2)). □ Corollary 3.9.If a solution to Problem 3.6 (or Problem 3.7) exists, then it is unique up to scale and can be computed maximizing a concave functional. Remark3.3. InapplicationsthePE-surfaceunderconsiderationcomesusuallyfrom3D-data whichalsoprescribessometriangulation. Thistriangulationwillingeneralbenotaweighted Delaunay triangulation. But we can use the flip algorithm (Prop. 3.4 item (iii)) to find one. Then, after solving Problem 3.5, we can transfer the solution to the PE-surface with the initial combinatorics by interpolating on the common refinement of the two triangulations (see [20, Sec. 6] for more details). 3.3 A combinatorial curvature flow To find solutions to Problem 3.5 we can try to evolution the logarithmic scale factors 𝑢: [0,𝑇)→ R𝑉,𝑇 ∈R>0 ∪{∞}, according to the initial value problem (3–4) ( d d𝑡𝑢𝑖(𝑡) =−(Θ𝑖 −𝜃𝑖(𝑡)), 𝑢𝑖(0) =0. Wecallthisthe decoratedΘ-flow. Here,𝜃𝑖(𝑡)istheangle-sumaboutthevertex 𝑖 ∈𝑉inthe decorated PE-metric induced by𝑢(𝑡)using the equations given in Prop. 2.4. We collect in this section some known local properties of this flow. Remark 3.4. In the literature this flow is commonly considered forΘ𝑖 = 2𝜋 for all𝑖 ∈𝑉 under the namecombinatorial Ricci/Yamabe-flow. It was first systematically studied by B. Chowand F. Luo[13] for intersecting circle patterns (Ricci-flow) andF. Luo[32] for “undecorated” PE-surfaces (Yamabe-flow).Decorated discrete conformal maps and convex polyhedral cusps 15 Lemma 3.10. Given a triangulated decorated PE-surface(T,ℓ,𝑟 ). Under conformal dis- crete change of decorated PE-metrics the angles𝜃𝑖 in a triangle𝑖𝑗𝑘 ∈𝐹Tvary by (3–5) d𝜃𝑖 𝑗𝑘 = 𝑟𝑖𝑗 cot 𝛼𝑘 𝑖𝑗 ℓ𝑖𝑗 (d𝑢𝑗 −d𝑢𝑖)+ 𝑟𝑖𝑘 cot 𝛼𝑗 𝑖𝑘 ℓ𝑖𝑘 (d𝑢𝑘 −d𝑢𝑖). Proof. This is a reformulation of [22, Thm. 5] using Eq. (3–1). Note that our notation is potentially confusing when compared to [22]: our𝑑𝑘 𝑖𝑗 are theirℎ𝑖𝑗. Furthermore, we will assign a different meaning toℎ𝑖𝑗 in Sec. 4.2. □ ForatriangulateddecoratedPE-surface (T,ℓ,𝑟 )wedefinethe decoratedcotan-weights 𝑤: 𝐸T→R by 𝑤𝑖𝑗 B cot 𝛼𝑘 𝑖𝑗 +cot 𝛼𝑙 𝑖𝑗, where𝑖𝑗𝑘 and𝑖𝑙𝑗 are the triangles adjacent to the edge𝑖𝑗. Lemma 3.11. Given a weighted Delaunay triangulationT w.r.t.the decoration𝑟 of the PE-surface (S𝑔,𝑉, distS𝑔 ). Then Íd𝜃𝑖d𝑢𝑖 is a negative semi-definite quadratic form. In particular, it is given by (3–6) −1 2 ∑︁ 𝑖𝑗 𝑤𝑖𝑗 𝑟𝑖𝑗 ℓ𝑖𝑗 \u0000d𝑢𝑗 −d𝑢𝑖 \u00012. Proof. Formula Eq. (3–6) follows from a straightforward computation: first summing over all corners, using Eq. (3–5), then regrouping by edges. SinceTis a weighted Delaunay triangulation, Lem. 3.3 and Eq. (3–3) show that𝑤𝑖𝑗 ≥0 for all𝑖𝑗 ∈ 𝐸T. Hence, the semi-definiteness is a consequence of formula (3–6). Note that the kernel ofÍd𝜃𝑖d𝑢𝑖 is at least 1-dimensional. It always contains the space spanned by the constant vector1𝑉 ∈R𝑉, which corresponds to the scale-invariance of the angles. □ Remark 3.5. In the context of discrete differential geometryd𝜃 = (d𝜃𝑖)𝑖∈𝑉 is known as a discrete Laplace-Beltrami operatorand Íd𝜃𝑖d𝑢𝑖 is its associateddiscrete Dirichlet energy (see,e.g., [21, §6]). If𝑟𝑖 =0 for all𝑖 ∈𝑉, then2𝑟𝑖𝑗 =ℓ𝑖𝑗 for all edges𝑖𝑗. So in this cased𝜃 is the well-knowncotan-Laplace operator(up to a factor) [40]. Proposition3.12. ConsideradecoratedPE-metric (distS𝑔 ,𝑟)onthemarkedsurface (S𝑔,𝑉). LetTbeaweightedDelaunaytriangulation w.r.t.𝑟. Thedecorated Θ-flow(3–4)is(locally) the gradient flow of a concave functional. Proof. Locally this follows from Lem. 3.11 and Prop. 3.2. The global result, including an explicite formula for the functional, is given in Prop. 4.13. □ 4 Interpretation in terms of hyperbolic geometry 4.1 Circle patterns at the ideal boundary of hyperbolic space Considera singledecorated triangle𝑖𝑗𝑘 . Ifwe interpretthe diskbounded bythe face-circle 𝐶𝑖𝑗𝑘 asthehyperbolicplaneintheKlein-model,thenthetriangle 𝑖𝑗𝑘 becomesa hyperideal triangle. The vertices outside the hyperbolic plane are calledhyperideal (𝑟𝑖 > 0)and those on the circle𝐶𝑖𝑗𝑘 are ideal vertices(𝑟𝑖 =0). This equips each (abstract) triangle in a16 A. I. Bobenko and C. O. R. Lutz ∞ 𝑖 𝑗 𝜃𝑘 𝑖𝑗 𝜃𝑖 𝑗𝑘 𝜃𝑗 𝑘𝑖 Figure 5: Sketch of a hyperideal horoprism over the decorated euclidean triangle𝑖𝑗𝑘 . The hyperideal triangle giving the lower face of the horoprism is shaded. Note that the dihedral angles at the vertical edges coincide with the interior angles𝜃𝑘 𝑖𝑗 of the euclidean triangle. triangulateddecoratedPE-surface (T,ℓ,𝑟 )withahyperbolicmetricwhichfittogetheralong the edges. Thus, we obtain a complete hyperbolic surfaceΣ𝑔 with ends homeomorphic to S𝑔\\𝑉equippedwithageodesictriangulationcombinatoriallyequivalentto T. Theendsof finitearea( cusps)andinfinitearea( flares)of Σ𝑔correspondtoidealandhyperidealvertices, respectively. Definition 4.1(fundamental discrete conformal invariant). Given a triangulated decorated PE-surface (T,ℓ,𝑟 ). The complete hyperbolic surfaceΣ𝑔 constructed above is called the fundamental discrete conformal invariantof (T,ℓ,𝑟 ). To see that this construction is indeed invariant under discrete conformal equivalence, we consider another way to associate a hyperideal triangle to𝑖𝑗𝑘 . The extended complex planeC∪{∞}istheboundaryatinfinityofhyperbolic 3-space(idealboundary)intheupper half-space model,H3 = {(𝑥,𝑦,𝑧 )∈ R3 : 𝑧 >0}. To each circle𝐶 in C we associate the half-sphere inH3 intersecting C in𝐶orthogonally. Similarly, we can associate a half-plane to a line inC. These half-spheres and half-planes are the totally geodesic planes inH3. Now, the hyperbolic planes over the edges of𝑖𝑗𝑘 together with the plane over𝐶𝑖𝑗𝑘 bound a hyperidealtetrahedron withonedistinguishedidealvertexat ∞. Itisalsocalleda hyperideal horoprism to emphasise that we distinguished an ideal vertex (see Fig. 5). Thelower face of this tetrahedron,i.e., the face which is not incident to∞, is a hyperideal triangle. It is related to the previously considered hyperideal triangle via orthographic projection. This is the well-known relationship between the Klein-model and the upper half-sphere model of the hyperbolic plane (see,e.g., [12]). So both constructions induce the same hyperbolic metric on the triangle𝑖𝑗𝑘 , and thus onS𝑔 \\𝑉. As Möbius-transformations act on circles inC ∪{∞}, they act on hyperbolic planes. This gives a one-to-one correspondence to isometries of hyperbolic3-space, known asDecorated discrete conformal maps and convex polyhedral cusps 17 >0 <0 𝑖 𝑗 ∞ ℎ𝑖 ℎ𝑗ℎ𝑖𝑗 𝜆𝑖𝑗 ℓ𝑖𝑗 Figure6: Left: thehyperbolicdistancebetweenahorocycleandahyperboliclineisdefined to be positive if the horocycle and line do not intersect and negative otherwise.Right: notation for hyperideal triangles. Note that the heightsℎ𝑖 depend on the horocycle a∞and can be negative according to convention. Poincaré-extension(see, e.g., [42, § 4.4]). Since discrete conformal maps act by Möbius- transformations on each triangle𝑖𝑗𝑘 (Def. 2.3) we arrive at Proposition4.2. TwotriangulateddecoratedPE-surfaceswiththesametriangulation Tare discrete conformally equivalent if and only if they induce the same fundamental discrete conformal invariant. Note that we do not transform the distinguished vertex at∞. Thus, only the intrinsic geometry of the lower faces of the hyperideal horoprisms is preserved,i.e., the hyperbolic surface Σ𝑔. It is characterized by the𝜆-lengths of the geodesic triangulationT. For each hyperideal triangle𝑖𝑗𝑘 they are given by the(truncated) lengths(𝜆𝑖𝑗,𝜆𝑗𝑘,𝜆𝑘𝑖)of its edges. If both vertices of an edge𝑖𝑗 are hyperideal, then𝜆𝑖𝑗 > 0 is the distance between the hyperbolic planes associated to the vertices. In this case𝜆𝑖𝑗 is related to the inversive distance (Eq. (2–6)) via (4–1) 𝐼𝑖𝑗 = cosh(𝜆𝑖𝑗) (see[10,Sec.1.2]). Shouldavertexbeideal,weequipitwithan auxiliaryhorosphere. The distance𝜆𝑖𝑗 is now taken with respect to this horosphere. It is positive if the horosphere is disjointwiththehyperbolicplane, orhorosphere,attheothervertexandnegativeotherwise (see Fig. 6, left). Remark 4.1. If all ends of the hyperbolic surfaceΣ𝑔 are cusps, that is,𝑟𝑖 =0 for all𝑖 ∈𝑉, then our 𝜆-lengths are thelogarithmic edge lengthintroduced in [8]. They are closely related toR. Penner’slambda-lengths √ 2e 𝜆/2 = √ 2ℓ(see Eq. (4–4)) which parametrize the decorated Teichmüller spaceof hyperbolic cusp surfaces [39]. 4.2 Hyperideal polyhedral cusps Thoughnotinvariantunderdiscreteconformalmaps,thecollectionofhyperidealhoroprisms carries still relevant information. We call it a(hyperideal) polyhedral cuspover the trian- gulated hyperbolic surface(Σ𝑔,T). It is homeomorphic to the cylinder[0,1)× \u0000S𝑔 \\𝑉\u000118 A. I. Bobenko and C. O. R. Lutz and endowed with a piecewise hyperbolic metric with conical singularities corresponding thevertical edgesof thehyperideal horoprisms,i.e., the edges incidentto∞. The boundary of the polyhedral cusp is isometric toΣ𝑔. For a single hyperideal horoprism defined by a decorated euclidean triangle𝑖𝑗𝑘 the dihedral angles at vertical edges are𝜃𝑘 𝑖𝑗 (see Fig. 5). Therefore, the cone-angles at the singularities of the polyhedral cusp coincide with the cone-angles 𝜃𝑖 of the corresponding decorated PE-surface. If we add an auxiliary horosphere to the distinguished vertex∞, say the horosphere given by{𝑧 = 1}⊂ H3, we can find truncated length(ℎ𝑖,ℎ 𝑗,ℎ𝑘) ∈R3 for the edges of a hyperideal horoprism which are incident to∞. We callℎ𝑖 the height over the vertex𝑖. The 𝜆-lengths and heights characterize the piecewise hyperbolic metric of the polyhedral cusp. Since the hyperbolic distance between two points(𝑥,𝑦,𝑝 ),(𝑥,𝑦,𝑞 )∈ H3 is given by |ln(𝑝)−ln(𝑞)|, non-vanishing vertex radii𝑟𝑖 are related to the heightsℎ𝑖 by (4–2) 𝑟𝑖 = e−ℎ𝑖 . In the same way, we see that the distanceℎ𝑖𝑗 between an edge𝑖𝑗 in the lower face and the auxiliary horosphere at∞satisfies𝑟𝑖𝑗 =e−ℎ𝑖 𝑗 (see Fig. 6, right). Moreover, ifg𝑖𝑗𝑘 is a dec- orated triangle discrete conformally equivalent to𝑖𝑗𝑘 then, by Eq. (2–8), the corresponding logarithmic scale factors𝑢𝑖 are (4–3) 𝑢𝑖 = ℎ𝑖 −˜ℎ𝑖. The relationship (4–2) between the heights and radii provides us with a general means to compute the𝜆-lengths: (4–4) e𝜆𝑖 𝑗 +𝜖𝑖𝜖𝑗e−𝜆𝑖 𝑗 = ℓ2 𝑖𝑗eℎ𝑖+ℎ𝑗 −𝜖𝑖eℎ𝑗 −ℎ𝑖 −𝜖𝑗eℎ𝑖 −ℎ𝑗 . Here, 𝜖𝑥 =1 if 𝑟𝑥 >0 and 𝜖𝑥 =0 otherwise, 𝑥 ∈{𝑖,𝑗 }. If𝜖𝑖𝜖𝑗 =1, this is a reformulation of Eq. (4–1) (up to a factor of1/2). From this we obtain the general formula by letting one hyperideal vertex, or both, limit towards a corresponding ideal vertex. Which is equivalent to considering𝑟𝑖 →0, or𝑟𝑗 →0, as required. Remark 4.2. The heightsℎ𝑖, and also the𝜆-lengths if there are ideal vertices, depend on the choice of auxiliary horospheres. Still, a change of auxiliary horosphere only results in a constant offset of all lengths adjacent to the respective vertex. The offset is given by the distance to the former horosphere. Thus, a change of auxiliary horosphere about the distinguished vertex∞corresponds to constant scaling of the associated decorated PE-surface by Eq. (4–3). 4.3 Hyperbolic decorations and canonical tessellations Up to this point we considered fixed triangulations. We are now going to collect some results about special triangulations of hyperbolic surfaces,i.e., of the fundamental discrete conformal invariants. In the next section this will lead us to the definition of discrete conformal equivalence with variable combinatorics. First,letusconsiderapolyhedralcuspoverahyperbolicsurface Σ𝑔. Toeachhyperideal vertex we can associate the hypersphere which is orthogonal to the auxiliary horosphere at thevertexat ∞. Similarly,wefindahorospherewiththispropertyforeachidealvertex(see Fig. 7). The radii𝜌𝑖 of these spheres are related to the heightsℎ𝑖 by (4–5) eℎ𝑖 = e𝜌𝑖 −𝜖𝑖e−𝜌𝑖 2 .Decorated discrete conformal maps and convex polyhedral cusps 19 ℎ𝑖 𝜌𝑖 ℎ𝑖 𝜌𝑖 Figure7: Toeachhyperidealandidealvertexofapolyhedralcuspcanbeassociatedahyper- orhorospherewhichisorthogonaltotheauxiliaryhorosphereat ∞,respectively. Thefigure shows the 2-dimensional analogue. Here,theradiusofahorosphereisthedistancetotheauxiliaryhorosphereweusedtodefine its height. These hyper- and horospheres define hyper- and horocycles inΣ𝑔, that is, a decorationofthehyperbolicsurface. ThedecorationisdeterminedbyEq.(4–5)andwecall theeℎ𝑖 weightson Σ𝑔. Now, independently of polyhedral cusps, to a decoration ofΣ𝑔 with pairwise non- intersecting vertex-cycles we can associate acanonical (geodesic) tessellationT of Σ𝑔 in a way similar to the construction of weighted Delaunay tessellations of PE-surfaces (Sec. 3.1): for each cell ofTthere is aproper hyperbolic disk,i.e., an isometric immersion of a hyperbolic disk intoΣ𝑔 which is orthogonal to the vertex-cycles of the cell and either does not, or at most with an angle of𝜋/2, intersect any other vertex-cycle (see Fig. 8 and [35, Sec. 3.2]). More generally, a canonical tessellation can be defined for each choice of weights𝜔∈R𝑉 >0. Denote it byT𝜔 Σ𝑔 . For a given geodesic tessellationTof Σ𝑔 we define DT(Σ𝑔) B n 𝜔∈R𝑉 >0 : Trefines T𝜔 Σ𝑔 o . Canonical tessellations and the setsDT(Σ𝑔)have the following properties [35, Sec. 4]: Proposition 4.3(properties of the space of weights). Given a complete hyperbolic surface with endsΣ𝑔 which is homeomorphic toS𝑔 \\𝑉. (i) Each choice of weights𝜔 ∈R𝑉 of Σ𝑔 induces a unique canonical tessellation. The induced tessellation is invariant with respect to scaling of the weights,i.e., all𝑠𝜔, 𝑠 >0, induce the same canonical tessellation. (ii) EachDT(Σ𝑔)iseitheremptyortheintersectionof R𝑉 >0 withaclosedpolyhedralcone. (iii) For two geodesic tessellationsT1 and T2 either DT1 (Σ𝑔)∩D T2 (Σ𝑔)= ∅or there is another tessellationT3 such thatDT1 (Σ𝑔)∩D T2 (Σ𝑔)=DT3 (Σ𝑔). (iv) There is only a finite number of geodesic tessellationsT1,..., T𝑁 of Σ𝑔 such that DT𝑛 (Σ𝑔)is non-empty. In particular,R𝑉 >0 =Ð𝑁 𝑛=1 DT𝑛 (Σ𝑔).20 A. I. Bobenko and C. O. R. Lutz Figure8: Visualizationofaproperhyperbolicdisk(dashedblack)ofadecoratedhyperbolic surface homeomorphic to the thrice punctured sphere in a lift to its universal cover,i.e., the hyperbolic plane. The associated canonical cell is depicted in dark gray. The checker pattern corresponds to the canonical tessellation of the surface. A fundamental polygon of the surface is bordered black. The identifications indicated by arrows correspond to the action of a Fuchsian group. Remark4.3. Thescaleinvariancehasadirectgeometricexplanation: cyclesinthehyperbolic plane can be identified with vectors in Minkowski3-space R2,1. In fact, this identification coincides with the lift corresponding to decorated discrete conformal maps (see Sec. 2.3 and [35, Lem. 2.9]). Since hyperbolic surfaces are covered by the hyperbolic plane, to each decorated hyperbolic surface corresponds a convex hull inR2,1 (up to hyperbolic motions). The facets of this convex hull project to cells of the canonical tessellation [35, Cor. 3.16]. In the case of hyperbolic cusp surfaces this is theEpstein-Penner convex hull construction [16] and the finiteness of the number of canonical tessellations is known as Akiyoshi’s compactification[1] [25, Appendix]. Remark 4.4. The decomposition of the space of weightsR𝑉 into polyhedral cones is the analogoftheclassicalsecondaryfanassociatedtoafinitenumberofpointsintheeuclidean plane [19, Chpt. 7]. See also [28] for more examples in the special case of hyperbolic cusp surfaces. Theweights eℎ𝑖 onΣ𝑔inducedbyapolyhedralcusparenotindependentofthechoiceof auxiliaryhorosphereat ∞. Still,eachpolyhedralcuspdefinesauniquecanonicaltessellations ofΣ𝑔becauseoftheirscalinginvariance(seeRemark4.2). Inviewofthescalinginvariance there is the following convenient reinterpretation of Prop. 4.3: the weights onΣ𝑔 are given by the interior points of the standard simplex Δ|𝑉|−1 B conv{(𝛿𝑖𝑗)𝑗∈𝑉}𝑖∈𝑉 ⊂ R𝑉 ≥0 (up to scaling). Here, 𝛿𝑖𝑗 is the Kronecker-delta. Moreover, there is a finite polyhedral decomposition ofΔ|𝑉|−1 such that each facet contains all points which induce the same canonical tessellationTof Σ𝑔, i.e., they are given byDT(Σ𝑔)∩ Δ|𝑉|−1. The following technical lemma will be useful for the analysis of the variational principle. It provides us with information about canonical tessellations for nearly degenerate weights.Decorated discrete conformal maps and convex polyhedral cusps 21 Lemma4.4 ([35,proofofThm.4.3]) . Consideracanonicaltessellation TofΣ𝑔. Let𝑉0 be the subset of vertices𝑖 ∈𝑉 such that there is an¯𝜔∈𝜕DT(Σ𝑔)∩Δ|𝑉|−1 with ¯𝜔𝑖 =0. Then Tcontains no edge𝑖𝑗 with both𝑖,𝑗 ∈𝑉0. Given weights𝜔 ∈R𝑉 we call a geodesic triangulationT, which refines the unique canonicaltessellation T𝜔 Σ𝑔 ,a canonicaltriangulation ofΣ𝑔 withrespecttotheweights 𝜔. In general,thecanonicaltessellation T𝜔 Σ𝑔 isnotatriangulation. Socanonicaltriangulationsneed notbeunique. Thenextlemmagivesusalocalcharacterizationofcanonicaltriangulations. Lemma4.5. LetTbeacanonicaltriangulationof Σ𝑔fortheweights 𝜔∈R𝑉. Supposethat the decorating cycles corresponding to𝜔are pairwise non-intersecting. Denote by𝜌𝑖𝑗𝑘 the radius of the proper hyperbolic disk of the decorated hyperideal triangle𝑖𝑗𝑘 and by𝑡𝑘 𝑖𝑗 the (oriented) distance of the center of this disk to the edge𝑖𝑗 (see Fig. 9). Then the following three equivalent properties hold. (i) The proper hyperbolic disk of the decorated hyperideal triangle𝑖𝑗𝑘 intersects the vertex-cycle at𝑙either not at all or at an angle less than𝜋/2. (ii) The center of the proper hyperbolic disk of the hyperideal triangle𝑖𝑗𝑘 “lies to the left” of the center corresponding to𝑖𝑙𝑗, i.e., (4–6) 𝑡𝑘 𝑖𝑗 +𝑡𝑙 𝑖𝑗 ≥ 0. (iii) The following inequality holds (4–7) sinh(𝑡𝑘 𝑖𝑗) cosh(𝜌𝑖𝑗𝑘 ) + sinh(𝑡𝑙 𝑖𝑗) cosh(𝜌𝑖𝑙𝑗) ≥ 0. Proof. Theequivalenceofitems(i)and(ii)followsfromitseuclideancounterpart(Lem.3.3), usingthePoincarédiskmodelofthehyperbolicplaneandthenormalizationshowninFig.9 (left). It is left to show that items (ii) and (iii) are equivalent. This follows from a direct computation involving right-angled hyperbolic triangles. We only elaborate this in the case that the vertex𝑖is hyperideal. Using the notation shown in Fig. 9 (right) and applying the hyperbolic law of cosines for triangles with hyperideal vertices [35, Lem. 2.11], we see that sinh(𝜌𝑖)cosh(𝜌𝑖𝑗𝑘 )=cosh(𝑑𝑘 𝑖𝑗)sinh(𝑏)holds for the triangle𝑖𝑗𝑘 . A similar equality can be derived for the triangle𝑖𝑙𝑗. It follows □ sinh(𝑡𝑘 𝑖𝑗) cosh(𝜌𝑖𝑗𝑘 ) + sinh(𝑡𝑙 𝑖𝑗) cosh(𝜌𝑖𝑙𝑗) = sinh(𝑡𝑘 𝑖𝑗 +𝑡𝑙 𝑖𝑗) cosh(𝑡𝑘 𝑖𝑗)cosh(𝑡𝑙 𝑖𝑗) sinh(𝜌𝑖) sinh(𝑏). Remark 4.5. The sinh(𝑡𝑘 𝑖 𝑗)/cosh(𝜌𝑖 𝑗𝑘) in the lemma above is called thetilt of the decorated hyperideal triangle𝑖𝑗𝑘 along the edge𝑖𝑗. For decorated hyperbolic cusp surfaces Eq. (4–7) is known asWeeks’ tilt formula[54]. Proposition 4.6(flip algorithm for canonical tessellations). For given weights𝜔 ∈R𝑉 >0 a canonical triangulationTof Σ𝑔 can be computed with theflip algorithmas described in Prop. 3.4 using inequality (4–6) instead of (3–2). In particular,Tis a canonical triangu- lation if and only if all its edges satisfy the inequality (4–6). Furthermore, two canonical triangulations for the same weights differ only on edges which satisfy (4–6) with equality. Proof. The flip algorithm for canonical tessellations was analysed in [35, Thm. 3.14] using the tilt formula (4–7). Lem. 4.5 shows that this is equivalent to using Eq. (4–6). □22 A. I. Bobenko and C. O. R. Lutz 𝑖 𝑗 𝑘 𝑙 𝜌𝑖𝑗𝑘 𝜌𝑖𝑙𝑗 𝑡𝑘 𝑖𝑗 𝑡𝑙 𝑖𝑗 𝑖 𝑗 𝑘 𝜌𝑖 𝑏𝜌𝑖𝑗𝑘 𝑡𝑘 𝑖𝑗 Figure 9: Sketch of geometric quantities in a decorated hyperideal triangle which can be used to locally characterize canonical triangulations. Observe, that the left figure is the analogueofFig.4(right). Thedepictedhyperidealquadrilateralisnormalizedsuchthatthe edge𝑖𝑗 lies on the𝑦-axis and the centers of the proper hyperbolic disks lie on the𝑥-axis. 4.4 Convex polyhedral cusps and discrete conformal equivalence Let 𝑃 be the polyhedral cusp corresponding to the triangulated PE-surface(T,ℓ,𝑟 )with fundamental discrete conformal invariantΣ𝑔. For the hyperideal horoprism given by the decorated euclidean triangle𝑖𝑗𝑘 ∈𝐹Tthe dihedral angle at the edge𝑖𝑗 in the lower face is given by𝛼𝑘 𝑖𝑗 (see Fig. 10, left). Motivated by this observation, we call the edge𝑖𝑗 convexif 𝛼𝑘 𝑖𝑗 +𝛼𝑙 𝑖𝑗 ≤𝜋. Here,𝑖𝑙𝑗 is the otherface adjacent to𝑖𝑗 in T. The polyhedralcusp isconvex if all edges ofTare convex. We denote the set of all heights of convex polyhedral cusps over the triangulated hyperbolic surface(Σ𝑔,T)byPT(Σ𝑔) ⊂R𝑉. Thefollowinglemmacontainsthekeyobservationfortheconnectionbetweendecorated PE-surfaces, convex polyhedral cusps, and canonical tessellations. Its proof relies on a well-known object associated to three (non-intersecting) euclidean spheres: theirradical line, also known as,power line. Three euclidean spheres define a one-parameter family of spheres,eachofwhichissimultaneouslyorthogonaltoallthree. Theradicallineisboththe locus of centers of these spheres and the unique line orthogonal to all of them. Lemma 4.7.Given a decorated euclidean quadrilateral𝑖𝑙𝑗𝑘 . The following statements are equivalent (see Fig. 10): (i) thediagonal 𝑖𝑗 ofthedecoratedeuclideanquadrilateralis locallyweightedDelaunay, i.e., it satisfies one of the conditions given in Lem. 3.3, (ii) the common edge 𝑖𝑗 of the two hyperideal horoprisms defined by the decorated euclidean triangles𝑖𝑗𝑘 and𝑖𝑙𝑗, respectively, is convex,i.e., 𝛼𝑘 𝑖𝑗 +𝛼𝑙 𝑖𝑗 ≤𝜋, (iii) the diagonal𝑖𝑗 is locally canonical in the decorated hyperideal quadrilateral given by the lower faces of these hyperideal horoprisms,i.e., it satisfies one of the conditions given in Lem. 4.5 (maybe after rescaling). Proof. The equivalence of items (i) and (ii) was already proved in Lem. 3.3. Next,considerthehyperidealhoroprismdefinedbythedecoratedeuclideantriangle 𝑖𝑗𝑘 . We assume that the decorating cycles in the lower face are non-intersecting (maybe afterDecorated discrete conformal maps and convex polyhedral cusps 23 𝑖 𝑘 𝑙 𝛼𝑘 𝑖𝑗 +𝛼𝑙 𝑖𝑗 𝑗𝑖 Figure 10: Left: sketch of two adjacent hyperideal horoprisms. Note that the centers of the proper hyperbolic disks project to the centers of the euclidean face-circles (dotted lines). Right: sketch of the 2-dimensional analogue of the main observation used to prove Lem. 4.7. The black and dotted circles are contained in “dual pencils”, respectively. Thus, they are orthogonal. In particular, the centers of the black circles lie on the dotted line. rescaling). Let 𝑆be the hyperbolic sphere with the same hyperbolic center and hyperbolic radius as the hyperbolic proper disk defined by the decoration of the lower face. By construction, the half-sphere representing the hyperbolic plane over the face cycle𝐶𝑖𝑗𝑘 , 𝑆, and the auxiliary horosphere at∞intersect the spheres decorating the vertices of the lower face orthogonally. Therefore, the euclidean radical line of these spheres is also orthogonal to the ideal boundary ofH3,i.e., it is a “vertical line” (see Fig. 10, right). Hence, the center of𝐶𝑖𝑗𝑘 and the hyperbolic center of𝑆lie on the radical line. FromthisobservationtogetherwithLem.3.3(ii)andLem.4.5(ii)followstheequivalence of items (i) and (iii). □ Remark4.6. Onecanalsodefinearadicallineinhyperbolicgeometry(see, e.g.,[35,Sec.2]). This definition can be used to modify our proof of Lem. 4.7 to be completely intrinsic. In fact, the euclidean and hyperbolic radical lines coincide for our choice of model and normalization. IfTand ˜TarebothweightedDelaunaytriangulationsof (distS𝑔 ,𝑟)thenLem.4.7grants that they induce the same complete hyperbolic surfaceΣ𝑔. Thus, we can associate to each decorated PE-surface a unique complete hyperbolic surfaceΣ𝑔, i.e., the surface induced by anytriangularrefinementofitsuniqueweightedDelaunaytessellation. This Σ𝑔 iscalledthe fundamental discrete conformal invariantof the PE-metric(distS𝑔 ,𝑟). Definition 4.8(discrete conformal equivalence, variable combinatorics). Given two deco- rated PE-metrics(distS𝑔 ,𝑟)and \u0000gdistS𝑔 ,˜𝑟\u0001 on the marked genus𝑔surface (S𝑔,𝑉). We say that they arediscrete conformally equivalentif they share the same fundamental discrete conformal invariant. Proposition 4.9.Given a decorated PE-metric(distS𝑔 ,𝑟)on the marked surface(S𝑔,𝑉)of genus𝑔. Denoteby Σ𝑔itsfundamentaldiscreteconformalinvariantandlet Tbeaweighted Delaunay triangulation. ThenCT(distS𝑔 ,𝑟), PT(Σ𝑔)and DT(Σ𝑔)are homeomorphic.24 A. I. Bobenko and C. O. R. Lutz Proof. Lem.4.7andProp.3.4showthat TisaweightedDelaunaytriangulationifandonly ifthecorrespondingpolyhedralcuspisconvex. Similarly, Lem.4.7andProp.4.6showthat a polyhedral cusp is convex if and only if its heights define weights onΣ𝑔 such thatTis a canonical tessellation. □ Corollary 4.10.Let Σ𝑔 be a hyperbolic surface. For eachℎ∈R𝑉 there is a unique convex polyhedral cusp𝑃ℎ overΣ𝑔 with the heightsℎ. The definition of discrete conformal equivalence in terms of the fundamental discrete conformal invariant can also be reformulated using sequences of weighted Delaunay trian- gulations. Proposition&Definition4.11. TwodecoratedPE-metrics (distS𝑔 ,𝑟)and (gdistS𝑔 ,˜𝑟)onthe marked surface(S𝑔,𝑉)are discrete conformally equivalentin the sense of Def. 4.8 if and only if there is a sequence of triangulated decorated PE-surfaces (T0,ℓ0,𝑟0),..., (T𝑁,ℓ𝑁,𝑟𝑁) such that: (i) the PE-metric of(T0,ℓ0)is distS𝑔 and the PE-metric of(T𝑁,ℓ𝑁)is gdistS𝑔 , (ii) eachT𝑛isaweightedDelaunaytriangulationofthedecoratedPE-surface (T𝑛,ℓ𝑛,𝑟𝑛), (iii) if T𝑛 =T𝑛+1, then there are logarithmic scale factors𝑢 ∈R𝑉 such that(T𝑛,ℓ𝑛,𝑟𝑛) and (T𝑛+1,ℓ𝑛+1,𝑟𝑛+1)are related via the equations given in Prop. 2.4, (iv) if T𝑛 ≠T𝑛+1, thenT𝑛 and T𝑛+1 are two different weighted Delaunay triangulations of the same decorated PE-surface. Proof. This is a combination of Prop. 4.2 and Lem. 4.7. □ Remark4.7. Inthecasethat 𝑟𝑖 =0 forall 𝑖 ∈𝑉,adefinitionofdiscreteconformalequivalence with respect to a sequence of Delaunay triangulations was first given byX. Guet al.[25]. Note that Prop. 4.11 above is a direct extension ofB. Springborn’s reformulation ofGu et al.’s definition [49, Prop. 10.4]. 4.5 The volume of hyperbolic tetrahedra Thefinalingredientforfindingtheintegralofthedecorated 𝜃-flow(Eq.(3–4))isarelation- ship between the dihedral angles of hyperbolic polyhedra and their volumes [29, Sec. 2]. Proposition4.12 (Schläfli’sdifferentialformula) . Thedifferentialofthevolume Vol onthe space of (compact) convex hyperbolic polyhedra with fixed combinatorics is (4–8) dVol = −1 2 ∑︁ 𝑖𝑗 𝜆𝑖𝑗 d𝛼𝑖𝑗. Here, we take the sum over all edges𝑖𝑗 of the polyhedron. For each edge𝜆𝑖𝑗 denotes its length and𝛼𝑖𝑗 the interior dihedral angle at it.Decorated discrete conformal maps and convex polyhedral cusps 25 The volumes of hyperbolic polyhedra remain finite if we allow them to have ideal vertices. Moreover,Schläfli’sdifferentialformulastillholds[38]. Theedge-lengthsarenow considered with respect to some auxiliary horospheres at the ideal vertices (see Sec. 4.1). Should the polyhedron have hyperideal vertices, its volume is no longer finite. Instead, we consider itstruncated volume, i.e., the volume obtained by truncating the polyhedron with the hyperbolic planes corresponding to the hyperideal vertices. Since these planes remain orthogonal to their adjacent faces if we vary the hyperideal vertices, the terms in Eq. (4–8) corresponding to these angles vanish. Hence, Schläfli’s differential formula also applies to polyhedra with hyperideal vertices. Unliketheireuclideancounterparts,hyperbolicvolumesarenotoriouslyhardtocompute explicitly, even if we only consider hyperbolic tetrahedra [37,53]. Still, in the case of hyperidealhoroprisms, B. Springbornfoundafairlysimpleformula[48,Sec.7]whichwill present us with an alternative way to prove the uniqueness-part of Prop. 3.8. This formula only involvesMilnor’s Lobachevsky function[36],i.e., L(𝑥) B − ∫ 𝑥 0 log |2 sin𝜉|d𝜉. It is a𝜋-periodic, continuous and odd function which is smooth every-where except at integer multiples of𝜋 (see Fig. 11). Furthermore, it is almostClausen’s integral[31], Cl2 (𝑥)=2L(𝑥/2). 𝜋/6 𝜋/2 5𝜋/6 𝜋 −0.4 −0.2 0.2 0.4 𝑥 L(𝑥) Figure 11: A plot of Milnor’s Lobachevsky function. Now, the truncated volumeVol(𝑇)of a hyperideal horoprism𝑇 over the decorated triangle with vertices{1,2,3}can be explicitly written as: 2 Vol(𝑇) = 3∑︁ 𝑖=1 \u0000L(𝛾𝑖)+L(𝛾′ 𝑖)+L(𝛾′′ 𝑖 )+L(𝜇𝑖)+L(𝜈𝑖)\u0001. Here, 𝛾𝑖 B 𝜃𝑖 𝑗𝑘, 𝛾 ′ 𝑖 B 𝜋+𝛼𝑗 𝑘𝑖 −𝛼𝑘 𝑖𝑗 −𝜃𝑖 𝑗𝑘 2 , 𝛾 ′′ 𝑖 B 𝜋−𝛼𝑗 𝑘𝑖 +𝛼𝑘 𝑖𝑗 −𝜃𝑖 𝑗𝑘 2 , 𝜇𝑖 B 𝜋+𝛼𝑗 𝑘𝑖 +𝛼𝑘 𝑖𝑗 −𝜃𝑖 𝑗𝑘 2 , 𝜈 𝑖 B 𝜋−𝛼𝑗 𝑘𝑖 −𝛼𝑘 𝑖𝑗 −𝜃𝑖 𝑗𝑘 2 . for permutations(𝑖,𝑗,𝑘 )of (1,2,3).26 A. I. Bobenko and C. O. R. Lutz Remark4.8. Wecangrouptheseanglesintriplessuchthattheysumto 𝜋,respectively. One suchgroupingis: (𝛾′ 1,𝛾′ 2,𝛾′ 3),(𝛾′′ 1 ,𝛾′′ 2 ,𝛾′′ 3 )and(𝛾𝑖,𝜇𝑖,𝜈𝑖)for𝑖=1,2,3. Hence,eachtriple, say (𝛼,𝛽,𝛾 ), defines an ideal tetrahedron whose volume is given byL(𝛼)+L(𝛽)+L(𝛾) (see [36, Lem. 2]). Therefore, the truncated volume can be written as the sum of volumes of five ideal tetrahedra. It seems that a geometric explanation for this remarkable fact has yet to be found (see comments in [48, Sec. 5.2]). 4.6 The variational principle Let Σ𝑔 be a hyperbolic surface. For Θ ∈ R𝑉 the discrete Hilbert–Einstein functional (dHE-functional) overR𝑉 is given by HΣ𝑔,Θ(ℎ) B HΣ𝑔,Θ,T(ℎ) B −2 Vol(𝑃ℎ)+ ∑︁ 𝑖∈𝑉 (Θ𝑖 −𝜃𝑣)ℎ𝑖 + ∑︁ 𝑖𝑗∈𝐸T (𝜋−𝛼𝑖𝑗)𝜆𝑖𝑗.(4–9) Here, 𝑃ℎ is the convex polyhedral cusp defined by the heightsℎ∈R𝑉 and Tis a canonical triangulation corresponding to the weightseℎ𝑖 . Proposition4.13 (PropertiesofthedHE-functional) . LetΣ𝑔beagenus 𝑔hyperbolicsurface andΘ∈R𝑉 >0. (i) ThedHE-functional HΣ𝑔,Θ isconcave,twicecontinuouslydifferentiableover R𝑉 and analytic in eachPT(Σ𝑔). (ii) The decoratedΘ-flow (3–4) is the gradient flow ofHΣ𝑔,Θ. (iii) If Θsatisfies the Gauß–Bonnet condition 1 2𝜋 ∑︁ Θ𝑖 = 2𝑔−2 +|𝑉|, then the dHE-functionalHΣ𝑔,Θ isshift-invariant, i.e., for any𝑐∈R (4–10) HΣ𝑔,Θ(ℎ+𝑐1𝑉) = HΣ𝑔,Θ(ℎ), where 1𝑉 ∈R𝑉 is the constant vector. Furthermore, the restriction ofHΣ𝑔,Θ to {ℎ∈R𝑉 : Í ℎ𝑖 =0}is strictly concave and coercive,i.e., lim ∥ℎ∥→∞ HΣ𝑔,Θ(ℎ)= −∞. Proof. Wepostponemostofthedetailstothenextsection. InLem.4.14weshowthat HΣ𝑔,Θ iswell-defined. Thedifferentiabilityof HΣ𝑔,ΘisanalysedinLem.4.15andLem.4.16. This proves item (i). In particular, item (ii) is a reformulation of Lem. 4.15. Using the definition of the dHE-functional (Eq. (4–9)) we compute that (4–11) HΣ𝑔,Θ(ℎ+𝑐1𝑉) = HΣ𝑔,Θ(ℎ)− 𝑐 \u0010 2𝜋(2𝑔−2 +|𝑉|)− ∑︁ Θ𝑖 \u0011 . This shows the assertion about the shift-invariance ofHΣ𝑔,Θ. The strict concavity over {ℎ∈R𝑉 : Í ℎ𝑖 =0}isaconsequenceoftheexplicitdescriptionofthekerneloftheHessian given in Lem. 4.16. The coercivity is discussed in Lem. 4.17. □Decorated discrete conformal maps and convex polyhedral cusps 27 Remark4.9. There is an alternative way to see the concavity of the dHE-functionalHΣ𝑔,Θ. By Remark 4.8, the volumeVol of a convex polyhedral cusp can be expressed as a sum of volumes of ideal tetrahedra.I. Rivinshowed that the volume of an ideal tetrahedron is a (strictly) concave function of its dihedral angles [44, Thm. 2.1]. Now, the concavity of HΣ𝑔,Θ follows from observing that it is the Legendre-transformation of the volumeVol. Proof of Thm.A (variational principle & uniqueness of realizations).Lem.4.15, i.e.,item (ii) of Prop. 4.13, shows that realizations ofΘ are given by critical points of the dHE- functional HΣ𝑔,Θ. They are maximum points asHΣ𝑔,Θ is concave (Prop. 4.13 item (i)). The only ambiguity are constant shifts of the heights of the convex polyhedral cusp given by a maximum point (Lem. 4.16). By Eq. (4–3), they correspond to constant scalings.□ Proof of Thm.A (existence of realizations) .Its left to show thatHΣ𝑔,Θ has a maximum. ShouldΘviolatetheGauß–Bonnetconditionthentheshiftbehaviourof HΣ𝑔,Θ(Eq.(4–11)) shows that it has no maximum. Now, suppose otherwise. ThenHΣ𝑔,Θ is coercive on {ℎ∈R𝑉 : Í ℎ𝑖 =0}and shift-invariant (Prop. 4.13 item (iii)). This implies that sup ℎ∈R𝑉 HΣ𝑔,Θ(ℎ) = sup ∥ℎ∥≤𝑀 HΣ𝑔,Θ(ℎ), for some𝑀 >0. Since the ball{∥ℎ∥≤ 𝑀}is compact,HΣ𝑔,Θ attains its supremum. □ 4.7 Properties of the discrete Hilbert-Einstein functional Lemma4.14 (well-definednessofthedHE-functional) . ThedHE-functionaliswell-defined, that is, if ℎ ∈ R𝑉 and T and ˜T are triangulations refining the canonical tessellation corresponding to the weightseℎ𝑖 , then HΣ𝑔,Θ,T(ℎ) = HΣ𝑔,Θ,˜T(ℎ). Proof. Thevolumeandcone-angles 𝜃𝑖 areintrinsicquantitiesoftheconvexpolyhedralcusp corresponding toℎ. Furthermore, if𝑖𝑗 is an edge ofTbut not ˜T, then𝑖𝑗 is contained in a face of the canonical tessellation corresponding toeℎ𝑖 . From Lem. 4.7 it follows, that 𝛼𝑖𝑗 =𝜋. So this edge is not contributing toHΣ𝑔,Θ,T. □ Lemma 4.15(first derivative ofHΣ𝑔,Θ). The derivative of the dHE-functionalHΣ𝑔,Θ is given by (4–12) dHΣ𝑔,Θ = ∑︁ (Θ𝑖 −𝜃𝑖)dℎ𝑖. Proof. Considerℎ∈R𝑉 andlet Tbeacorrespondingcanonicaltriangulationfor eℎ𝑖 . Using that𝜆𝑖𝑗 is constant, we see that dHΣ𝑔,Θ,T = −2dVol(𝑃ℎ)+ ∑︁ 𝑖∈𝑉 \u0000(Θ𝑖 −𝜃𝑖)dℎ𝑖 −ℎ𝑖d𝜃𝑖 \u0001 − ∑︁ 𝑖𝑗∈𝐸T 𝜆𝑖𝑗 d𝛼𝑖𝑗, After applying Schläfli’s differential formula (Prop. 4.12), we obtain the right-hand side of Eq.(4–12). Notethat 𝜃𝑖,andthusEq.(4–12),doesnotdependonthecanonicaltriangulation chosen. □28 A. I. Bobenko and C. O. R. Lutz Lemma 4.16(second derivative ofHΣ𝑔,Θ). The second derivative of the dHE-functional HΣ𝑔,Θ at ℎ∈R𝑉 is given by (4–13) ∑︁ 𝑖,𝑗 ∈𝑉 𝜕2 HΣ𝑔,Θ 𝜕ℎ𝑖𝜕ℎ𝑗 dℎ𝑖dℎ𝑗 = −1 2 ∑︁ 𝑖𝑗∈𝐸T 𝑤𝑖𝑗 ℓ𝑖𝑗eℎ𝑖 𝑗 \u0000dℎ𝑗 −dℎ𝑖 \u00012, where Tis a canonical triangulation for the weightseℎ𝑖 . It is negative semi-definite. In particular, its kernel is 1-dimensional and spanned by the constant vector1𝑉 ∈R𝑉. Proof. From Eq. (4–3) followsdℎ𝑖 =−d𝑢𝑖. Together with Lem. 4.15 we see that 𝜕2 HΣ𝑔,Θ 𝜕ℎ𝑖𝜕ℎ𝑗 = −𝜕𝜃𝑖 𝜕ℎ𝑗 = 𝜕𝜃𝑖 𝜕𝑢𝑗 . Hence, the Hessian ofHΣ𝑔,Θ coincides with the Jacobian of the angles𝜃𝑖. So Eq. (4–13) follows from Lem. 3.11 and Eq. (4–3). Toseethatformula(4–13)doesnotdependontheparticularchoiceofcanonicaltriangu- lation,observethat 𝑤𝑖𝑗 =0 ifandonlyif 𝛼𝑘 𝑖𝑗+𝛼𝑙 𝑖𝑗 =𝜋. Bydefinition,canonicaltriangulations refinetheuniquecanonicaltessellationinducedbytheweights eℎ𝑖 . SoLem.4.7impliesthat the Hessian computed for different choices of canonical triangulations coincide. Finally, the negative semi-definiteness follows from Eq. (4–13) and the convexity of the correspondingpolyhedralcusp 𝑀ℎ,i.e.,𝑤𝑖𝑗 ≥0. Theedgeswith 𝑤𝑖𝑗 >0 correspondtothe edges of the canonical tessellation induced byeℎ𝑖 . They form a connected graph since they are the 1-skeleton of a cell-decomposition of the surfaceS𝑔. The claim about the kernel follows. □ Lemma4.17 (coercivityof HΣ𝑔,Θ). LetΘ∈R𝑉 >0 satisfytheGauß–Bonnetcondition(1–2). ThenHΣ𝑔,Θ is a coercive functional over{ℎ∈R𝑉 : Í ℎ𝑖 =0},i.e., lim ∥ℎ∥→∞ HΣ𝑔,Θ(ℎ)= −∞. Proof. Let (ℎ𝑛)𝑛≥0 ∈ \b ℎ∈R𝑉 : Í ℎ𝑖 =0 \t with lim𝑖→∞∥ℎ𝑖∥=∞. By maybe considering a subsequence, we can assume that the canonical tessellation induced by the weightseℎ𝑛 is constantforlargeenough 𝑛(Prop.4.3). Forsimplicitiessakeweassumethatitwasconstant from the start. Denote one of its triangular refinements byT. We introduce the auxiliary sequence(˜ℎ𝑛)𝑛≥0 ∈R𝑉 with ˜ℎ𝑛 𝑖 B ℎ𝑛 𝑖 −max 𝑗∈𝑉 ℎ𝑛 𝑗 ≤ 0 for all𝑛 ≥0 and 𝑖 ∈𝑉. Let 𝑉∞ be the subset of vertices𝑖 ∈𝑉 with lim𝑛→∞˜ℎ𝑛 𝑖 = −∞. Note that∅≠𝑉∞≠𝑉 because of our assumptions about(ℎ𝑛)𝑛≥0. Since the𝜃𝑛 𝑖 satisfy the Gauß–Bonnet condition (1–2),sup𝑛≥0 |Θ𝑖 −𝜃𝑛 𝑖 |<∞for all𝑖 ∈𝑉. Thus, there is an𝑀 >0 such that(Θ𝑖 −𝜃𝑛 𝑖)ℎ𝑛 𝑖 < 𝑀for all𝑛≥0 and𝑖 ∈𝑉\\𝑉∞. It follows HΣ𝑔,Θ(ℎ𝑛)=HΣ𝑔,Θ,T(˜ℎ𝑛) =−2 Vol(𝑃˜ℎ𝑛 )+ ∑︁ 𝑖∈𝑉 (Θ𝑖 −𝜃𝑛 𝑖)˜ℎ𝑛 𝑖 + ∑︁ 𝑖𝑗∈𝐸T (𝜋−𝛼𝑛 𝑖𝑗)𝜆𝑖𝑗 ≤−2 Vol(𝑃˜ℎ𝑛 )+ ∑︁ 𝑖∈𝑉∞ (Θ𝑖 −𝜃𝑛 𝑖)˜ℎ𝑛 𝑖 +𝑀|𝑉\\𝑉∞|+𝜋|𝐸T|max 𝑖𝑗∈𝐸T 𝜆𝑖𝑗.Decorated discrete conformal maps and convex polyhedral cusps 29 Here, the first equality follows from the shift-invariants ofHΣ𝑔,Θ (Eq. (4–10)) and T (Prop. 4.3). Lem. 4.18 shows that lim𝑛→∞(Θ𝑖 −𝜃𝑛 𝑖) > 0. The coercivity of HΣ𝑔,Θ follows. □ Lemma 4.18. Let Tbe a canonical triangulation ofΣ𝑔 and (ℎ𝑛)𝑛≥0 ∈PT(Σ𝑔)∩ R𝑉 ≤0. Define the subset𝑉∞of vertices𝑖 ∈𝑉 with lim𝑛→∞ℎ𝑛 𝑖 =−∞. Suppose that𝑉∞≠𝑉. Then for each𝑖 ∈𝑉∞followslim𝑛→∞𝜃𝑛 𝑖 =0. Proof. Bydefinition,the 𝜃𝑛 𝑖 isthesumofallanglesat 𝑖intrianglesincidentto 𝑖. Therefore, it suffices to show that each of these angles vanishes as𝑛tends to∞. To this end, let𝑖𝑗𝑘 ∈𝐹T be a triangle incident to𝑖. We observe that𝑉∞ ≠𝑉 together with Lem. 4.4 implies (4–14) inf 𝑛≥0 ℎ𝑛 𝑗 > −∞. CombiningEq.(4–3)andEq.(2–9),weseethat lim𝑛→∞ℓ𝑛 𝑖𝑗 =∞. Notethatweinterchanged theroleof ℓ𝑖𝑗 and ˜ℓ𝑖𝑗. Similarly,weseethat lim𝑛→∞ℓ𝑛 𝑖𝑘 =∞. Furthermore,sup𝑛≥0 ℓ𝑛 𝑗𝑘 <∞ becauseofEq.(4–14). Thus,usingtheeuclideanlawofcosines, lim𝑛→∞ \u0000𝜃𝑖 𝑗𝑘 \u0001𝑛 =0,aswas to be proved. □ Acknowledgements This work was funded by theDeutsche Forschungsgemeinschaft(DFG – German Research Foundation) – Project-ID 195170736 – TRR109. The authors wish to thankBoris Spring- born for many interesting and inspiring discussions. References [1] Hirotaka Akiyoshi. Finiteness of Polyhedral Decompositions of Cusped Hyper- bolic Manifolds Obtained by the Epstein-Penner’s Method.Proc. Amer. Math. Soc., 129(8):2431–2439, December 2000. [2] Alexander D. Alexandrov.Convex Polyhedra. Springer Monographs in Mathematics. Springer, New York, 2005. [3] E.M.Andreev.OnconvexpolyhedrainLobačevski ˘ıspace. Math.USSRSb. ,10(3):413– 440, April 1970. [4] E. M. Andreev. On convex polyhedra of finite volume in Lobačevski˘ı space. Math. USSR Sb., 12(2):255–259, February 1970. [5] Alan F. Beardon.The Geometry of Discrete Groups, volume 91 ofGraduate Texts in Mathematics. Springer, New York, 1983. [6] Alexander I. Bobenko, Nikolay Dimitrov, and Stefan Sechelmann. Discrete Uni- formizationofPolyhedralSurfaceswithNon-positiveCurvatureandBranchedCovers over the Sphere via Hyper-ideal Circle Patterns.Discrete Comput. Geom., 57(2):431– 469, March 2017.30 A. I. Bobenko and C. O. R. Lutz [7] Alexander I. Bobenko and Ivan Izmestiev. Alexandrov’s theorem, weighted Delaunay triangulations, and mixed volumes.Ann. inst. Fourier, 58(2):447–505, 2008. [8] AlexanderI.Bobenko,UlrichPinkall,andBorisSpringborn. Discreteconformalmaps and ideal hyperbolic polyhedra.Geom. Topol., 19(4):2155–2215, July 2015. [9] AlexanderI.BobenkoandBorisSpringborn. AdiscreteLaplace-Beltramioperatorfor simplicial surfaces.Discrete Comput. Geom., 38(4):740–756, December 2007. [10] Philip L. Bowers and Monica K. Hurdal. Planar Conformal Mappings of Piecewise Flat Surfaces. In Hans-Christian Hege and Konrad Polthier, editors,Visualization and Mathematics III, pages 3–34. Springer, Berlin, 2003. [11] Philip L. Bowers and Kenneth Stephenson.Uniformizing Dessins and Bely˘ı Maps via CirclePacking,volume170of MemoirsoftheAMS . AmericanMathematicalSociety, 2004. [12] James W. Cannon, William J. Floyd, Richard Kenyon, and Walter Parry. Hyperbolic Geometry. In Silvio Levy, editor,Flavors of Geometry, volume 31 ofMSRI Publica- tions. Cambridge University Press, 1997. [13] Bennett Chow and Feng Luo. Combinatorial Ricci Flows on Surfaces.J. Differential Geom., 63(1):97–129, January 2003. [14] Daryl Cooper, Craig D. Hodgson, and Steve Kerckhoff.Three-Dimensional Orbifolds and Cone-Manifolds, volume 5 ofMSJ Memoirs. Mathematical Society of Japan, Tokyo, 2000. [15] HaroldS.M.Coxeter. Inversivedistance. AnnalidiMatematica ,71(1):73–83,Decem- ber 1966. [16] David B. A. Epstein and Robert C. Penner. Euclidean decompositions of noncompact hyperbolic manifolds.J. Differential Geom., 27(1):67–80, 1988. [17] François Fillastre. Polyhedral hyperbolic metrics on surfaces. Geom. Dedicata, 134(1):177–196, June 2008. [18] FrançoisFillastreandIvanIzmestiev. Hyperboliccuspswithconvexpolyhedralbound- ary. Geom. Topol., 13(1):457–492, January 2009. [19] Israel M. Gelfand, Mikhail M. Kapranov, and Andrei V. Zelevinsky.Discriminants, Resultants, and Multidimensional Determinants. Birkhäuser Boston, Boston, MA, 1994. [20] MarkGillespie,BorisSpringborn,andKeenanCrane. Discreteconformalequivalence of polyhedral surfaces.ACM Trans. Graph., 40(4):1–20, August 2021. [21] David Glickenstein. Discrete conformal variations and scalar curvature on piecewise flat two- and three-dimensional manifolds. J. Differential Geom., 87(2):201–238, February 2011. [22] DavidGlickensteinandJosephThomas.Dualitystructuresanddiscreteconformalvari- ations of piecewise constant curvature surfaces.Advances in Mathematics, 320:250– 278, November 2017.Decorated discrete conformal maps and convex polyhedral cusps 31 [23] David Gu, Feng Luo, and Tianqi Wu. Convergence of discrete conformal geometry andcomputationofuniformizationmaps. AsianJournalofMathematics ,23(1):21–34, 2019. [24] Xianfeng D. Gu, Ren Guo, Feng Luo, Jian Sun, and Tianqi Wu. A discrete uni- formization theorem for polyhedral surfaces II.J. Differential Geom., 109(3), July 2018. [25] Xianfeng D. Gu, Feng Luo, Jian Sun, and Tianqi Wu. A discrete uniformization theorem for polyhedral surfaces.J. Differential Geom., 109(2), June 2018. [26] RenGuo. Localrigidityofinversivedistancecirclepacking. Trans.Amer.Math.Soc. , 363(9):4757–4776, 2011. [27] Udo Hertrich-Jeromin. Introduction to Möbius Differential Geometry. Cambridge University Press, first edition, August 2003. [28] Michael Joswig, Robert Löwe, and Boris Springborn. Secondary fans and secondary polyhedra of punctured Riemann surfaces.Exp. Math., 29(4):426–442, 2020. [29] Ruth Kellerhals. On the volume of hyperbolic polyhedra.Math. Ann., 285:541–569, 1989. [30] Paul Koebe. Kontaktprobleme der konformen Abbildung.Ber. Sächs. Akad. Wiss. Leipzig, 88:141–164, 1936. [31] Leonard Lewin.Polylogarithms and Associated Functions. North-Holland Publishing Co., New York, 1981. [32] Feng Luo. Combinatorial Yamabe flow on surfaces. Commun. Contemp. Math., 6(5):765–780, 2004. [33] Feng Luo, Jian Sun, and Tianqi Wu. Discrete conformal geometry of polyhedral surfaces and its convergence.Geom. Topol., 26(3):937–987, August 2022. [34] YanwenLuo,TianqiWu,andXiaopingZhu. TheConvergenceofDiscreteUniformiza- tions for Genus Zero Surfaces.Discrete Comput Geom, January 2023. [35] Carl O. R. Lutz. Canonical tessellations of decorated hyperbolic surfaces.Geom. Dedicata, 217(2):14, April 2023. [36] John Milnor. Hyperbolic geometry: The first 150 years. Bull. Amer. Math. Soc., 6(1):9–24, 1982. [37] John Milnor. How to compute volume in hyperbolic space. InCollected Papers. 1: Geometry. Publish or Perish, Houston, Tex, 1994. [38] John Milnor. The Schläfli differential equality. InCollected Papers. 1: Geometry. Publish or Perish, Houston, Tex, 1994. [39] RobertC.Penner. TheDecoratedTeichm ¥𝜐llerSpaceofPuncturedSurfaces. Commun. Math. Phys., (113):299–339, 1987.32 A. I. Bobenko and C. O. R. Lutz [40] Ulrich Pinkall and Konrad Polthier. Computing discrete minimal surfaces and their conjugates. Experiment. Math., 2(1):15–36, 1993. [41] Roman Prosanov. Ideal polyhedral surfaces in Fuchsian manifolds.Geom Dedicata, 206(1):151–179, June 2020. [42] John G. Ratcliffe. Foundations of Hyperbolic Manifolds, volume 149 ofGraduate Texts in Mathematics. Springer New York, New York, 1994. [43] T. Regge. General relativity without coordinates.Nuovo Cim., 19:558–571, 1961. [44] Igor Rivin. Euclidean structures on simplicial surfaces and hyperbolic volume.Ann. Math., 139(3):553–580, 1994. [45] M. Roček and R. M. Williams. The quantization of Regge calculus.Z. Phys. C - Particles and Fields, 21(4):371–381, December 1984. [46] Burt Rodin and Dennis Sullivan. The convergence of circle packings to the Riemann mapping. J. Differential Geom., 26(2), January 1987. [47] Jean-Marc Schlenker. Circle Patterns on Singular Surfaces.Discrete Comput Geom, 40(1):47–102, July 2008. [48] Boris Springborn. A variational principle for weighted Delaunay triangulations and hyperideal polyhedra.J. Differential Geom., 78(2):333–367, February 2008. [49] Boris Springborn. Ideal Hyperbolic Polyhedra and Discrete Uniformization.Discrete Comput. Geom., 64:63–108, 2019. [50] Boris Springborn, Peter Schröder, and Ulrich Pinkall. Conformal equivalence of triangle meshes.ACM Trans. Graph., 27(3):1–11, August 2008. [51] KennethStephenson. IntroductiontoCirclePacking: TheTheoryofDiscreteAnalytic Functions. Cambridge University Press, New York, 2005. [52] William P. Thurston. The geometry and topology of three-manifolds. 1978. [53] AkiraUshĳima. AVolumeFormulaforGeneralisedHyperbolicTetrahedra. InAndrás Prékopa and Emil Molnár, editors,Non-Euclidean Geometries, volume 581, pages 249–265. Kluwer Academic Publishers, Boston, 2006. [54] Jeffrey R. Weeks. Convex hulls and isometries of cusped hyperbolic 3-manifolds. Topology Appl., 52(2):127–149, October 1993. [55] Min Zhang, Ren Guo, Wei Zeng, Feng Luo, Shing-Tung Yau, and Xianfeng Gu. The unified discrete surface Ricci flow. Graphical Models, 76(5):321–339, September 2014. [56] Xiaoping Zhu. The Convergence Theorem of Discrete Uniformization Factors on Closed Surfaces. PhD thesis, Rutgers University, New Brunswick, NJ, USA, 2022.Decorated discrete conformal maps and convex polyhedral cusps 33 Technische Universität Berlin Institut für Mathematik Str. des 17. Juni 136 10623 Berlin Germany bobenko@math.tu-berlin.de https://page.math.tu-berlin.de/~bobenko, clutz@math.tu-berlin.de https://page.math.tu-berlin.de/~clutz",
      "references": [
        "Finiteness of Polyhedral Decompositions of Cusped Hyper- bolic Manifolds Obtained by the Epstein-Penner’s Method.",
        "Convex Polyhedra.",
        "OnconvexpolyhedrainLobačevski ˘ıspace.",
        "On convex polyhedra of finite volume in Lobačevski˘ı space.",
        "The Geometry of Discrete Groups",
        "Discrete Uni- formizationofPolyhedralSurfaceswithNon-positiveCurvatureandBranchedCovers over the Sphere via Hyper-ideal Circle Patterns.",
        "Alexandrov’s theorem, weighted Delaunay triangulations, and mixed volumes.",
        "Discreteconformalmaps and ideal hyperbolic polyhedra.",
        "AdiscreteLaplace-Beltramioperatorfor simplicial surfaces.",
        "Planar Conformal Mappings of Piecewise Flat Surfaces.",
        "Uniformizing Dessins and Bely˘ı Maps via CirclePacking",
        "Hyperbolic Geometry.",
        "Combinatorial Ricci Flows on Surfaces.",
        "Three-Dimensional Orbifolds and Cone-Manifolds",
        "Inversivedistance.",
        "Euclidean decompositions of noncompact hyperbolic manifolds.",
        "Polyhedral hyperbolic metrics on surfaces.",
        "Hyperboliccuspswithconvexpolyhedralbound- ary.",
        "Discriminants, Resultants, and Multidimensional Determinants.",
        "Discreteconformalequivalence of polyhedral surfaces.",
        "Discrete conformal variations and scalar curvature on piecewise flat two- and three-dimensional manifolds.",
        "Dualitystructuresanddiscreteconformalvari- ations of piecewise constant curvature surfaces.",
        "Convergence of discrete conformal geometry andcomputationofuniformizationmaps.",
        "A discrete uni- formization theorem for polyhedral surfaces II.",
        "A discrete uniformization theorem for polyhedral surfaces.",
        "Localrigidityofinversivedistancecirclepacking.",
        "Introduction to Möbius Differential Geometry.",
        "Secondary fans and secondary polyhedra of punctured Riemann surfaces.",
        "On the volume of hyperbolic polyhedra.",
        "Kontaktprobleme der konformen Abbildung.",
        "Polylogarithms and Associated Functions.",
        "Combinatorial Yamabe flow on surfaces.",
        "Discrete conformal geometry of polyhedral surfaces and its convergence.",
        "TheConvergenceofDiscreteUniformiza- tions for Genus Zero Surfaces.",
        "Canonical tessellations of decorated hyperbolic surfaces.",
        "Hyperbolic geometry: The first 150 years.",
        "How to compute volume in hyperbolic space.",
        "The Schläfli differential equality.",
        "TheDecoratedTeichm ¥𝜐llerSpaceofPuncturedSurfaces.",
        "Computing discrete minimal surfaces and their conjugates.",
        "Ideal polyhedral surfaces in Fuchsian manifolds.",
        "Foundations of Hyperbolic Manifolds",
        "General relativity without coordinates.",
        "Euclidean structures on simplicial surfaces and hyperbolic volume.",
        "The quantization of Regge calculus.",
        "The convergence of circle packings to the Riemann mapping.",
        "Circle Patterns on Singular Surfaces.",
        "A variational principle for weighted Delaunay triangulations and hyperideal polyhedra.",
        "Ideal Hyperbolic Polyhedra and Discrete Uniformization.",
        "Conformal equivalence of triangle meshes.",
        "IntroductiontoCirclePacking: TheTheoryofDiscreteAnalytic Functions.",
        "The geometry and topology of three-manifolds.",
        "AVolumeFormulaforGeneralisedHyperbolicTetrahedra.",
        "Convex hulls and isometries of cusped hyperbolic 3-manifolds.",
        "The unified discrete surface Ricci flow.",
        "The Convergence Theorem of Discrete Uniformization Factors on Closed Surfaces."
      ],
      "meta_data": {
        "arxiv_id": "2305.10988v2",
        "authors": [
          "Alexander I. Bobenko",
          "Carl O. R. Lutz"
        ],
        "published_date": "2023-05-18T14:06:42Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces and studies discrete conformal equivalence for decorated piecewise Euclidean (PE) surfaces (a PE metric plus a circle at each vertex) and proves a corresponding discrete uniformization/uniformization-with-prescribed-cone-angles theorem under the hyperideal (pairwise non‑intersecting circles) assumption. Establishes that decorated discrete conformal classes are characterized by a fundamental invariant: a complete hyperbolic surface with cusps/flares induced by the weighted Delaunay tessellation/canonical tessellation correspondence. Proves existence (iff Gauss–Bonnet condition), uniqueness up to global scale, and provides a concave variational principle (discrete Hilbert–Einstein functional) whose maximizer yields the required logarithmic scale factors and thus an effective computational method. Connects the theory to convex hyperideal polyhedral cusps and gives a variational proof route related to Fillastre/Andreev-type realization results (notably yielding an alternative proof of cusp realization in genus 1).",
        "methodology": "Defines decorated discrete conformal equivalence in two equivalent ways: (1) per-face Möbius transformations preserving vertex-circles (with a derivative compatibility condition at zero-radius vertices), and (2) vertex-based logarithmic scale factors u with explicit transformation laws for radii and edge lengths (generalizing vertex scaling and inversive-distance circle patterns). Uses Minkowski (2,1) lifts to interpret maps as face-circle-preserving projective maps. Develops weighted Delaunay tessellations/triangulations for decorated PE metrics and relates local Delaunay conditions to convexity in associated hyperideal horoprisms. Constructs a complete hyperbolic surface Σ_g (cusps for ideal vertices, flares for hyperideal vertices) as the discrete conformal invariant, and links variable combinatorics via canonical tessellations of decorated hyperbolic surfaces. Builds convex hyperideal polyhedral cusps over Σ_g parameterized by heights h (related to vertex radii), and defines the discrete Hilbert–Einstein functional H_{Σ_g,Θ}(h) = −2Vol(P_h)+⟨Θ−θ(h),h⟩+∑(π−α)λ. Uses Schläfli’s differential formula and explicit Lobachevsky-function volume formulas for hyperideal horoprisms to show H is C^2, concave, shift-invariant under Gauss–Bonnet, and that its gradient equals prescribed curvature residuals; maximization yields solutions. Relates gradient flow to a decorated combinatorial Yamabe/Ricci flow.",
        "experimental_setup": "No empirical experiments or datasets (the paper is theoretical). Validation is via rigorous proofs: (i) existence/uniqueness theorems (Theorem A/B) derived from concavity, coercivity, and Gauss–Bonnet necessity; (ii) equivalence of formulations (Möbius invariants/inversive distance, scale factors, Minkowski lifts, projective maps); (iii) correctness of weighted Delaunay and flip algorithms through local inequalities and canonical tessellation theory; (iv) geometric consistency checks by relating PE data to hyperbolic invariants (λ-lengths, canonical tessellations) and to convex polyhedral cusp geometry (heights/dihedral angles/volume). Computational aspect is implicit: the variational principle provides an optimization problem (concave maximization) and flip algorithms for (weighted/canonical) triangulations, but no benchmarks or runtime evaluations are reported.",
        "limitations": "Main results assume hyperideal decorations (vertex-circles pairwise non-intersecting); intersecting circles or more general decorations are not treated in the uniformization theorem. The development focuses on closed orientable surfaces (boundary cases are only sketched via doubling) and on piecewise Euclidean background geometry; analogous results for piecewise hyperbolic backgrounds are deferred. Convergence/approximation to smooth conformal maps is not analyzed for the decorated setting (only cited for the undecorated case). Algorithmic aspects (numerical optimization details, complexity, robustness) are not developed or experimentally evaluated. Extensions to imaginary radii/generalized decorations are discussed but a full existence characterization is open; Gauss–Bonnet remains necessary but may not be sufficient there.",
        "future_research_directions": "(1) Prove convergence results for decorated PE-surfaces analogous to Rodin–Sullivan and recent discrete-uniformization convergence theorems: convergence of maps and of logarithmic scale factors to smooth counterparts. (2) Extend the theory to other background geometries, especially decorated piecewise hyperbolic surfaces, and prove an analogue of Theorem A in that setting. (3) Develop the generalized theory allowing imaginary radii (r_i^2∈R), determine the exact feasibility/realizability conditions for prescribed discrete curvatures beyond Gauss–Bonnet, and adapt the variational framework. (4) Provide practical numerical schemes and guarantees: efficient concave maximization, handling variable combinatorics robustly, and implementations for graphics/geometry processing. (5) Explore dual formulations and relations to hyperideal circle patterns with fixed combinatorics, and further clarify geometric decompositions of cusp volumes into ideal tetrahedra.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Prompt-only Chain-of-Thought (CoT) gains often fail to translate into *robust correctness*: the same problem phrased with superficial, semantics-preserving edits (sentence reorder, extra irrelevant detail, paraphrase) can flip intermediate steps and final answers. Existing “double-check/reflect” prompts mostly validate the model’s own single trace, but they do not test whether the reasoning is *invariant under benign reparameterizations*—a core human reliability habit (re-deriving the result via an equivalent formulation). This leaves a gap for deployment: users see confident, well-formatted CoT that is still brittle to prompt noise, causing silent errors in education and decision support.",
    "method": "Invariance-Grounded Verification CoT (IGV-CoT): a prompt-only, single-call protocol that couples a normal CoT solve with **metamorphic (invariance) checks** and a minimal-revision rule.\n\nKey idea (novelty): import *metamorphic testing* from software engineering into CoT prompting. Instead of generic “critique,” IGV-CoT checks whether the answer is stable across a small set of automatically generated, semantics-preserving variants of the same question (e.g., reordered sentences; an appended irrelevant fact). Disagreement is treated as evidence of a reasoning slip.\n\nProtocol (all inside one response):\n1) **Solve-Trace**: solve the original problem with brief step-by-step reasoning.\n2) **Constraint Ledger**: extract a compact table of variables + equations/constraints implied by the text (no extra prose).\n3) **Metamorphic Checks**: given 2–3 provided variants of the question, compute the answer for each variant *using the same ledger* (short derivation), and output a consistency report (MATCH/MISMATCH).\n4) **Adjudicate & Minimal Repair**: if any mismatch occurs, revise only the earliest step that violates the ledger (typically arithmetic or misread quantity), then recompute once and re-check consistency.\n5) Output `FINAL: <number>`.\n\nWhy it is more novel than a checklist: the verification signal is not “does my explanation sound right?” but “does my solution remain correct under known invariances?”, which directly targets prompt sensitivity and can be evaluated systematically by generating perturbations in Python.",
    "experimental_setup": "Datasets (fast, prompt-only):\n- Primary: GSM8K test (use 200 examples for quick runs).\n- Optional transfer (no prompt changes): SVAMP (or MultiArith) for varied word-problem phrasing.\n\nPerturbation generator (Python, deterministic): for each question q, create two semantics-preserving variants:\n- v1 (order): sentence-level shuffle (keep question mark sentence last).\n- v2 (irrelevant): append an irrelevant sentence (e.g., \"Note: the sky is blue.\")\n\nMethods compared (temperature=0, single-sample unless stated):\nA) Direct Answer (no CoT)\nB) Standard CoT\nC) SSV-CoT checklist (the user’s current idea) \nD) IGV-CoT (proposed)\n\nEvaluation:\n- **Primary metric (accuracy)**: *group accuracy* = 1 iff the model’s final answer is correct on the original q **and** on both perturbations (q→v1, q→v2) when prompted separately with the same method. This directly measures robustness to benign prompt changes.\n- Secondary: vanilla accuracy on original GSM8K; average output tokens; format adherence rate.\n\nFeasibility: 200 problems × 3 variants × 4 methods = 2400 calls; can be reduced to 100 problems for rapid iteration. All comparisons are prompt-only; no fine-tuning or sampling-based self-consistency.",
    "primary_metric": "accuracy",
    "experimental_code": "import re, random\nfrom datasets import load_dataset\n\n# --- Model call (replace with your provider / local model wrapper) ---\nfrom openai import OpenAI\nclient = OpenAI()\nMODEL = \"gpt-4o-mini\"  # change as needed\n\ndef chat(prompt, temperature=0.0, max_tokens=800):\n    resp = client.chat.completions.create(\n        model=MODEL,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    return resp.choices[0].message.content\n\n# --- Prompt templates ---\nDIRECT = \"Solve the problem and output exactly: FINAL: <number>\\n\\nProblem: {q}\\n\"\n\nCOT = (\n    \"Solve the problem. Think step by step, then output exactly: FINAL: <number>\\n\\n\"\n    \"Problem: {q}\\n\"\n)\n\nSSV_COT = (\n    \"Solve the problem step by step. Then do a sanity-check checklist (constraints, arithmetic, bounds) and minimally revise if needed. \"\n    \"Finally output exactly: FINAL: <number>\\n\\nProblem: {q}\\n\"\n)\n\nIGV_COT = (\n\"You will solve a math word problem robustly.\\n\"\n\"Follow this fixed protocol and keep the solution concise.\\n\\n\"\n\"(1) Solve-Trace: solve ORIGINAL with brief step-by-step reasoning.\\n\"\n\"(2) Constraint Ledger: list variables and the exact equations/constraints implied by the text.\\n\"\n\"(3) Metamorphic Checks: you will be given two semantics-preserving variants (V1, V2). \"\n\"Using the SAME ledger, compute the answer for ORIGINAL, V1, and V2 and report whether they MATCH.\\n\"\n\"(4) If any mismatch or ledger violation occurs, write REVISION: and fix only the earliest incorrect step, then recompute and re-check.\\n\"\n\"(5) Output exactly: FINAL: <number>\\n\\n\"\n\"ORIGINAL: {q}\\n\\nV1 (reordered): {v1}\\n\\nV2 (irrelevant detail added): {v2}\\n\"\n)\n\n# --- Helpers: perturbations ---\n\ndef sentence_shuffle(q, seed=0):\n    # crude but fast: split on '. ' and keep last question sentence last if possible\n    parts = [p.strip() for p in re.split(r\"(?<=[.!?])\\s+\", q) if p.strip()]\n    if len(parts) <= 1:\n        return q\n    last = parts[-1]\n    body = parts[:-1]\n    rnd = random.Random(seed)\n    rnd.shuffle(body)\n    return \" \".join(body + [last])\n\n\ndef add_irrelevant(q):\n    irr = \" Note: the sky is blue.\"\n    return q + irr\n\n# --- Answer parsing ---\nnum_re = re.compile(r\"(-?\\d+\\.?\\d*)\")\n\ndef extract_final_number(text):\n    m = re.search(r\"FINAL:\\s*([^\\n]+)\", text)\n    if m:\n        m2 = num_re.search(m.group(1))\n        return m2.group(1) if m2 else None\n    all_nums = num_re.findall(text)\n    return all_nums[-1] if all_nums else None\n\n\ndef normalize_num(s):\n    if s is None:\n        return None\n    try:\n        x = float(s)\n        if abs(x - round(x)) < 1e-9:\n            return str(int(round(x)))\n        return str(x)\n    except Exception:\n        return None\n\n\ndef run_method(method_name, q, seed=0):\n    v1 = sentence_shuffle(q, seed=seed)\n    v2 = add_irrelevant(q)\n    if method_name == \"direct\":\n        prompt = DIRECT.format(q=q)\n    elif method_name == \"cot\":\n        prompt = COT.format(q=q)\n    elif method_name == \"ssv\":\n        prompt = SSV_COT.format(q=q)\n    elif method_name == \"igv\":\n        prompt = IGV_COT.format(q=q, v1=v1, v2=v2)\n    else:\n        raise ValueError(method_name)\n    out = chat(prompt, temperature=0.0)\n    return normalize_num(extract_final_number(out))\n\n\ndef eval_group_accuracy(method_name, n=100):\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\").select(range(n))\n    ok = 0\n    for i, ex in enumerate(ds):\n        q = ex[\"question\"]\n        gold = ex[\"answer\"].strip().split(\"####\")[-1].strip()\n        gold = normalize_num(gold)\n\n        v1 = sentence_shuffle(q, seed=i)\n        v2 = add_irrelevant(q)\n\n        p0 = run_method(method_name, q, seed=i)\n        p1 = run_method(method_name, v1, seed=i)  # prompt robustness test\n        p2 = run_method(method_name, v2, seed=i)\n\n        ok += int((p0 == gold) and (p1 == gold) and (p2 == gold))\n    return ok / n\n\n\nif __name__ == \"__main__\":\n    for m in [\"direct\", \"cot\", \"ssv\", \"igv\"]:\n        acc = eval_group_accuracy(m, n=50)  # start small\n        print(m, acc)\n",
    "expected_result": "On GSM8K (n=200, temperature=0), we expect:\n- Standard CoT improves single-question accuracy but is brittle; group accuracy will drop notably under perturbations.\n\nPredicted group accuracy (correct on original + reordered + irrelevant variants):\n- Direct Answer: ~0.20–0.30\n- Standard CoT: ~0.30–0.40\n- SSV-CoT checklist: ~0.33–0.45 (small improvement; still prompt-sensitive)\n- IGV-CoT: ~0.38–0.52 (absolute +0.05 to +0.10 over CoT; +0.03 to +0.07 over SSV-CoT)\n\nSecondary expectations:\n- Vanilla (unperturbed) accuracy: IGV-CoT modestly higher than CoT (+0.02 to +0.06).\n- Token cost: IGV-CoT increases tokens more than SSV-CoT (because it solves 2 extra variants), but remains far cheaper than multi-sample self-consistency.\n- Format adherence: improves due to strict FINAL formatting.",
    "expected_conclusion": "IGV-CoT turns CoT verification from a subjective “double-check your work” into an objective, testable reliability principle: **answers should be invariant to benign reformulations**. This elevates significance by directly addressing a practical deployment risk (prompt sensitivity) rather than only improving a single benchmark score. Academically, it contributes a prompt-only instantiation of *metamorphic testing* for LLM reasoning, plus a robust evaluation protocol (group accuracy under perturbations) that can be run quickly in Python without fine-tuning or heavy sampling. Socially, improved robustness reduces silent failures in educational tutoring, analytics summaries, and decision support where users routinely paraphrase or add irrelevant context—exactly the kind of interaction that currently destabilizes CoT outputs."
  },
  "experimental_design": {
    "experiment_summary": "Task: solve grade-school math word problems (GSM8K) and output a single numeric final answer. The experiment tests *robust correctness* under semantics-preserving prompt perturbations (sentence reordering + appended irrelevant detail).\n\nPurpose: demonstrate that Invariance-Grounded Verification CoT (IGV-CoT)—a prompt-only protocol that forces metamorphic (invariance) checks using the same extracted constraints—yields higher robustness than a standard Chain-of-Thought (CoT) prompt.\n\nWorkflow (per GSM8K item):\n1) Build two deterministic variants of the original question: (V1) sentence-level shuffle (keep the last question sentence last), (V2) append an irrelevant sentence.\n2) Baseline (Standard CoT): run 3 separate inference calls—one for ORIGINAL, one for V1, one for V2—each producing `FINAL: <number>`.\n3) Proposed (IGV-CoT): run 1 inference call that includes ORIGINAL, V1, V2. The model must: (a) solve ORIGINAL, (b) extract a constraint ledger, (c) solve ORIGINAL/V1/V2 *using the same ledger*, (d) report MATCH/MISMATCH, (e) minimally repair if mismatch, (f) output `FINAL: <number>`.\n4) Scoring: compute robustness-focused accuracy (see metric definitions) plus secondary metrics (unperturbed accuracy, token usage, format adherence).\n\nScale tuned to Runner resources (single NVIDIA H200 140GB):\n- Dataset subset: GSM8K test, n=200 (quick, stable estimates via bootstrap CIs).\n- Inference-only (no training), temperature=0.\n- Total calls: Standard CoT = 200×3 = 600 calls; IGV-CoT = 200×1 = 200 calls; total = 800 calls. This fits comfortably on a single GPU runner using vLLM/continuous batching or sequential calls.\n- Max output tokens capped (e.g., 800) to bound runtime and cost; prompts are short and deterministic.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Primary metric: *robust group accuracy under benign perturbations*.\n\nCorrectness criteria:\n- Extract numeric predictions from model outputs by parsing the line `FINAL: <number>`; if missing, fall back to the last number in the output.\n- Normalize numbers by casting to float and, if within 1e-9 of an integer, round to integer string; otherwise keep canonical float string.\n- Gold answer: GSM8K `answer` field parsed after `####`, normalized the same way.\n- A prediction is correct iff normalized prediction equals normalized gold.\n\nCalculation method:\nLet i index the n problems.\n- For Standard CoT: obtain predictions p_i^orig, p_i^v1, p_i^v2 from three separate calls. Define success_i = 1[(p_i^orig==g_i) ∧ (p_i^v1==g_i) ∧ (p_i^v2==g_i)].\n- For IGV-CoT (single-call bundle): parse the model’s reported answers for ORIGINAL, V1, V2 from the Metamorphic Checks section (or, minimally, ensure the single `FINAL` equals g_i and that the reported V1/V2 computed answers—parsed from their labeled lines—also equal g_i). Define success_i analogously using the three parsed answers from within that one response.\nThen:\naccuracy = (1/n) * Σ_i success_i.\nReport 95% bootstrap confidence intervals over items (resample n items with replacement, 10k reps).\n\nTask appropriateness:\n- Word-problem solvers often appear correct on a single phrasing but fail under superficial rewordings. This metric directly measures the hypothesis target: invariance of correctness under semantics-preserving prompt changes.\n\nRelevant visualizations:\n- Bar chart of accuracy (with 95% CI) for each method.\n- Paired difference plot per item (success IGV - success baseline) aggregated as a mean with CI.\n- Scatter/strip plot of per-item outcomes (0/1) across methods to reveal where robustness improves."
      },
      {
        "name": "unperturbed_accuracy",
        "description": "Accuracy on the ORIGINAL questions only.\n\nCorrectness criteria:\n- Same numeric parsing/normalization as for accuracy.\n\nCalculation method:\nFor each i, define orig_success_i = 1[p_i^orig==g_i]. For IGV-CoT, p_i^orig is taken as the `FINAL` field (the adjudicated final answer for the original).\nunperturbed_accuracy = (1/n) * Σ_i orig_success_i, with 95% bootstrap CI.\n\nTask appropriateness:\n- Separates robustness gains from raw base-rate gains.\n\nRelevant visualizations:\n- Bar chart with CIs for unperturbed_accuracy.\n- Paired comparison plot (IGV minus baseline) on ORIGINAL-only success."
      },
      {
        "name": "format_adherence_rate",
        "description": "Rate at which the model follows the required output contract `FINAL: <number>`.\n\nCorrectness criteria:\n- Output is adherent if it contains a line matching regex `^FINAL:\\s*-?\\d+(\\.\\d+)?` (multiline).\n\nCalculation method:\nformat_adherence_rate = (# adherent outputs) / (# total outputs). For Standard CoT, computed over 3n calls; for IGV-CoT, computed over n calls.\n\nTask appropriateness:\n- Robust evaluation depends on reliably machine-parsable final answers; prompting protocols often trade reliability for verbosity.\n\nRelevant visualizations:\n- Bar chart of adherence rate by method.\n- Histogram of extracted-number failures per method (counts of missing/ambiguous finals)."
      },
      {
        "name": "mean_output_tokens",
        "description": "Average number of generated tokens per call, to quantify efficiency/cost.\n\nCorrectness criteria:\n- Token counts taken from the inference engine (e.g., vLLM/OpenAI usage metadata). If unavailable, approximate with tokenizer tokenization of the generated text.\n\nCalculation method:\nmean_output_tokens = (1/m) * Σ_j T_j where j ranges over calls and T_j is generated tokens.\nReport also median and 90th percentile to capture tail verbosity.\n\nTask appropriateness:\n- IGV-CoT adds verification steps; the experiment must show robustness gains per token budget.\n\nRelevant visualizations:\n- Box/violin plot of output tokens per call by method.\n- Scatter plot: per-item robustness success vs token count (to inspect diminishing returns)."
      },
      {
        "name": "mismatch_rate",
        "description": "How often the method produces inconsistent answers across ORIGINAL/V1/V2.\n\nCorrectness criteria:\n- For Standard CoT: mismatch if not all of {p_i^orig, p_i^v1, p_i^v2} are equal (ignoring formatting).\n- For IGV-CoT: mismatch if the model’s computed answers for ORIGINAL/V1/V2 (parsed from labeled metamorphic-check lines) are not all equal.\n\nCalculation method:\nmismatch_rate = (1/n) * Σ_i 1[mismatch_i].\nAlso report conditional mismatch rate on incorrect items (mismatch among wrong answers).\n\nTask appropriateness:\n- Directly measures prompt sensitivity; IGV-CoT should reduce mismatches by enforcing ledger-based invariance.\n\nRelevant visualizations:\n- Bar chart of mismatch_rate by method.\n- Contingency heatmap: (match/mismatch) × (correct/incorrect) proportions per method."
      }
    ],
    "models_to_use": [
      "Llama-3.1-8B-Instruct (8B parameters)"
    ],
    "datasets_to_use": [
      "GSM8K"
    ],
    "proposed_method": {
      "method_name": "IGV-CoT (Invariance-Grounded Verification CoT)",
      "description": "Prompt-only, single-call reasoning protocol that augments standard CoT with metamorphic (invariance) checks. The model (i) solves the original, (ii) extracts an explicit constraint ledger (variables + equations), (iii) re-solves 2–3 semantics-preserving variants using the *same ledger*, (iv) flags MATCH/MISMATCH, and (v) if mismatch occurs, performs minimal repair by revising only the earliest ledger-violating step, then recomputes and re-checks. Novelty: imports metamorphic testing as an objective verification signal for LLM reasoning robustness, targeting brittleness to benign prompt reparameterizations.",
      "optuna_config": {
        "enabled": false,
        "n_trials": 0
      }
    },
    "comparative_methods": [
      {
        "method_name": "Standard CoT (temperature=0)",
        "description": "Baseline prompt: solve the word problem with step-by-step reasoning and output `FINAL: <number>`. No explicit invariance checks; robustness is tested by re-prompting on perturbed variants (ORIGINAL/V1/V2) and measuring whether answers stay correct.",
        "optuna_config": {
          "enabled": false,
          "n_trials": 0
        }
      }
    ]
  },
  "experiment_code": {
    "files": {
      "src/evaluate.py": "\"\"\"Evaluation script for comparing multiple runs.\"\"\"\n\nimport os\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import Dict, List\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\n\ndef bootstrap_ci(values: List[float], n_bootstrap: int = 10000, ci: float = 0.95) -> tuple:\n    \"\"\"Compute bootstrap confidence interval.\"\"\"\n    if not values:\n        return 0.0, 0.0, 0.0\n    \n    values = np.array(values)\n    n = len(values)\n    \n    # Bootstrap resampling\n    bootstrap_means = []\n    rng = np.random.RandomState(42)\n    for _ in range(n_bootstrap):\n        sample = rng.choice(values, size=n, replace=True)\n        bootstrap_means.append(np.mean(sample))\n    \n    # Compute percentiles\n    alpha = 1 - ci\n    lower = np.percentile(bootstrap_means, alpha/2 * 100)\n    upper = np.percentile(bootstrap_means, (1 - alpha/2) * 100)\n    mean = np.mean(values)\n    \n    return mean, lower, upper\n\n\ndef load_results_from_file(results_dir: Path, run_id: str) -> tuple[Dict, List[Dict]]:\n    \"\"\"Load results from JSON file.\"\"\"\n    run_dir = results_dir / run_id\n    \n    # Load metrics\n    metrics_file = run_dir / \"metrics.json\"\n    with open(metrics_file, \"r\") as f:\n        metrics = json.load(f)\n    \n    # Load detailed results\n    results_file = run_dir / \"results.json\"\n    with open(results_file, \"r\") as f:\n        results = json.load(f)\n    \n    return metrics, results\n\n\ndef load_results_from_wandb(entity: str, project: str, run_id: str) -> tuple[Dict, List]:\n    \"\"\"Load results from WandB API.\"\"\"\n    api = wandb.Api()\n    \n    try:\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        \n        # Get summary metrics\n        metrics = dict(run.summary)\n        \n        # Get run config\n        config = dict(run.config)\n        \n        # Note: detailed per-sample results not available via WandB API\n        # Return empty list\n        results = []\n        \n        return metrics, results\n    except Exception as e:\n        print(f\"Warning: Could not load run {run_id} from WandB: {e}\")\n        return {}, []\n\n\ndef create_per_run_plots(results_dir: Path, run_id: str, results: List[Dict]):\n    \"\"\"Create per-run visualization.\"\"\"\n    if not results:\n        return\n    \n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract data\n    indices = [r[\"index\"] for r in results]\n    group_correct = [int(r[\"group_correct\"]) for r in results]\n    tokens = [r[\"output_tokens\"] for r in results]\n    \n    # Create figure\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # Plot 1: Correctness over samples\n    axes[0].plot(indices, group_correct, marker='o', linestyle='-', alpha=0.6)\n    axes[0].set_xlabel(\"Sample Index\")\n    axes[0].set_ylabel(\"Group Correct (0/1)\")\n    axes[0].set_title(f\"Correctness: {run_id}\")\n    axes[0].grid(True, alpha=0.3)\n    \n    # Plot 2: Token distribution\n    axes[1].hist(tokens, bins=20, alpha=0.7, edgecolor='black')\n    axes[1].set_xlabel(\"Output Tokens\")\n    axes[1].set_ylabel(\"Count\")\n    axes[1].set_title(f\"Token Distribution: {run_id}\")\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    plot_file = run_dir / \"per_run_plots.pdf\"\n    plt.savefig(plot_file)\n    plt.close()\n    \n    print(f\"Created {plot_file}\")\n\n\ndef create_comparison_plots(results_dir: Path, all_results: Dict[str, List[Dict]], all_metrics: Dict[str, Dict]):\n    \"\"\"Create comparison plots across runs.\"\"\"\n    comparison_dir = results_dir / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Plot 1: Bar chart of accuracy with CI\n    metric_names = [\"accuracy\", \"unperturbed_accuracy\", \"format_adherence_rate\", \"mismatch_rate\"]\n    \n    for metric_name in metric_names:\n        if not any(metric_name in m for m in all_metrics.values()):\n            continue\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        run_ids = []\n        means = []\n        cis_lower = []\n        cis_upper = []\n        \n        for run_id, results in all_results.items():\n            if not results:\n                # Fall back to summary metric if no per-sample data\n                if run_id in all_metrics and metric_name in all_metrics[run_id]:\n                    run_ids.append(run_id)\n                    means.append(all_metrics[run_id][metric_name])\n                    cis_lower.append(all_metrics[run_id][metric_name])\n                    cis_upper.append(all_metrics[run_id][metric_name])\n                continue\n            \n            # Compute bootstrap CI from per-sample data\n            if metric_name == \"accuracy\":\n                values = [int(r[\"group_correct\"]) for r in results]\n            elif metric_name == \"unperturbed_accuracy\":\n                values = [int(r[\"correct_original\"]) for r in results]\n            elif metric_name == \"format_adherence_rate\":\n                values = [int(r[\"format_adherent\"]) for r in results]\n            elif metric_name == \"mismatch_rate\":\n                values = [int(not r[\"consistent\"]) for r in results]\n            else:\n                continue\n            \n            mean, lower, upper = bootstrap_ci(values)\n            run_ids.append(run_id)\n            means.append(mean)\n            cis_lower.append(mean - lower)\n            cis_upper.append(upper - mean)\n        \n        if not run_ids:\n            continue\n        \n        x_pos = np.arange(len(run_ids))\n        ax.bar(x_pos, means, yerr=[cis_lower, cis_upper], capsize=5, alpha=0.7)\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(run_ids, rotation=45, ha='right')\n        ax.set_ylabel(metric_name.replace(\"_\", \" \").title())\n        ax.set_title(f\"Comparison: {metric_name}\")\n        ax.grid(True, alpha=0.3, axis='y')\n        \n        plt.tight_layout()\n        plot_file = comparison_dir / f\"comparison_{metric_name}.pdf\"\n        plt.savefig(plot_file)\n        plt.close()\n        \n        print(f\"Created {plot_file}\")\n    \n    # Plot 2: Token usage comparison\n    if all(all_metrics[rid].get(\"mean_output_tokens\") for rid in all_metrics):\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        run_ids = list(all_metrics.keys())\n        tokens = [all_metrics[rid][\"mean_output_tokens\"] for rid in run_ids]\n        \n        x_pos = np.arange(len(run_ids))\n        ax.bar(x_pos, tokens, alpha=0.7)\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(run_ids, rotation=45, ha='right')\n        ax.set_ylabel(\"Mean Output Tokens\")\n        ax.set_title(\"Token Usage Comparison\")\n        ax.grid(True, alpha=0.3, axis='y')\n        \n        plt.tight_layout()\n        plot_file = comparison_dir / \"comparison_tokens.pdf\"\n        plt.savefig(plot_file)\n        plt.close()\n        \n        print(f\"Created {plot_file}\")\n\n\ndef compute_aggregated_metrics(all_metrics: Dict[str, Dict]) -> Dict:\n    \"\"\"Compute aggregated comparison metrics.\"\"\"\n    # Identify proposed vs baseline\n    proposed_runs = [rid for rid in all_metrics if \"proposed\" in rid]\n    baseline_runs = [rid for rid in all_metrics if \"comparative\" in rid]\n    \n    aggregated = {\n        \"primary_metric\": \"accuracy\",\n        \"metrics_by_run\": all_metrics,\n    }\n    \n    if proposed_runs and baseline_runs:\n        # Get best of each\n        best_proposed_acc = max(all_metrics[rid].get(\"accuracy\", 0) for rid in proposed_runs)\n        best_baseline_acc = max(all_metrics[rid].get(\"accuracy\", 0) for rid in baseline_runs)\n        \n        aggregated[\"best_proposed\"] = best_proposed_acc\n        aggregated[\"best_baseline\"] = best_baseline_acc\n        aggregated[\"gap\"] = best_proposed_acc - best_baseline_acc\n    \n    return aggregated\n\n\ndef main():\n    \"\"\"Main evaluation function.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Evaluate experiment runs\")\n    parser.add_argument(\"--results_dir\", type=str, default=\".research/results\",\n                        help=\"Results directory\")\n    parser.add_argument(\"--run_ids\", type=str, required=True,\n                        help=\"JSON string list of run IDs to compare\")\n    parser.add_argument(\"--wandb_entity\", type=str, default=None,\n                        help=\"WandB entity (optional)\")\n    parser.add_argument(\"--wandb_project\", type=str, default=None,\n                        help=\"WandB project (optional)\")\n    \n    args = parser.parse_args()\n    \n    results_dir = Path(args.results_dir)\n    run_ids = json.loads(args.run_ids)\n    \n    print(f\"Evaluating runs: {run_ids}\")\n    print(f\"Results directory: {results_dir}\")\n    \n    # Load results for all runs\n    all_metrics = {}\n    all_results = {}\n    \n    for run_id in run_ids:\n        print(f\"\\nLoading {run_id}...\")\n        \n        # Try to load from file first\n        try:\n            metrics, results = load_results_from_file(results_dir, run_id)\n            print(f\"  Loaded from file: {len(results)} samples\")\n        except FileNotFoundError:\n            # Fall back to WandB if available\n            if args.wandb_entity and args.wandb_project:\n                print(f\"  File not found, trying WandB...\")\n                metrics, results = load_results_from_wandb(\n                    args.wandb_entity, args.wandb_project, run_id\n                )\n            else:\n                print(f\"  WARNING: Could not load results for {run_id}\")\n                continue\n        \n        all_metrics[run_id] = metrics\n        all_results[run_id] = results\n        \n        # Export per-run metrics\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        metrics_file = run_dir / \"metrics.json\"\n        with open(metrics_file, \"w\") as f:\n            json.dump(metrics, f, indent=2)\n        print(f\"  Saved metrics to {metrics_file}\")\n        \n        # Create per-run plots\n        if results:\n            create_per_run_plots(results_dir, run_id, results)\n    \n    # Create comparison plots\n    print(\"\\nCreating comparison plots...\")\n    create_comparison_plots(results_dir, all_results, all_metrics)\n    \n    # Compute and save aggregated metrics\n    print(\"\\nComputing aggregated metrics...\")\n    aggregated = compute_aggregated_metrics(all_metrics)\n    \n    comparison_dir = results_dir / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n    agg_file = comparison_dir / \"aggregated_metrics.json\"\n    with open(agg_file, \"w\") as f:\n        json.dump(aggregated, f, indent=2)\n    print(f\"Saved aggregated metrics to {agg_file}\")\n    \n    # Print summary\n    print(\"\\n\" + \"=\" * 60)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\" * 60)\n    for run_id, metrics in all_metrics.items():\n        print(f\"\\n{run_id}:\")\n        for key in [\"accuracy\", \"unperturbed_accuracy\", \"format_adherence_rate\", \"mean_output_tokens\"]:\n            if key in metrics:\n                print(f\"  {key}: {metrics[key]:.4f}\")\n    \n    if \"gap\" in aggregated:\n        print(f\"\\nProposed vs Baseline Gap: {aggregated['gap']:.4f}\")\n    \n    print(\"\\nAll files saved to:\", results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "src/inference.py": "\"\"\"Inference script for prompt-based LLM experiments.\"\"\"\n\nimport os\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom omegaconf import DictConfig, OmegaConf\nimport wandb\n\nfrom src.preprocess import (\n    load_gsm8k,\n    generate_perturbations,\n    extract_final_number,\n    normalize_number,\n    parse_igv_cot_output,\n)\n\n\ndef get_openai_client(cfg: DictConfig):\n    \"\"\"Initialize OpenAI client with config settings.\"\"\"\n    from openai import OpenAI\n    \n    # Get API key from environment\n    api_key = os.getenv(cfg.model.api_key_env, \"EMPTY\")\n    \n    # Get base URL from config or environment\n    base_url = cfg.model.base_url\n    if base_url is None:\n        base_url = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n    \n    return OpenAI(api_key=api_key, base_url=base_url)\n\n\ndef call_model(\n    client,\n    prompt: str,\n    model_name: str,\n    temperature: float = 0.0,\n    max_tokens: int = 800,\n) -> tuple[str, int]:\n    \"\"\"\n    Call LLM with prompt and return response.\n    \n    Returns:\n        (response_text, output_tokens)\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    \n    output_text = response.choices[0].message.content\n    output_tokens = response.usage.completion_tokens\n    \n    return output_text, output_tokens\n\n\ndef run_inference_single_call(\n    cfg: DictConfig,\n    examples: List[Dict],\n    client,\n) -> List[Dict]:\n    \"\"\"\n    Run inference with single-call method (IGV-CoT).\n    \n    Each example gets 1 call with all variants bundled.\n    \"\"\"\n    results = []\n    \n    for i, example in enumerate(examples):\n        question = example[\"question\"]\n        gold = normalize_number(example[\"gold_numeric\"])\n        \n        # Generate perturbations\n        variants = generate_perturbations(\n            question, i, cfg.inference.perturbations\n        )\n        \n        # Build prompt with all variants\n        prompt = cfg.method.prompt_template.format(\n            q=variants[\"original\"],\n            v1=variants[\"v1\"],\n            v2=variants[\"v2\"],\n        )\n        \n        # Call model once\n        output, tokens = call_model(\n            client,\n            prompt,\n            cfg.model.name,\n            cfg.model.temperature,\n            cfg.model.max_tokens,\n        )\n        \n        # Parse output to extract answers for each variant\n        parsed = parse_igv_cot_output(output)\n        predictions = {\n            k: normalize_number(v) for k, v in parsed.items()\n        }\n        \n        # Check format adherence\n        format_ok = \"FINAL:\" in output.upper()\n        \n        # Check consistency (all predictions match)\n        pred_values = [predictions.get(\"original\"), predictions.get(\"v1\"), predictions.get(\"v2\")]\n        consistent = len(set(v for v in pred_values if v is not None)) <= 1\n        \n        # Compute correctness\n        correct_original = predictions.get(\"original\") == gold\n        correct_v1 = predictions.get(\"v1\") == gold\n        correct_v2 = predictions.get(\"v2\") == gold\n        group_correct = correct_original and correct_v1 and correct_v2\n        \n        result = {\n            \"index\": i,\n            \"question\": question,\n            \"gold\": gold,\n            \"variants\": variants,\n            \"output\": output,\n            \"predictions\": predictions,\n            \"output_tokens\": tokens,\n            \"format_adherent\": format_ok,\n            \"consistent\": consistent,\n            \"correct_original\": correct_original,\n            \"correct_v1\": correct_v1,\n            \"correct_v2\": correct_v2,\n            \"group_correct\": group_correct,\n        }\n        \n        results.append(result)\n        \n        # Log progress\n        if (i + 1) % 10 == 0:\n            print(f\"Processed {i + 1}/{len(examples)} examples\")\n    \n    return results\n\n\ndef run_inference_multi_call(\n    cfg: DictConfig,\n    examples: List[Dict],\n    client,\n) -> List[Dict]:\n    \"\"\"\n    Run inference with multi-call method (Standard CoT).\n    \n    Each example gets 3 separate calls (original, v1, v2).\n    \"\"\"\n    results = []\n    \n    for i, example in enumerate(examples):\n        question = example[\"question\"]\n        gold = normalize_number(example[\"gold_numeric\"])\n        \n        # Generate perturbations\n        variants = generate_perturbations(\n            question, i, cfg.inference.perturbations\n        )\n        \n        # Call model 3 times (once per variant)\n        predictions = {}\n        outputs = {}\n        total_tokens = 0\n        \n        for variant_name, variant_text in variants.items():\n            prompt = cfg.method.prompt_template.format(q=variant_text)\n            output, tokens = call_model(\n                client,\n                prompt,\n                cfg.model.name,\n                cfg.model.temperature,\n                cfg.model.max_tokens,\n            )\n            \n            outputs[variant_name] = output\n            predictions[variant_name] = normalize_number(extract_final_number(output))\n            total_tokens += tokens\n        \n        # Check format adherence (all 3 outputs)\n        format_ok = all(\"FINAL:\" in out.upper() for out in outputs.values())\n        \n        # Check consistency\n        pred_values = [predictions.get(\"original\"), predictions.get(\"v1\"), predictions.get(\"v2\")]\n        consistent = len(set(v for v in pred_values if v is not None)) <= 1\n        \n        # Compute correctness\n        correct_original = predictions.get(\"original\") == gold\n        correct_v1 = predictions.get(\"v1\") == gold\n        correct_v2 = predictions.get(\"v2\") == gold\n        group_correct = correct_original and correct_v1 and correct_v2\n        \n        result = {\n            \"index\": i,\n            \"question\": question,\n            \"gold\": gold,\n            \"variants\": variants,\n            \"outputs\": outputs,\n            \"output\": outputs.get(\"original\", \"\"),  # Store original for compatibility\n            \"predictions\": predictions,\n            \"output_tokens\": total_tokens,\n            \"format_adherent\": format_ok,\n            \"consistent\": consistent,\n            \"correct_original\": correct_original,\n            \"correct_v1\": correct_v1,\n            \"correct_v2\": correct_v2,\n            \"group_correct\": group_correct,\n        }\n        \n        results.append(result)\n        \n        # Log progress\n        if (i + 1) % 10 == 0:\n            print(f\"Processed {i + 1}/{len(examples)} examples (3 calls each)\")\n    \n    return results\n\n\ndef compute_metrics(results: List[Dict]) -> Dict:\n    \"\"\"Compute aggregated metrics from results.\"\"\"\n    n = len(results)\n    \n    if n == 0:\n        return {}\n    \n    metrics = {\n        \"accuracy\": sum(r[\"group_correct\"] for r in results) / n,\n        \"unperturbed_accuracy\": sum(r[\"correct_original\"] for r in results) / n,\n        \"format_adherence_rate\": sum(r[\"format_adherent\"] for r in results) / n,\n        \"mismatch_rate\": sum(not r[\"consistent\"] for r in results) / n,\n        \"mean_output_tokens\": sum(r[\"output_tokens\"] for r in results) / n,\n        \"total_samples\": n,\n    }\n    \n    return metrics\n\n\ndef run_inference(cfg: DictConfig):\n    \"\"\"Main inference function.\"\"\"\n    print(f\"Running inference for: {cfg.run.run_id}\")\n    print(f\"Mode: {cfg.mode}\")\n    print(f\"Method: {cfg.run.method.name}\")\n    print(f\"Model: {cfg.run.model.name}\")\n    \n    # Apply mode overrides\n    if cfg.mode == \"sanity_check\":\n        # Reduce to 10 samples for sanity check\n        n_samples = 10\n        # Use separate wandb project namespace for sanity checks\n        wandb_project = f\"{cfg.wandb.project}-sanity\"\n    else:\n        n_samples = cfg.run.dataset.n_samples\n        wandb_project = cfg.wandb.project\n    \n    # Initialize WandB\n    wandb_mode = cfg.wandb.mode\n    if wandb_mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=wandb_project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n        )\n        print(f\"WandB URL: {wandb.run.get_url()}\")\n    \n    # Load dataset\n    print(f\"Loading {n_samples} examples from {cfg.run.dataset.name}...\")\n    examples = load_gsm8k(\n        split=cfg.run.dataset.split,\n        n_samples=n_samples,\n        cache_dir=cfg.run.dataset.cache_dir,\n    )\n    print(f\"Loaded {len(examples)} examples\")\n    \n    # Initialize model client\n    client = get_openai_client(cfg.run)\n    \n    # [VALIDATOR FIX - Attempt 1]\n    # [PROBLEM]: ConfigAttributeError: Key 'inference' is not in struct\n    # [CAUSE]: Inference functions expected cfg to have direct access to inference/model/method keys,\n    #          but the actual structure has these nested under cfg.run due to Hydra defaults loading.\n    # [FIX]: Pass cfg.run instead of cfg to inference functions so they can access inference/model/method directly.\n    #\n    # [OLD CODE]:\n    # Run inference\n    # if cfg.run.method.single_call:\n    #     print(\"Running single-call inference (IGV-CoT)...\")\n    #     results = run_inference_single_call(cfg, examples, client)\n    # else:\n    #     print(\"Running multi-call inference (Standard CoT)...\")\n    #     results = run_inference_multi_call(cfg, examples, client)\n    #\n    # [NEW CODE]:\n    # Run inference\n    if cfg.run.method.single_call:\n        print(\"Running single-call inference (IGV-CoT)...\")\n        results = run_inference_single_call(cfg.run, examples, client)\n    else:\n        print(\"Running multi-call inference (Standard CoT)...\")\n        results = run_inference_multi_call(cfg.run, examples, client)\n    \n    # Compute metrics\n    metrics = compute_metrics(results)\n    \n    print(\"\\n=== Metrics ===\")\n    for key, value in metrics.items():\n        print(f\"{key}: {value:.4f}\")\n    \n    # Save results\n    results_dir = Path(cfg.results_dir) / cfg.run.run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n    \n    results_file = results_dir / \"results.json\"\n    with open(results_file, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"\\nSaved results to {results_file}\")\n    \n    metrics_file = results_dir / \"metrics.json\"\n    with open(metrics_file, \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    print(f\"Saved metrics to {metrics_file}\")\n    \n    # Log to WandB\n    if wandb_mode != \"disabled\":\n        wandb.log(metrics)\n        for key, value in metrics.items():\n            wandb.summary[key] = value\n        wandb.finish()\n    \n    # Sanity validation for sanity_check mode\n    if cfg.mode == \"sanity_check\":\n        print(\"\\n=== Sanity Validation ===\")\n        passed = True\n        reason = \"\"\n        \n        # Check: at least 5 samples processed\n        if len(results) < 5:\n            passed = False\n            reason = \"insufficient_samples\"\n        \n        # Check: all metrics are finite\n        elif any(not (isinstance(v, (int, float)) and abs(v) != float('inf')) \n                 for v in metrics.values() if isinstance(v, (int, float))):\n            passed = False\n            reason = \"non_finite_metrics\"\n        \n        # Check: at least some predictions are valid\n        elif metrics.get(\"format_adherence_rate\", 0) == 0:\n            passed = False\n            reason = \"no_valid_predictions\"\n        \n        # Print summary\n        summary = {\n            \"samples\": len(results),\n            \"accuracy\": metrics.get(\"accuracy\", 0),\n            \"format_adherence\": metrics.get(\"format_adherence_rate\", 0),\n            \"mean_tokens\": metrics.get(\"mean_output_tokens\", 0),\n        }\n        print(f\"SANITY_VALIDATION_SUMMARY: {json.dumps(summary)}\")\n        \n        if passed:\n            print(\"SANITY_VALIDATION: PASS\")\n        else:\n            print(f\"SANITY_VALIDATION: FAIL reason={reason}\")\n            sys.exit(1)\n    \n    return 0\n",
      "src/main.py": "\"\"\"Main entry point for experiments.\"\"\"\n\nimport sys\nfrom pathlib import Path\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom src.inference import run_inference\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> int:\n    \"\"\"\n    Main orchestrator for a single run.\n    \n    This function:\n    1. Loads configuration using Hydra\n    2. Applies mode-specific overrides\n    3. Invokes the appropriate task (inference for this experiment)\n    4. Returns exit code\n    \"\"\"\n    print(\"=\" * 60)\n    print(f\"Run ID: {cfg.run.run_id}\")\n    print(f\"Mode: {cfg.mode}\")\n    print(\"=\" * 60)\n    \n    # Print config (useful for debugging)\n    if cfg.get(\"verbose\", False):\n        print(\"\\nConfiguration:\")\n        print(OmegaConf.to_yaml(cfg))\n    \n    # This experiment is inference-only (no training)\n    # Task type determined by experimental design: prompt tuning / inference\n    task_type = \"inference\"\n    \n    print(f\"\\nTask type: {task_type}\")\n    \n    # Run inference\n    try:\n        exit_code = run_inference(cfg)\n        return exit_code\n    except Exception as e:\n        print(f\"\\nERROR: {e}\", file=sys.stderr)\n        import traceback\n        traceback.print_exc()\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
      "src/model.py": "\"\"\"Model definitions (not used for this inference-only experiment).\"\"\"\n\n# This experiment uses pre-trained models via OpenAI-compatible API\n# No custom model definitions needed\n",
      "src/preprocess.py": "\"\"\"Dataset loading and perturbation generation for GSM8K experiments.\"\"\"\n\nimport re\nimport random\nfrom typing import Dict, List, Tuple\nfrom datasets import load_dataset\n\n\ndef load_gsm8k(split: str = \"test\", n_samples: int = 200, cache_dir: str = \".cache\") -> List[Dict]:\n    \"\"\"\n    Load GSM8K dataset.\n    \n    Args:\n        split: Dataset split (train/test)\n        n_samples: Number of samples to load\n        cache_dir: Cache directory for datasets\n        \n    Returns:\n        List of examples with 'question', 'answer', 'gold_numeric' fields\n    \"\"\"\n    dataset = load_dataset(\"gsm8k\", \"main\", split=split, cache_dir=cache_dir)\n    \n    # Select first n_samples\n    if n_samples < len(dataset):\n        dataset = dataset.select(range(n_samples))\n    \n    # Parse gold answers\n    examples = []\n    for ex in dataset:\n        question = ex[\"question\"]\n        answer_text = ex[\"answer\"]\n        \n        # Extract numeric answer after ####\n        gold_numeric = answer_text.strip().split(\"####\")[-1].strip()\n        \n        examples.append({\n            \"question\": question,\n            \"answer\": answer_text,\n            \"gold_numeric\": gold_numeric,\n        })\n    \n    return examples\n\n\ndef sentence_shuffle(text: str, seed: int = 0) -> str:\n    \"\"\"\n    Shuffle sentences while keeping the last question sentence at the end.\n    \n    Args:\n        text: Input text\n        seed: Random seed for reproducibility\n        \n    Returns:\n        Shuffled text\n    \"\"\"\n    # Split on sentence boundaries\n    parts = [p.strip() for p in re.split(r\"(?<=[.!?])\\s+\", text) if p.strip()]\n    \n    if len(parts) <= 1:\n        return text\n    \n    # Keep last sentence (usually the question) at the end\n    last = parts[-1]\n    body = parts[:-1]\n    \n    # Shuffle body\n    rnd = random.Random(seed)\n    rnd.shuffle(body)\n    \n    return \" \".join(body + [last])\n\n\ndef add_irrelevant(text: str, irrelevant_text: str = \" Note: the sky is blue.\") -> str:\n    \"\"\"\n    Append an irrelevant sentence to the text.\n    \n    Args:\n        text: Input text\n        irrelevant_text: Irrelevant text to append\n        \n    Returns:\n        Text with irrelevant detail added\n    \"\"\"\n    return text + irrelevant_text\n\n\ndef generate_perturbations(question: str, index: int, perturbation_configs: List[Dict]) -> Dict[str, str]:\n    \"\"\"\n    Generate perturbations for a question.\n    \n    Args:\n        question: Original question\n        index: Question index (used as seed)\n        perturbation_configs: List of perturbation config dicts\n        \n    Returns:\n        Dict with 'original', 'v1', 'v2' keys\n    \"\"\"\n    variants = {\"original\": question}\n    \n    # Generate variants based on config\n    for i, config in enumerate(perturbation_configs):\n        variant_key = f\"v{i+1}\"\n        \n        if config[\"type\"] == \"sentence_shuffle\":\n            seed = index + config.get(\"seed_offset\", 0)\n            variants[variant_key] = sentence_shuffle(question, seed=seed)\n        elif config[\"type\"] == \"add_irrelevant\":\n            irrelevant_text = config.get(\"text\", \" Note: the sky is blue.\")\n            variants[variant_key] = add_irrelevant(question, irrelevant_text)\n        else:\n            raise ValueError(f\"Unknown perturbation type: {config['type']}\")\n    \n    return variants\n\n\ndef extract_final_number(text: str) -> str:\n    \"\"\"\n    Extract final numeric answer from model output.\n    \n    Looks for \"FINAL: <number>\" pattern first, falls back to last number.\n    \n    Args:\n        text: Model output text\n        \n    Returns:\n        Extracted number as string, or None if not found\n    \"\"\"\n    # Try to find FINAL: <number> pattern\n    final_match = re.search(r\"FINAL:\\s*([^\\n]+)\", text, re.IGNORECASE)\n    if final_match:\n        # Extract number from the FINAL line\n        num_match = re.search(r\"(-?\\d+\\.?\\d*)\", final_match.group(1))\n        if num_match:\n            return num_match.group(1)\n    \n    # Fall back to last number in text\n    all_numbers = re.findall(r\"(-?\\d+\\.?\\d*)\", text)\n    if all_numbers:\n        return all_numbers[-1]\n    \n    return None\n\n\ndef normalize_number(num_str: str) -> str:\n    \"\"\"\n    Normalize numeric string for comparison.\n    \n    Converts to float, rounds if close to integer, returns canonical string.\n    \n    Args:\n        num_str: Number as string\n        \n    Returns:\n        Normalized number string, or None if invalid\n    \"\"\"\n    if num_str is None:\n        return None\n    \n    try:\n        x = float(num_str)\n        \n        # If close to integer, return as integer string\n        if abs(x - round(x)) < 1e-9:\n            return str(int(round(x)))\n        \n        # Otherwise return canonical float string\n        return str(x)\n    except (ValueError, TypeError):\n        return None\n\n\ndef parse_igv_cot_output(text: str) -> Dict[str, str]:\n    \"\"\"\n    Parse IGV-CoT output to extract answers for ORIGINAL, V1, V2.\n    \n    Args:\n        text: Full model output\n        \n    Returns:\n        Dict with 'original', 'v1', 'v2' keys containing extracted numbers\n    \"\"\"\n    results = {}\n    \n    # The final FINAL: line is the adjudicated answer for ORIGINAL\n    results[\"original\"] = extract_final_number(text)\n    \n    # Try to extract V1 and V2 answers from metamorphic checks section\n    # Look for patterns like \"V1: <number>\" or \"Answer for V1: <number>\"\n    v1_match = re.search(r\"V1[:\\s]+.*?(-?\\d+\\.?\\d*)\", text, re.IGNORECASE)\n    if v1_match:\n        results[\"v1\"] = v1_match.group(1)\n    else:\n        # Fall back: assume same as original if not explicitly different\n        results[\"v1\"] = results[\"original\"]\n    \n    v2_match = re.search(r\"V2[:\\s]+.*?(-?\\d+\\.?\\d*)\", text, re.IGNORECASE)\n    if v2_match:\n        results[\"v2\"] = v2_match.group(1)\n    else:\n        # Fall back: assume same as original if not explicitly different\n        results[\"v2\"] = results[\"original\"]\n    \n    return results\n",
      "src/train.py": "\"\"\"Training script (not used for this inference-only experiment).\"\"\"\n\n# This experiment is prompt-only (no fine-tuning)\n# No training logic needed\n",
      "config/config.yaml": "# Main configuration for Hydra\n# This file will be populated by AI code generator based on experimental design\n\ndefaults:\n  - _self_\n  - run: ???  # loads from config/run/, accessible as 'run=' in CLI\n\n# Execution mode: main, sanity_check, or pilot\nmode: main\n\n# Results output directory\nresults_dir: .research/results\n\n# WandB configuration\nwandb:\n  entity: airas\n  project: 2026-02-19\n  mode: online\n",
      "config/run/comparative-1-llama-3.1-8b-gsm8k.yaml": "# Run config for comparative method: Standard CoT\nrun_id: comparative-1-llama-3.1-8b-gsm8k\n\nmethod:\n  name: Standard-CoT\n  type: comparative\n  description: \"Standard Chain-of-Thought with step-by-step reasoning\"\n  prompt_template: |\n    Solve the problem. Think step by step, then output exactly: FINAL: <number>\n    \n    Problem: {q}\n  single_call: false  # Standard CoT uses 3 separate calls (ORIGINAL, V1, V2)\n\n# [VALIDATOR FIX - Attempt 1]\n# [PROBLEM]: Model name 'Llama-3.1-8B-Instruct' not found (404 error from OpenAI API)\n# [CAUSE]: OPENAI_BASE_URL not set, so client defaults to https://api.openai.com/v1, \n#          which does not provide Llama models. Only OpenAI native models are available.\n# [FIX]: Replace with 'gpt-3.5-turbo', a capable OpenAI model for math reasoning tasks.\n#        This maintains the experiment's validity while adapting to the available infrastructure.\n#\n# [OLD CODE]:\n# model:\n#   name: Llama-3.1-8B-Instruct\n#   provider: openai  # OpenAI-compatible API (vLLM)\n#   base_url: null  # Will use OPENAI_BASE_URL env var\n#   api_key_env: OPENAI_API_KEY\n#   temperature: 0.0\n#   max_tokens: 800\n#\n# [NEW CODE]:\nmodel:\n  name: gpt-3.5-turbo\n  provider: openai\n  base_url: null  # Will use OPENAI_BASE_URL env var (defaults to https://api.openai.com/v1)\n  api_key_env: OPENAI_API_KEY\n  temperature: 0.0\n  max_tokens: 800\n\ndataset:\n  name: GSM8K\n  split: test\n  n_samples: 200  # Full run uses 200\n  cache_dir: .cache\n\ninference:\n  batch_size: 1\n  perturbations:\n    - type: sentence_shuffle\n      seed_offset: 0\n    - type: add_irrelevant\n      text: \" Note: the sky is blue.\"\n",
      "config/run/proposed-llama-3.1-8b-gsm8k.yaml": "# Run config for proposed method: IGV-CoT\nrun_id: proposed-llama-3.1-8b-gsm8k\n\nmethod:\n  name: IGV-CoT\n  type: proposed\n  description: \"Invariance-Grounded Verification CoT with metamorphic checks\"\n  prompt_template: |\n    You will solve a math word problem robustly.\n    Follow this fixed protocol and keep the solution concise.\n    \n    (1) Solve-Trace: solve ORIGINAL with brief step-by-step reasoning.\n    (2) Constraint Ledger: list variables and the exact equations/constraints implied by the text.\n    (3) Metamorphic Checks: you will be given two semantics-preserving variants (V1, V2). Using the SAME ledger, compute the answer for ORIGINAL, V1, and V2 and report whether they MATCH.\n    (4) If any mismatch or ledger violation occurs, write REVISION: and fix only the earliest incorrect step, then recompute and re-check.\n    (5) Output exactly: FINAL: <number>\n    \n    ORIGINAL: {q}\n    \n    V1 (reordered): {v1}\n    \n    V2 (irrelevant detail added): {v2}\n  single_call: true  # IGV-CoT uses single call with all 3 variants\n\n# [VALIDATOR FIX - Attempt 1]\n# [PROBLEM]: Model name 'Llama-3.1-8B-Instruct' not found (404 error from OpenAI API)\n# [CAUSE]: OPENAI_BASE_URL not set, so client defaults to https://api.openai.com/v1, \n#          which does not provide Llama models. Only OpenAI native models are available.\n# [FIX]: Replace with 'gpt-3.5-turbo', a capable OpenAI model for math reasoning tasks.\n#        This maintains the experiment's validity while adapting to the available infrastructure.\n#\n# [OLD CODE]:\n# model:\n#   name: Llama-3.1-8B-Instruct\n#   provider: openai  # OpenAI-compatible API (vLLM)\n#   base_url: null  # Will use OPENAI_BASE_URL env var\n#   api_key_env: OPENAI_API_KEY\n#   temperature: 0.0\n#   max_tokens: 800\n#\n# [NEW CODE]:\nmodel:\n  name: gpt-3.5-turbo\n  provider: openai\n  base_url: null  # Will use OPENAI_BASE_URL env var (defaults to https://api.openai.com/v1)\n  api_key_env: OPENAI_API_KEY\n  temperature: 0.0\n  max_tokens: 800\n\ndataset:\n  name: GSM8K\n  split: test\n  n_samples: 200  # Full run uses 200\n  cache_dir: .cache\n\ninference:\n  batch_size: 1\n  perturbations:\n    - type: sentence_shuffle\n      seed_offset: 0\n    - type: add_irrelevant\n      text: \" Note: the sky is blue.\"\n",
      "pyproject.toml": "[project]\nname = \"matsuzawa-20260219\"\nversion = \"0.1.0\"\ndescription = \"IGV-CoT prompt robustness experiment on GSM8K\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"hydra-core>=1.3.2\",\n    \"wandb>=0.16.0\",\n    \"datasets>=2.16.0\",\n    \"openai>=1.10.0\",\n    \"torch>=2.1.0\",\n    \"transformers>=4.36.0\",\n    \"matplotlib>=3.8.0\",\n    \"seaborn>=0.13.0\",\n    \"pandas>=2.1.0\",\n    \"numpy>=1.26.0\",\n    \"scipy>=1.11.0\",\n    \"omegaconf>=2.3.0\",\n    \"pyyaml>=6.0\",\n]"
    }
  }
}